{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3fdb94",
   "metadata": {},
   "source": [
    "# Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106525e",
   "metadata": {},
   "source": [
    "Once you have managed the anomalies and created a clean dataset, you now need to create a pipeline that allows you to extract three datasets based on content from a total dataset:\n",
    "- pets dataset\n",
    "- children dataset\n",
    "- disability dataset\n",
    "\n",
    "To do this, several steps must be carried out:\n",
    "- stop word removal\n",
    "- tokenize the text\n",
    "- lemmatize the text\n",
    "- extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4eb448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import concurrent.futures\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90f863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0685adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 23:50:17,676 - INFO - NUM_THREAD fixed to 8\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48226ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'only', 'under', 'during', 't', \"he'll\", 'more', 'into', \"he'd\", 'those', 'be', 'our', \"wasn't\", 'his', \"won't\", 'wouldn', 'down', 'their', 'did', \"they'll\", \"you're\", 'needn', 'myself', \"we'd\", 'had', 'why', 'o', 'ain', 'before', 'itself', 'when', 'on', 'not', 'd', 'don', 'were', 'himself', 's', 'have', 'theirs', 'yourselves', 'below', 'haven', 'its', 'an', 'own', \"shouldn't\", 'your', 'yours', 'was', 'will', \"we've\", 'isn', 'weren', 'once', \"you'd\", 'just', \"hadn't\", 'now', 'other', 'all', \"don't\", 'from', 'ours', 've', 'above', 'that', 'ma', 'through', 'while', \"i've\", 'after', 'because', 'too', \"weren't\", 'which', \"he's\", 'such', 'wasn', 'being', 'shan', \"you'll\", 'whom', \"it'd\", \"i'm\", 'again', 'hasn', 'further', \"they're\", 'she', 'over', \"doesn't\", 'here', 'them', \"it's\", 'any', 'where', 'until', 'a', 'it', 'with', 'her', 'been', 'hers', 'there', 'doesn', 'm', 'by', 'does', 'nor', \"we're\", \"haven't\", 'he', 'if', \"i'd\", 'or', \"needn't\", 'very', 'my', 'having', 'most', \"they'd\", 'mightn', \"aren't\", 'him', 'off', \"she's\", \"they've\", 'out', 'is', 'so', 'am', 'ourselves', \"we'll\", \"shan't\", 'about', 'against', 're', 'than', 'we', 'me', \"i'll\", \"you've\", 'can', 'i', \"should've\", 'the', 'same', 'both', 'to', \"mustn't\", 'each', 'couldn', \"wouldn't\", 'at', \"didn't\", 'they', 'but', 'between', \"isn't\", 'you', 'do', 'as', 'in', 'shouldn', 'aren', 'who', 'themselves', 'herself', 'how', 'no', 'these', 'then', 'up', 'hadn', 'll', \"couldn't\", 'should', \"that'll\", 'y', \"she'd\", 'are', \"she'll\", 'and', 'has', \"mightn't\", 'this', 'won', \"hasn't\", \"it'll\", 'for', 'mustn', 'yourself', 'what', 'doing', 'of', 'didn', 'few', 'some'}\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d447",
   "metadata": {},
   "source": [
    "## Stop word removal and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a93b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "# Tokenizer qui ne casse pas les contractions\n",
    "tokenizer = TweetTokenizer(preserve_case=False)\n",
    "\n",
    "# Exceptions\n",
    "exceptions_to_keep = {\n",
    "    \"not\",\"no\",\"nor\",\"aren't\",\"can't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\n",
    "    \"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"shouldn't\",\n",
    "    \"weren't\",\"won't\",\"wouldn't\",\n",
    "    \"am\",\"is\",\"are\",\"was\",\"were\",\"have\",\"has\",\"had\",\n",
    "    \"i\",\"we\",\"you\",\"he\",\"she\",\"they\",\n",
    "    \"i'm\",\"i've\",\"you're\",\"he's\",\"she's\",\"it's\",\"they're\",\"they've\",\n",
    "    \"we're\",\"we've\",\"who's\",\"what's\",\"where's\",\"that's\",\"there's\",\n",
    "}\n",
    "exceptions_to_keep = {w.lower() for w in exceptions_to_keep}\n",
    "\n",
    "\n",
    "def is_valid_token(token: str) -> bool:\n",
    "    \"\"\"Vérify if a token is valid\"\"\"\n",
    "    # tokens parasites comme 's, 're, 'm\n",
    "    if token.startswith(\"'\"):\n",
    "        return False\n",
    "\n",
    "    # alphabetic, contractions, tirets\n",
    "    return bool(re.match(r\"^[a-z]+(?:[-'][a-z]+)*$\", token))\n",
    "\n",
    "\n",
    "def remove_stopwords(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # uniformiser quotes\n",
    "        text = text.replace(\"’\", \"'\")\n",
    "\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        filtered = [\n",
    "            tok for tok in tokens\n",
    "            if is_valid_token(tok)\n",
    "            and (tok not in stop_words or tok in exceptions_to_keep)\n",
    "        ]\n",
    "\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text, return_dtype=pl.Utf8)\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_stopwords_categories(dico: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from every list of keywords in a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dico : Dict[str, List[str]]\n",
    "        Input dictionary containing lists of keywords.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[str]]\n",
    "        A new dictionary with stopwords removed from each list.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        text = text.replace(\"’\", \"'\")\n",
    "\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        \n",
    "        filtered = [\n",
    "            tok for tok in tokens\n",
    "            if is_valid_token(tok)\n",
    "            and (tok not in stop_words or tok in exceptions_to_keep)\n",
    "        ]\n",
    "\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    new_dico = {}\n",
    "\n",
    "    for key, lst in dico.items():\n",
    "        if isinstance(lst, list):\n",
    "            new_dico[key] = [clean_text(item) for item in lst]\n",
    "        else:\n",
    "            new_dico[key] = lst\n",
    "\n",
    "    return new_dico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fef9af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: i am going store you are coming\n",
      "1: i not like movie is not good\n",
      "2: i'm happy you're that's wonderful\n",
      "3: they're they've planned it's going well\n",
      "4: she is doctor he is engineer they are tired\n",
      "5: i think you shouldn't isn't safe\n",
      "6: well is obviously not i expected\n",
      "7: i can't believe they're sure it's happening\n",
      "8: no i don't think\n",
      "9: state-of-the-art system isn't working expected\n",
      "10: \n",
      "11: None\n",
      "{'handicap': ['weight lift', 'lift ticket', 'ski lift', 'elevator pitch', 'lift-off', 'lift bridge', 'blind eye', 'window blinds', 'window blind', 'blinds', 'blind date', 'blind mask'], 'pet': ['hot dog', 'hotdog', 'hotdogs', 'dogecoin', 'corn dog', 'dog day', 'pet project', 'cat scan', 'pet scan', 'pet peeve', 'pet hate', 'pet bottle', 'bulldog clip', 'dog-ear', 'dog-iron', 'dog bark', 'dog barking', 'dogs barking', 'bird displays', 'early bird', 'early birds', 'party animal', 'screech birds', 'screech bird', 'bird-eye', 'bird bbq', 'half bird', 'chicken bird', 'street dog', 'data dog', 'sneak dogs', 'cat bus', 'dog-eared', 'cat pee', 'cat urine'], 'child': ['childhood', 'poster child', 'child process', 'family friends', 'slept like baby', 'baby corn', \"family's airline\", 'baby shower']}\n",
      "{'handicap': ['accessible entrance', 'entrance accessible', 'accessible hostel', 'accessible hostels', 'accessible hotel', 'accessible hotels', 'hotel accessible', 'accessible friendly hotel', 'accessible friendly hotels', 'accessible activities', 'activities accessible', 'accessible house', 'house accessible', 'accessible lodging', 'lodging accessible', 'accessible room', 'room accessible', 'accessible stop', 'accessible stops', 'accessible elevator', 'elevator accessible', 'accessible taxi', 'taxi accessible', 'accessible toilet', 'toilet accessible', 'accessible van', 'van accessible', 'accessible bathroom', 'bathroom accessible', 'accessible kitchen', 'kitchen accessible', 'accessible floor', 'floor accessible', 'accessible lift', 'lift accessible', 'accessible accommodations', 'accommodations accessible', 'accessible travel', 'travel accessible', 'accessible beach', 'accessible beaches', 'bitch accessible', 'accessible tour', 'tour accessible', 'accessible rvs', 'accessible cruise', 'cruise accessible', 'accessible location', 'location accessible', 'ada', 'adhd', 'amputee', 'amputees', 'arthritis', 'arthritic', 'autism', 'autistic', 'back issue', 'back issues', 'back problem', 'back problems', 'bed height', 'height bed', 'blind', 'blinded', 'blindness', 'colorblindness', 'braille', 'cane', 'walking cane', 'commode seat', 'deaf', 'deafness', 'hearing impaired', 'hearing-impaired', 'deafblind', 'deaf-blind', 'disabilities', 'disability', 'disabled', 'impaired', 'impairment', 'visually impaired', 'visual impairment', 'invalid', 'grab bar', 'grab-bar', 'grabbars', 'grab-bars', 'handicap', 'handicapped', 'handicap-accessible', 'handicap accessible', 'accessible handicap', 'handicap-friendly', 'hoyer', 'incapacitated', 'level transfer', 'transfer level', 'patient lift', 'chair lift', 'maneuver', 'maneuvering space', 'mobility aid', 'mobility aids', 'scooter', 'scooters', 'standwalk chair', 'standwalk-chair', 'mobility challenged', 'mobility-challenged', 'paralyzed', 'paralysed', 'paralyzation', 'paralysation', 'paraplegic', 'quadriplegic', 'hemiplegic', 'portable oxygen', 'osteoarthritis', 'neuromuscular disease', 'phelan mcdermid syndrome', 'power chair', 'power-chair', 'wheelchair', 'wheel chair', 'wheel-chair', 'wheelchairs', 'wheel chairs', 'ptsd', 'ramp', 'ramps', 'roll shower', 'roll-in shower', 'rollin shower', 'roll desk', 'roll-under desk', 'roll sink', 'roll-under sink', 'roll table', 'roll-under table', 'rollator', 'sit-to-stand lift', 'sit-to-stand lifts', 'shower chair', 'shower-chair', 'shower chairs', 'showercommode chairs', 'spinal cord injury', 'spinal-cord injury', 'tactile signs', 'tactil signs', 'walker', 'walking walker', 'walkers', 'walking sticks', 'walking stick', 'marine veteran', 'mobility issues', 'syndrome', 'surgeries', 'surgery'], 'pet': ['pet', 'pets', 'pet-friendly', 'pets-friendly', 'animals allowed', 'animals-allowed', 'animal friendly', 'dog', 'dogs', 'dog-friendly', 'pup', 'pups', 'puppy', 'puppies', 'canine', 'cat', 'cats', 'cat-friendly', 'kitten', 'kittens', 'rabbit', 'rabbits', 'hamster', 'hamsters', 'ferret', 'ferrets', 'bird', 'birds', 'parrot', 'parrots', 'furry companion', 'furry companions'], 'child': ['child', 'children', 'kid', 'kids', 'baby', 'babies', 'infant', 'infants', 'toddler', 'toddlers', 'youngster', 'youngsters', 'son', 'sons', 'daughter', 'daugthers', 'family', 'family-friendly', 'childcare', 'child-care', 'kids menu', 'stroller', 'strollers', 'pram', 'prams', 'pushchair', 'pushchairs', 'baby buggy', 'baby-buggy', 'baby carriage', 'baby-carriage', 'high chair', 'high-chair', 'high chairs', 'high-chairs', 'baby-chair', 'baby-chairs', 'changing table', 'changing-table', 'cot', 'cots', 'crib', 'cribs', 'baby-bed', 'playground', 'play ground', 'little ones', 'litle one']}\n"
     ]
    }
   ],
   "source": [
    "df_test = pl.DataFrame({\n",
    "    \"text\": [\n",
    "        # 1 — Stopwords simples\n",
    "        \"I am going to the store and you are coming with me.\",\n",
    "        \n",
    "        # 2 — Vérification des exclusions (négations)\n",
    "        \"I do not like this movie because it is not good.\",\n",
    "        \n",
    "        # 3 — Contractions importantes à garder\n",
    "        \"I’m happy because you’re here and that’s wonderful.\",\n",
    "        \n",
    "        # 4 — Autres contractions + stopwords\n",
    "        \"They’re doing what they’ve planned, and it’s going well.\",\n",
    "        \n",
    "        # 5 — Stopwords mais verbes à garder (am, is, are…)\n",
    "        \"She is a doctor and he is an engineer but they are tired.\",\n",
    "        \n",
    "        # 6 — Majuscules et minuscules\n",
    "        \"I Think That You Shouldn’t Do This Because It Isn’t Safe.\",\n",
    "        \n",
    "        # 7 — Mélange mots valides / invalides (ponctuation)\n",
    "        \"Well, this is — obviously — not what I expected!\",\n",
    "        \n",
    "        # 8 — Tokens non alphabetiques à retirer\n",
    "        \"I can't believe it, they’re 100% sure it's happening!!!\",\n",
    "        \n",
    "        # 9 — Phrase courte avec exceptions\n",
    "        \"No, I don’t think so.\",\n",
    "        \n",
    "        # 10 — Phrase avec mots composés (test du regex A-Za-z-)\n",
    "        \"This state-of-the-art system isn’t working as expected.\",\n",
    "        \n",
    "        # 11 — Cas limite : chaîne vide\n",
    "        \"\",\n",
    "        \n",
    "        # 12 — Cas limite : texte non string\n",
    "        None\n",
    "    ]\n",
    "})\n",
    "\n",
    "# --- Test de la fonction ---\n",
    "df_cleaned = remove_stopwords(df_test, \"text\")\n",
    "# Afficher toutes les lignes\n",
    "for i, row in enumerate(df_cleaned[\"text\"]):\n",
    "    print(f\"{i}: {row}\")\n",
    "\n",
    "# Load categories\n",
    "with open(\"../data/categories.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    categories = json.load(f)\n",
    "\n",
    "# Load exclusions\n",
    "with open(\"../data/exclusions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    exclusions = json.load(f)\n",
    "\n",
    "exclusions_cleaned= remove_stopwords_categories(exclusions)\n",
    "categories_cleaned = remove_stopwords_categories(categories)\n",
    "\n",
    "print(exclusions_cleaned)\n",
    "print(categories_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985458e7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2788d30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moris\\Desktop\\large project\\venv\\Lib\\site-packages\\spacy\\util.py:1751: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
      "  warnings.warn(Warnings.W111)\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy (disable unnecessary components for faster performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def lemmatize_and_clean_texts(\n",
    "    texts: List[str],\n",
    "    batch_size: int = 2000,\n",
    "    n_process: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize a list of texts using spaCy with multiprocessing\n",
    "    and clean them for keyword matching.\n",
    "    \n",
    "    Cleaning:\n",
    "    - Replace \" - \" with \"-\" to handle multi-word keywords like \"pet-friendly\".\n",
    "    - Strip leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        text = \" \".join([token.lemma_ for token in doc])\n",
    "        text = text.replace(\" - \", \"-\").strip()\n",
    "        lemmatized.append(text)\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def lemmatize_column_fast(\n",
    "    df: pl.DataFrame, \n",
    "    col_name: str, \n",
    "    new_col_name: str = None, \n",
    "    chunk_size: int = 50000, \n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lemmatize a Polars DataFrame column efficiently in batches with multiprocessing.\n",
    "    \"\"\"\n",
    "    new_col_name = new_col_name or f\"{col_name}_lemmatized\"\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    lemmatized_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_and_clean_texts(chunk, n_process=n_process))\n",
    "\n",
    "    return df.with_columns(pl.Series(name=new_col_name, values=lemmatized_chunks))\n",
    "\n",
    "\n",
    "def lemmatize_categories(\n",
    "    categories: Dict[str, List[str]]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Lemmatize all keywords in category dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        category: list(set(lemmatize_and_clean_texts(keywords, batch_size=100, n_process=1)))\n",
    "        for category, keywords in categories.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9ebe",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ff92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import concurrent.futures\n",
    "from typing import Dict, List\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_all_categories(\n",
    "    df: pl.DataFrame,\n",
    "    col_name: str,\n",
    "    categories: Dict[str, List[str]],\n",
    "    exclusions: Dict[str, List[str]] = None,\n",
    "    n_process: int = 4,\n",
    "    id_col: str = \"id\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract reviews matching category keywords, keeping reviews if at least one keyword remains\n",
    "    after temporarily removing exclusion phrases.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input dataframe.\n",
    "        col_name (str): Column containing the text.\n",
    "        categories (Dict[str, List[str]]): {category: [lemmatized keywords]}.\n",
    "        exclusions (Dict[str, List[str]]): {category: [phrases to exclude]}.\n",
    "        n_process (int): Number of threads.\n",
    "        id_col (str): Column containing unique IDs.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with columns [id, review, keywords_found, category].\n",
    "    \"\"\"\n",
    "\n",
    "    texts = df.select([id_col, col_name]).to_pandas()\n",
    "    exclusions = exclusions or {}\n",
    "\n",
    "    def normalize_keyword(kw: str) -> str:\n",
    "        return kw.strip().replace(\" - \", \"-\")\n",
    "\n",
    "    def make_regex(kw: str) -> str:\n",
    "        kw = normalize_keyword(kw)\n",
    "        if \" \" in kw or \"-\" in kw:\n",
    "            return re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\")\n",
    "        else:\n",
    "            return r\"\\b\" + re.escape(kw) + r\"\\b\"\n",
    "\n",
    "    def process_category(category: str, keywords: List[str], excluded_phrases: List[str]):\n",
    "        results = []\n",
    "        for _, row in texts.iterrows():\n",
    "            text = row[col_name]\n",
    "            review_id = row[id_col]\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            temp_text = text\n",
    "            # Temporarily remove the exclusion phrases\n",
    "            if excluded_phrases:\n",
    "                for ex in excluded_phrases:\n",
    "                    temp_text = re.sub(make_regex(ex), \" \", temp_text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Check if at least one keyword remains\n",
    "            matched_keywords = [kw for kw in keywords if re.search(make_regex(kw), temp_text, flags=re.IGNORECASE)]\n",
    "\n",
    "            if matched_keywords:\n",
    "                results.append((review_id, text, \", \".join(matched_keywords), category))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Parallel processing\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_process) as executor:\n",
    "        futures = {executor.submit(process_category, cat, kws, exclusions.get(cat, [])): cat\n",
    "                   for cat, kws in categories.items()}\n",
    "        for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Keyword extraction\"):\n",
    "            all_results.extend(fut.result())\n",
    "\n",
    "    if not all_results:\n",
    "        return pl.DataFrame(schema={\n",
    "            id_col: pl.Int64,\n",
    "            \"review\": pl.Utf8,\n",
    "            \"keywords_found\": pl.Utf8,\n",
    "            \"category\": pl.Utf8\n",
    "        })\n",
    "\n",
    "    df_filtered = pl.DataFrame({\n",
    "        id_col: [r[0] for r in all_results],\n",
    "        \"review\": [r[1] for r in all_results],\n",
    "        \"keywords_found\": [r[2] for r in all_results],\n",
    "        \"category\": [r[3] for r in all_results]\n",
    "    })\n",
    "\n",
    "    logger.info(f\"Extracted {df_filtered.shape[0]} matching reviews across {len(categories)} categories.\")\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c933f",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f664aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline(input_csv: str, column_name: str, output_csv: str, nb_process:int):\n",
    "\n",
    "    df = pl.read_csv(input_csv)\n",
    "    logger.info(f\"DataFrame {os.path.splitext(os.path.basename(input_csv))[0]} loaded : {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    \n",
    "    # Load categories\n",
    "    with open(\"../data/categories.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        categories = json.load(f)\n",
    "\n",
    "    # Load exclusions\n",
    "    with open(\"../data/exclusions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        exclusions = json.load(f)\n",
    "\n",
    "    logger.info(\"Keywords and excluded words loaded\")\n",
    "\n",
    "    df_clean = remove_stopwords(df, column_name)\n",
    "    logger.info(\"Stop words have been removed from reviews\")\n",
    "\n",
    "    #lemmatized_categories = lemmatize_categories(categories)\n",
    "    #lemmatized_exclusions = lemmatize_categories(exclusions)\n",
    "    #logger.info(\"Keywords and excluded words have been lemmatized\")\n",
    "\n",
    "    #df_lem = lemmatize_column_fast(df_clean, column_name, n_process=NUM_THREAD)\n",
    "    #logger.info(\"DataFrame has been lemmatized\")\n",
    "\n",
    "    exclusions_cleaned= remove_stopwords_categories(exclusions)\n",
    "    categories_cleaned = remove_stopwords_categories(categories)\n",
    "    logger.info(\"Stop words have been removed from keywords\")\n",
    "\n",
    "    df_keywords = extract_all_categories(\n",
    "        df_clean, \n",
    "        col_name = column_name,\n",
    "        categories=categories_cleaned,\n",
    "        exclusions=exclusions_cleaned,\n",
    "        n_process=nb_process\n",
    "    )\n",
    "    logger.info(\"Keywords extraction finished\")\n",
    "\n",
    "    # Save\n",
    "    df_keywords.write_csv(output_csv)\n",
    "    logger.info(f\"DataFrame saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17ea3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     input_path=\"../data/processed/data_clean/data_yelp_reviews_cleaned.csv\"\n",
    "#     name_column = \"text\"\n",
    "#     output_path = \"../data/processed/data_categorized/key_words_data_yelp_reviews.csv\"\n",
    "#     logger.info(\"Beginning of the pipeline:\")\n",
    "#     process_pipeline(input_path,name_column,output_path,NUM_THREAD)\n",
    "#     logger.info(\"End of the pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685f07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 23:50:18,286 - INFO - Beginning of the pipeline:\n",
      "2025-11-20 23:50:23,445 - INFO - DataFrame data_yelp_reviews_cleaned loaded : 6974127 rows x 10 columns\n",
      "2025-11-20 23:50:23,445 - INFO - Keywords and excluded words loaded\n",
      "2025-11-21 02:12:27,048 - INFO - Stop words have been removed from reviews\n",
      "2025-11-21 02:12:27,050 - INFO - Stop words have been removed from keywords\n",
      "Keyword extraction: 100%|██████████| 3/3 [2:57:36<00:00, 3552.23s/it]  \n",
      "2025-11-21 05:10:11,493 - INFO - Extracted 949472 matching reviews across 3 categories.\n",
      "2025-11-21 05:10:12,119 - INFO - Keywords extraction finished\n",
      "2025-11-21 05:10:12,544 - INFO - DataFrame saved to ../data/processed/data_categorized/key_words_data_yelp_reviews.csv\n",
      "2025-11-21 05:10:12,551 - INFO - End of the pipeline\n"
     ]
    }
   ],
   "source": [
    "input_path=\"../data/processed/data_clean/data_yelp_reviews_cleaned.csv\"\n",
    "name_column = \"text\"\n",
    "output_path = \"../data/processed/data_categorized/key_words_data_yelp_reviews.csv\"\n",
    "logger.info(\"Beginning of the pipeline:\")\n",
    "process_pipeline(input_path,name_column,output_path,NUM_THREAD)\n",
    "logger.info(\"End of the pipeline\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Temps d'attente en secondes (ex: 1 heure = 3600 secondes)\n",
    "cooldown_time = 60\n",
    "time.sleep(cooldown_time)\n",
    "\n",
    "# Éteindre le PC\n",
    "os.system(\"shutdown /s /t 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# # Dossiers\n",
    "# input_folder = Path(\"../data/processed/data_clean\")\n",
    "# output_folder = Path(\"../data/processed/data_categorized\")\n",
    "# output_folder.mkdir(parents=True, exist_ok=True)  # créer le dossier si besoin\n",
    "\n",
    "# # Lancer le pipeline sur tous les fichiers CSV\n",
    "# for file_path in input_folder.glob(\"*.csv\"):\n",
    "#     stem = file_path.stem  # ex: data_european_restaurant_reviews_cleaned\n",
    "\n",
    "#     # Ignorer certains fichiers\n",
    "#     if \"accessiblego\" in stem.lower():\n",
    "#         logger.info(f\"Skipping file {file_path.name} (contains 'accessiblego')\")\n",
    "#         continue\n",
    "\n",
    "#     # Choix de la colonne à traiter\n",
    "#     column_name = \"review\"\n",
    "#     if \"yelp\" in stem.lower():\n",
    "#         column_name = \"text\"\n",
    "\n",
    "#     output_file = output_folder / f\"key_words_{stem}.csv\"\n",
    "\n",
    "#     logger.info(f\"Processing file {file_path.name}...\")\n",
    "#     process_pipeline(str(file_path), column_name, str(output_file), NUM_THREAD)\n",
    "#     logger.info(f\"Finished processing {file_path.name}\")\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# # Temps d'attente en secondes (ex: 1 heure = 3600 secondes)\n",
    "# cooldown_time = 450\n",
    "# time.sleep(cooldown_time)\n",
    "\n",
    "# # Éteindre le PC\n",
    "# os.system(\"shutdown /s /t 0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
