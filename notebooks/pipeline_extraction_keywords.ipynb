{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3fdb94",
   "metadata": {},
   "source": [
    "# Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106525e",
   "metadata": {},
   "source": [
    "Once you have managed the anomalies and created a clean dataset, you now need to create a pipeline that allows you to extract three datasets based on content from a total dataset:\n",
    "- pets dataset\n",
    "- children dataset\n",
    "- disability dataset\n",
    "\n",
    "To do this, several steps must be carried out:\n",
    "- stop word removal\n",
    "- tokenize the text\n",
    "- lemmatize the text\n",
    "- extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4eb448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import concurrent.futures\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0685adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 22:20:46,034 - INFO - NUM_THREAD fixed to 12\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d447",
   "metadata": {},
   "source": [
    "## Stop word removal and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "NEGATIONS = {\"not\", \"no\", \"never\", \"none\", \"cannot\", \"can't\", \"don't\", \n",
    "             \"doesn't\", \"isn't\", \"wasn't\", \"weren't\", \"wouldn't\", \"shouldn't\", \"couldn't\"}\n",
    "\n",
    "\n",
    "def remove_stopwords(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given text column in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing text data.\n",
    "    column_name : str\n",
    "        Name of the column containing the text to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A new DataFrame with stopwords removed from the specified column.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered = [word for word in tokens if re.match(r\"^[A-Za-z-]+$\", word) \n",
    "                    and (word not in stop_words or word in NEGATIONS)]\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text, return_dtype=pl.Utf8).alias(column_name)\n",
    "    )\n",
    "\n",
    "def remove_stopwords_categories(dico: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dico : Dict\n",
    "        Input dictionnary containing three lists of keywords.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict\n",
    "        A new dictionary with stopwords removed from every keywords list.\n",
    "    \"\"\"\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered = [\n",
    "            word for word in tokens\n",
    "            if re.match(r\"^[A-Za-z-]+$\", word)\n",
    "            and (word not in stop_words or word in NEGATIONS)\n",
    "        ]\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    new_dico = {}\n",
    "\n",
    "    for key, lst in dico.items():\n",
    "        if isinstance(lst, list):\n",
    "            new_dico[key] = [clean_text(item) for item in lst]\n",
    "        else:\n",
    "            new_dico[key] = lst\n",
    "\n",
    "    return new_dico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985458e7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2788d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy (disable unnecessary components for faster performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def lemmatize_and_clean_texts(\n",
    "    texts: List[str],\n",
    "    batch_size: int = 2000,\n",
    "    n_process: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize a list of texts using spaCy with multiprocessing\n",
    "    and clean them for keyword matching.\n",
    "    \n",
    "    Cleaning:\n",
    "    - Replace \" - \" with \"-\" to handle multi-word keywords like \"pet-friendly\".\n",
    "    - Strip leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        text = \" \".join([token.lemma_ for token in doc])\n",
    "        text = text.replace(\" - \", \"-\").strip()\n",
    "        lemmatized.append(text)\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def lemmatize_column_fast(\n",
    "    df: pl.DataFrame, \n",
    "    col_name: str, \n",
    "    new_col_name: str = None, \n",
    "    chunk_size: int = 30000, \n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lemmatize a Polars DataFrame column efficiently in batches with multiprocessing.\n",
    "    \"\"\"\n",
    "    new_col_name = new_col_name or f\"{col_name}_lemmatized\"\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    lemmatized_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_and_clean_texts(chunk, n_process=n_process))\n",
    "\n",
    "    return df.with_columns(pl.Series(name=new_col_name, values=lemmatized_chunks))\n",
    "\n",
    "\n",
    "def lemmatize_categories(\n",
    "    categories: Dict[str, List[str]]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Lemmatize all keywords in category dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        category: list(set(lemmatize_and_clean_texts(keywords, batch_size=100, n_process=1)))\n",
    "        for category, keywords in categories.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9ebe",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ff92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import concurrent.futures\n",
    "from typing import Dict, List\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_all_categories(\n",
    "    df: pl.DataFrame,\n",
    "    col_name: str,\n",
    "    categories: Dict[str, List[str]],\n",
    "    exclusions: Dict[str, List[str]] = None,\n",
    "    n_process: int = 4,\n",
    "    id_col: str = \"id\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract reviews matching category keywords, keeping reviews if at least one keyword remains\n",
    "    after temporarily removing exclusion phrases.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input dataframe.\n",
    "        col_name (str): Column containing the text.\n",
    "        categories (Dict[str, List[str]]): {category: [lemmatized keywords]}.\n",
    "        exclusions (Dict[str, List[str]]): {category: [phrases to exclude]}.\n",
    "        n_process (int): Number of threads.\n",
    "        id_col (str): Column containing unique IDs.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with columns [id, review, keywords_found, category].\n",
    "    \"\"\"\n",
    "\n",
    "    texts = df.select([id_col, col_name]).to_pandas()\n",
    "    exclusions = exclusions or {}\n",
    "\n",
    "    def normalize_keyword(kw: str) -> str:\n",
    "        return kw.strip().replace(\" - \", \"-\")\n",
    "\n",
    "    def make_regex(kw: str) -> str:\n",
    "        kw = normalize_keyword(kw)\n",
    "        if \" \" in kw or \"-\" in kw:\n",
    "            return re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\")\n",
    "        else:\n",
    "            return r\"\\b\" + re.escape(kw) + r\"\\b\"\n",
    "\n",
    "    def process_category(category: str, keywords: List[str], excluded_phrases: List[str]):\n",
    "        results = []\n",
    "        for _, row in texts.iterrows():\n",
    "            text = row[col_name]\n",
    "            review_id = row[id_col]\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            temp_text = text\n",
    "            # Temporarily remove the exclusion phrases\n",
    "            if excluded_phrases:\n",
    "                for ex in excluded_phrases:\n",
    "                    temp_text = re.sub(make_regex(ex), \" \", temp_text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Check if at least one keyword remains\n",
    "            matched_keywords = [kw for kw in keywords if re.search(make_regex(kw), temp_text, flags=re.IGNORECASE)]\n",
    "\n",
    "            if matched_keywords:\n",
    "                results.append((review_id, text, \", \".join(matched_keywords), category))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Parallel processing\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_process) as executor:\n",
    "        futures = {executor.submit(process_category, cat, kws, exclusions.get(cat, [])): cat\n",
    "                   for cat, kws in categories.items()}\n",
    "        for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Keyword extraction\"):\n",
    "            all_results.extend(fut.result())\n",
    "\n",
    "    if not all_results:\n",
    "        return pl.DataFrame(schema={\n",
    "            id_col: pl.Int64,\n",
    "            \"review\": pl.Utf8,\n",
    "            \"keywords_found\": pl.Utf8,\n",
    "            \"category\": pl.Utf8\n",
    "        })\n",
    "\n",
    "    df_filtered = pl.DataFrame({\n",
    "        id_col: [r[0] for r in all_results],\n",
    "        \"review\": [r[1] for r in all_results],\n",
    "        \"keywords_found\": [r[2] for r in all_results],\n",
    "        \"category\": [r[3] for r in all_results]\n",
    "    })\n",
    "\n",
    "    logger.info(f\"Extracted {df_filtered.shape[0]} matching reviews across {len(categories)} categories.\")\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c933f",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f664aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline(input_csv: str, column_name: str, output_csv: str, nb_process:int):\n",
    "\n",
    "    df = pl.read_csv(input_csv)\n",
    "    logger.info(f\"DataFrame {os.path.splitext(os.path.basename(input_csv))[0]} loaded : {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    \n",
    "    # Load categories\n",
    "    with open(\"../data/categories.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        categories = json.load(f)\n",
    "\n",
    "    # Load exclusions\n",
    "    with open(\"../data/exclusions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        exclusions = json.load(f)\n",
    "\n",
    "    logger.info(\"Keywords and excluded words loaded\")\n",
    "\n",
    "    df_clean = remove_stopwords(df, column_name)\n",
    "    logger.info(\"Stop words have been removed from reviews\")\n",
    "\n",
    "    #lemmatized_categories = lemmatize_categories(categories)\n",
    "    #lemmatized_exclusions = lemmatize_categories(exclusions)\n",
    "    #logger.info(\"Keywords and excluded words have been lemmatized\")\n",
    "\n",
    "    #df_lem = lemmatize_column_fast(df_clean, column_name, n_process=NUM_THREAD)\n",
    "    #logger.info(\"DataFrame has been lemmatized\")\n",
    "\n",
    "    exclusions_cleaned= remove_stopwords_categories(exclusions)\n",
    "    categories_cleaned = remove_stopwords_categories(categories)\n",
    "    logger.info(\"Stop words have been removed from keywords\")\n",
    "\n",
    "    df_keywords = extract_all_categories(\n",
    "        df_clean, \n",
    "        col_name = column_name,\n",
    "        categories=categories_cleaned,\n",
    "        exclusions=exclusions_cleaned,\n",
    "        n_process=nb_process\n",
    "    )\n",
    "    logger.info(\"Keywords extraction finished\")\n",
    "\n",
    "    # Save\n",
    "    df_keywords.write_csv(output_csv)\n",
    "    logger.info(f\"DataFrame saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17ea3906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 22:59:39,574 - INFO - Beginning of the pipeline:\n",
      "2025-11-17 22:59:39,577 - INFO - DataFrame data_accessiblego_cleaned loaded : 1896 rows x 5 columns\n",
      "2025-11-17 22:59:39,588 - INFO - Keywords and excluded words loaded\n",
      "2025-11-17 22:59:39,978 - INFO - Stop words have been removed from reviews\n",
      "2025-11-17 22:59:39,988 - INFO - Stop words have been removed from keywords\n",
      "Keyword extraction: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n",
      "2025-11-17 22:59:42,364 - INFO - Extracted 1702 matching reviews across 3 categories.\n",
      "2025-11-17 22:59:42,364 - INFO - Keywords extraction finished\n",
      "2025-11-17 22:59:42,369 - INFO - DataFrame saved to ../data/processed/data_categorized/key_words_data_accessiblego.csv\n",
      "2025-11-17 22:59:42,369 - INFO - End of the pipeline\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_path=\"../data/processed/data_clean/data_accessiblego_cleaned.csv\"\n",
    "    name_column = \"review\"\n",
    "    output_path = \"../data/processed/data_categorized/key_words_data_accessiblego.csv\"\n",
    "    logger.info(\"Beginning of the pipeline:\")\n",
    "    process_pipeline(input_path,name_column,output_path,NUM_THREAD)\n",
    "    logger.info(\"End of the pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e683e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': ['walker']}\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords_categories({\"test\":[\"walker\"]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
