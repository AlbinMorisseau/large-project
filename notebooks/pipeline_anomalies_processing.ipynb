{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2536e8",
   "metadata": {},
   "source": [
    "# Pipeline to manage anomalies processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5131a",
   "metadata": {},
   "source": [
    "Before using any dataset, it is necessary to clean the data to obtain a clear, formatted, and complete dataset. This tedious step is the first step in data analysis.\n",
    "\n",
    "In this notebook, there are building blocks (functions) that can be used later to perform the identified preprocessing steps.\n",
    "\n",
    "- Remove missing values\n",
    "- Remove duplicates\n",
    "- Remove special characters\n",
    "- Convert numbers to letters\n",
    "- Identify the language of the review and translate it if necessary\n",
    "- Correct spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7caac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from num2words import num2words\n",
    "import langid\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from spellchecker import SpellChecker\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b666cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.getLogger(\"langid\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "98210f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:43:18,625 - INFO - NUM_THREAD fixed to 12\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4736d",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "735da6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df: pl.DataFrame, column_name: str) -> tuple[pl.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Remove rows from a DataFrame where the specified column has missing values,\n",
    "    and return the cleaned DataFrame and the number of missing values removed.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to clean.\n",
    "        column_name (str): Column to check for missing values.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, int]: Cleaned DataFrame and number of missing values removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop rows where the specified column is null\n",
    "    df_clean = df.drop_nulls(subset=[column_name])\n",
    "\n",
    "    nb_missing_values = df[column_name].null_count()\n",
    "    \n",
    "    return df_clean,nb_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17059c65",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8caa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pl.DataFrame, subset_columns: list) -> tuple[pl.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a DataFrame based on specified columns,\n",
    "    and return the cleaned DataFrame and the number of duplicates removed.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to clean.\n",
    "        subset_columns (list): List of columns to consider for duplicates.\n",
    "        \n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, int]: DataFrame with duplicates removed and number of duplicates found.\n",
    "    \"\"\"\n",
    "    # Nombre de lignes avant suppression\n",
    "    initial_count = df.height\n",
    "\n",
    "    # Supprimer les doublons selon les colonnes spÃ©cifiÃ©es\n",
    "    df_clean = df.unique(subset=subset_columns)\n",
    "\n",
    "    # Nombre de doublons supprimÃ©s\n",
    "    nb_duplicates = initial_count - df_clean.height\n",
    "\n",
    "    return df_clean, nb_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373ce80",
   "metadata": {},
   "source": [
    "## Remove spacial characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8a391adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_special_characters(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Remove special characters from a specified text column using regex.\n",
    "\n",
    "#     Args:\n",
    "#         df (pl.DataFrame): Input Polars DataFrame.\n",
    "#         column_name (str): Name of the text column to clean.\n",
    "#         keep (str): Optional string of characters to preserve (e.g., \".,\" to keep dots and commas).\n",
    "\n",
    "#     Returns:\n",
    "#         pl.DataFrame: New DataFrame with cleaned text in the specified column.\n",
    "#     \"\"\"\n",
    "\n",
    "#     keep = \"'â€™\"\n",
    "\n",
    "#     # Pattern to remove emails\n",
    "#     email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "\n",
    "#     # Pattern to remove unwanted characters\n",
    "#     chars_pattern = rf\"[^\\w\\s{re.escape(keep)}]\"\n",
    "\n",
    "#     # Pattern to remove hashtags (#word)\n",
    "#     hashtag_pattern = r\"#\\S+\"\n",
    "\n",
    "#     # Apostrophe\n",
    "#     apostrophe_pattern = r\"(?<!n)'|'(?!t)\"\n",
    "\n",
    "#     # Pattern to replace apostrophes with a space\n",
    "#     #apostrophe_pattern = r\"(?<![A-Za-z])[â€™']|[â€™'](?![A-Za-z])\"\n",
    "\n",
    "#     # Pattern to remove url\n",
    "#     url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "\n",
    "   \n",
    "#     def clean_text(text):\n",
    "#         # 1. Remove emails\n",
    "#         text = re.sub(email_pattern, \"\", text)\n",
    "#         # 2. Remove URLs\n",
    "#         text = re.sub(url_pattern, \"\", text)\n",
    "#         # 3. Remove hashtags\n",
    "#         text = re.sub(hashtag_pattern, \"\", text)\n",
    "#         # 4. Keep apostrophes between two letters\n",
    "#         text = re.sub(apostrophe_pattern, \" \", text)\n",
    "#         # 4.1 Replace slashes with spaces\n",
    "#         text = text.replace(\"/\", \" \")\n",
    "#         # 4.1 Replace hyphen with spaces\n",
    "#         text = text.replace(\"-\", \" \")\n",
    "#         # 5. Remove unwanted characters\n",
    "#         text = re.sub(chars_pattern, \"\", text)\n",
    "#         # 6. Clean multiple spaces\n",
    "#         text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#         return text\n",
    "\n",
    "#     df_cleaned = df.with_columns(\n",
    "#         pl.col(column_name).map_elements(clean_text).alias(column_name)\n",
    "#     )\n",
    "\n",
    "#     return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b2bc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des contractions courantes Ã  conserver\n",
    "CONTRACTIONS = [\n",
    "    \"aren't\",\"can't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\n",
    "    \"haven't\",\"he's\",\"I'm\",\"I've\",\"isn't\",\"it's\",\"she's\",\n",
    "    \"shouldn't\",\"that's\",\"there's\",\"they're\",\"they've\",\n",
    "    \"we're\",\"we've\",\"weren't\",\"what's\",\"where's\",\"who's\",\"won't\",\"wouldn't\",\"you're\",\"you've\",\n",
    "\n",
    "    \"arenâ€™t\",\"canâ€™t\",\"couldnâ€™t\",\"didnâ€™t\",\"doesnâ€™t\",\"donâ€™t\",\"hadnâ€™t\",\"hasnâ€™t\",\n",
    "    \"havenâ€™t\",\"heâ€™s\",\"Iâ€™m\",\"Iâ€™ve\",\"isnâ€™t\",\"itâ€™s\",\"sheâ€™s\",\n",
    "    \"shouldnâ€™t\",\"thatâ€™s\",\"thereâ€™s\",\"theyâ€™re\",\"theyâ€™ve\",\n",
    "    \"weâ€™re\",\"weâ€™ve\",\"werenâ€™t\",\"whatâ€™s\",\"whereâ€™s\",\"whoâ€™s\",\"wonâ€™t\",\"wouldnâ€™t\",\"youâ€™re\",\"youâ€™ve\"\n",
    "]\n",
    "\n",
    "CONTRACTIONS_PATTERN = re.compile(\n",
    "    r'\\b(' + '|'.join(re.escape(c) for c in CONTRACTIONS) + r')\\b', re.IGNORECASE\n",
    ")\n",
    "\n",
    "def remove_special_characters(df: pl.DataFrame, column_name: str, keep: str = \"\") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean text column while preserving English contractions,\n",
    "    removing possessive apostrophes, emails, URLs, hashtags, unwanted chars,\n",
    "    replacing / and - with space.\n",
    "    \"\"\"\n",
    "\n",
    "    # Emails, URLs, hashtags\n",
    "    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    hashtag_pattern = r\"#\\S+\"\n",
    "\n",
    "    # Possessive apostrophes ('s or â€™s)\n",
    "    possessive_pattern = r\"(\\w)[â€™']s\\b\"\n",
    "\n",
    "    # Other apostrophes (to be removed) except contractions\n",
    "    apostrophe_pattern = r\"[â€™']\"\n",
    "\n",
    "    # Unwanted chars except letters, digits, spaces, keep chars, and '\n",
    "    chars_pattern = rf\"[^\\w\\s{re.escape(keep)}']\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        # Preserve contractions\n",
    "        contractions_map = {}\n",
    "        def replace_contraction(m):\n",
    "            key = f\"__CONTRACTION_{len(contractions_map)}__\"\n",
    "            contractions_map[key] = m.group(0)\n",
    "            return key\n",
    "\n",
    "        text = CONTRACTIONS_PATTERN.sub(replace_contraction, text)\n",
    "\n",
    "        # 1. Remove emails, URLs, hashtags\n",
    "        text = re.sub(email_pattern, \"\", text)\n",
    "        text = re.sub(url_pattern, \"\", text)\n",
    "        text = re.sub(hashtag_pattern, \"\", text)\n",
    "\n",
    "        # 2. Remove possessive apostrophes\n",
    "        text = re.sub(possessive_pattern, r\"\\1 s\", text)\n",
    "\n",
    "        # 3. Remove remaining apostrophes\n",
    "        text = re.sub(apostrophe_pattern, \" \", text)\n",
    "\n",
    "        # 4. Replace / and - with spaces\n",
    "        text = text.replace(\"/\", \" \").replace(\"-\", \" \")\n",
    "\n",
    "        # 5. Remove unwanted characters\n",
    "        text = re.sub(chars_pattern, \" \", text)\n",
    "\n",
    "        # 6. Clean multiple spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        # 7. Restore contractions\n",
    "        for k, v in contractions_map.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        # HTML artefacts\n",
    "        text = re.sub(r\"strong\\s+text\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        # with abreviation\n",
    "        text = re.sub(r\"\\bw\\b\", \"with\", text, flags=re.IGNORECASE)\n",
    "\n",
    "        return text\n",
    "\n",
    "    df_cleaned = df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text).alias(column_name)\n",
    "    )\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ceaebaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Contact me at or visit\n",
      "1: Check or mail me at\n",
      "2: Learning and is fun\n",
      "3: I'm happy it's amazing don't worry he can't come she's here we've done it Youpi Help LOL\n",
      "4: John s book teacher s notes O Reilly s guide my friend s dog\n",
      "5: This is a lone apostrophe alone also\n",
      "6: Hello world well known concepts 12 05 2024 a b c\n",
      "7: Keep commas periods And exclamations\n",
      "8: CafÃ© rÃ©sumÃ© niÃ±o faÃ§ade naÃ¯ve coÃ¶perate emoji symbol hey ho\n",
      "9: She's John s friend s dog cat don't forget Visit\n",
      "10: She d be happy I ll be there in a minute but i wouldn't do this if i were you\n",
      "11: Does accessibleGO have ANY option for airline travel First Class\n",
      "12: It's I've I'm\n",
      "13: Iâ€™m traveling to Maui in late July does anyone have recommendations on hotels etc that have handicap accessible pools I use a scooter walker can not do steps stairs very well without my braces arenâ€™t water proof lol\n",
      "14: Anyone with private pool\n",
      "15: Tim\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "df_test_complet = pl.DataFrame({\n",
    "    \"text\": [\n",
    "        # Emails / URLs\n",
    "        \"Contact me at test@example.com or visit https://example.com!\",\n",
    "        \"Check www.site.com/path or mail me at user.name+tag@domain.co.uk\",\n",
    "        \n",
    "        # Hashtags\n",
    "        \"Learning #Python3 and #DataScience is fun!\",\n",
    "\n",
    "        # Contractions Ã  garder\n",
    "        \"I'm happy, it's amazing, don't worry, he can't come, she's here, we've done it. Youpi!Help  ? LOL\",\n",
    "        \n",
    "        # Possessifs Ã  transformer\n",
    "        \"John's book, teacher's notes, O'Reilly's guide, my friend's dog.\",\n",
    "\n",
    "        # Apostrophes isolÃ©es\n",
    "        \"This ' is a lone apostrophe, 'alone' also.\",\n",
    "\n",
    "        # Slashes / tirets\n",
    "        \"Hello/world, well-known concepts, 12/05/2024, a/b/c\",\n",
    "        \n",
    "        # Ponctuation Ã  garder\n",
    "        \"Keep commas, periods. And exclamations!\",\n",
    "\n",
    "        # Accents et Unicode\n",
    "        \"CafÃ©, rÃ©sumÃ©, niÃ±o, faÃ§ade, naÃ¯ve, coÃ¶perate, ðŸ˜Š emoji,Â©symbol hey,ho\",\n",
    "\n",
    "        # Combinaison complexe\n",
    "        \"She's John's friend's dog/cat, don't forget! Visit www.site.com/path.\",\n",
    "\n",
    "        \"She'd be happy.I'll be there in a minute but i wouldn't do this if i were you\",\n",
    "\n",
    "        \"strong textDoes accessibleGO have ANY option for airline travel First Class\",\n",
    "\n",
    "        \"It's I've I'm\",\n",
    "        \"Iâ€™m traveling to Maui in late July, does anyone have recommendations on hotels, etc that have handicap accessible pools? I use a scooter/walker, can not do steps/stairs very well without my braces (arenâ€™t water proof lol).\",\n",
    "        \"Anyone w private pool ?\",\n",
    "        \"Tim timshe52@Gmail.com\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "# --- Test de la fonction ---\n",
    "df_cleaned = remove_special_characters(df_test_complet, \"text\")\n",
    "# Afficher toutes les lignes\n",
    "for i, row in enumerate(df_cleaned[\"text\"]):\n",
    "    print(f\"{i}: {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f61e8e",
   "metadata": {},
   "source": [
    "## Convert numbers to letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4310662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_words(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert all numbers in a text column into words using num2words.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame.\n",
    "        column_name (str): Name of the text column to process.\n",
    "        lang (str): Language code (e.g., 'en' or 'fr').\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: New DataFrame with numbers replaced by words.\n",
    "    \"\"\"\n",
    "    def convert_numbers(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "\n",
    "        def replace_number(m):\n",
    "            num_text = num2words(int(m.group()))\n",
    "            left = ' ' if m.start() > 0 and text[m.start()-1].isalnum() else ''\n",
    "            right = ' ' if m.end() < len(text) and text[m.end():m.end()+1].isalnum() else ''\n",
    "            return f\"{left}{num_text}{right}\"\n",
    "\n",
    "        return re.sub(r'\\d+', replace_number, text)\n",
    "\n",
    "    df_converted = df.with_columns(\n",
    "        pl.col(column_name).map_elements(convert_numbers).alias(column_name)\n",
    "    )\n",
    "\n",
    "    return df_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7955e",
   "metadata": {},
   "source": [
    "## Languages and translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2e3d6",
   "metadata": {},
   "source": [
    "### Languages detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5ed3ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_parallel(df: pl.DataFrame, column_name: str, num_threads: int = 4) -> tuple[pl.DataFrame, int]:\n",
    "    \"\"\"\n",
    "    Detect the language of a text column in a Polars DataFrame using langid in parallel.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame.\n",
    "        column_name (str): Name of the text column to process.\n",
    "        num_threads (int): Number of threads to use for parallel processing (default=4).\n",
    "\n",
    "    Returns:\n",
    "        tuple[pl.DataFrame, int]: \n",
    "            - DataFrame with an added column 'detected_lang' containing language codes.\n",
    "            - Number of sentences not detected as English ('en').\n",
    "    \"\"\"\n",
    "    def detect_lang(text: str) -> str:\n",
    "        \"\"\"Return the language code of a single text using langid.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return None\n",
    "        lang, _ = langid.classify(text)\n",
    "        return lang\n",
    "\n",
    "    # Convert the Polars column to a Python list\n",
    "    texts = df[column_name].to_list()\n",
    "\n",
    "    # Parallel processing with ThreadPoolExecutor\n",
    "    all_langs = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for result in tqdm(executor.map(detect_lang, texts), total=len(texts), desc=\"Language detection\"):\n",
    "            all_langs.append(result)\n",
    "\n",
    "    # Return new DataFrame with added column\n",
    "    df_result = df.with_columns(pl.Series(\"detected_lang\", all_langs))\n",
    "\n",
    "    nb_non_english = sum(lang != \"en\" for lang in all_langs if lang is not None)\n",
    "\n",
    "    return df_result,nb_non_english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40e2c5",
   "metadata": {},
   "source": [
    "### Translation in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfaf2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_non_english_threadsafe(df: pl.DataFrame,\n",
    "                                     column_name: str,\n",
    "                                     detected_lang_col: str = \"detected_lang\",\n",
    "                                     num_threads: int = 4) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Thread-safe translation: one translator per thread.\n",
    "    \"\"\"\n",
    "    def translate_one(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        try:\n",
    "            translator = GoogleTranslator(source='auto', target='en')  # local traductor\n",
    "            return translator.translate(text)\n",
    "        except Exception as e:\n",
    "            return f\"[ERROR: {e}]\"\n",
    "\n",
    "    indices_to_translate = []\n",
    "    texts_to_translate = []\n",
    "    for i, (text, lang) in enumerate(zip(df[column_name].to_list(), df[detected_lang_col].to_list())):\n",
    "        if lang != 'en':\n",
    "            indices_to_translate.append(i)\n",
    "            texts_to_translate.append(text)\n",
    "\n",
    "    translated_texts = df[column_name].to_list()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for idx, translation in zip(\n",
    "            indices_to_translate,\n",
    "            tqdm(executor.map(translate_one, texts_to_translate),\n",
    "                 total=len(texts_to_translate),\n",
    "                 desc=\"Translating non-English reviews\")\n",
    "        ):\n",
    "            translated_texts[idx] = translation\n",
    "\n",
    "    return df.with_columns(pl.Series(column_name, translated_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e047051",
   "metadata": {},
   "source": [
    "## Correction of spelling error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c69a1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling_symspell(\n",
    "    df: pl.DataFrame,\n",
    "    column_name: str,\n",
    "    batch_size: int = 10000,\n",
    "    n_threads: int = 4,\n",
    "    max_edit_distance: int = 2,\n",
    "    dictionary_path: str = \"frequency_dictionary_en_82_765.txt\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Correct spelling using SymSpell (fast) on a Polars DataFrame text column.\n",
    "    Supports batching and multithreading.\n",
    "\n",
    "    Args:\n",
    "        df: Polars DataFrame\n",
    "        column_name: Name of the text column\n",
    "        batch_size: Number of rows per batch\n",
    "        n_threads: Number of parallel threads\n",
    "        max_edit_distance: Max edit distance for corrections\n",
    "        dictionary_path: Path to SymSpell frequency dictionary\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with corrected text\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize SymSpell \n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=max_edit_distance, prefix_length=7)\n",
    "    if not os.path.exists(dictionary_path):\n",
    "        raise FileNotFoundError(f\"Dictionary not found: {dictionary_path}\")\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "    # Function to correct a single review \n",
    "    def fix_text(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        corrected_words = []\n",
    "        for word in text.split():\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance)\n",
    "            corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "        return \" \".join(corrected_words)\n",
    "\n",
    "    # Split dataframe into batches\n",
    "    batches = [df.slice(i, batch_size) for i in range(0, len(df), batch_size)]\n",
    "    corrected_reviews = []\n",
    "\n",
    "    # Parallel batch processing \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        for batch_result in tqdm(\n",
    "            executor.map(lambda b: [fix_text(t) for t in b[column_name]], batches),\n",
    "            total=len(batches),\n",
    "            desc=\"Spell-checking with SymSpell\"\n",
    "        ):\n",
    "            corrected_reviews.extend(batch_result)\n",
    "\n",
    "    # --- Return DataFrame with corrected column ---\n",
    "    df_corrected = df.with_columns(\n",
    "        pl.Series(name=column_name, values=corrected_reviews)\n",
    "    )\n",
    "\n",
    "    return df_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e5420",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9116c0",
   "metadata": {},
   "source": [
    "This script automatically runs the entire pipeline on a .csv file and saves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83a5c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(input_csv: str, column_name: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Apply the full preprocessing pipeline to the given CSV file.\n",
    "    \"\"\"\n",
    "    df = pl.read_csv(input_csv)\n",
    "    logger.info(f\"DataFrame {os.path.splitext(os.path.basename(input_csv))[0]} loaded : {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "    df,nb_missing_values = clean_missing_values(df, column_name)\n",
    "    logger.info(f\"{nb_missing_values} missing reviews detected and cleaned.\")\n",
    "    df,nb_duplicates = remove_duplicates(df, column_name)\n",
    "    logger.info(f\"{nb_duplicates} duplicated reviews detected and cleaned.\")\n",
    "    df = numbers_to_words(df, column_name)\n",
    "    logger.info(f\"Numerical numbers converted to string numbers.\")\n",
    "    df = remove_special_characters(df, column_name)\n",
    "    logger.info(f\" Special characters removed.\")\n",
    "    df,nb_to_translate = detect_language_parallel(df, column_name, NUM_THREAD)\n",
    "    logger.info(f\"{nb_to_translate} reviews are potentially not in english.\")\n",
    "    df = translate_non_english_threadsafe(df, column_name, \"detected_lang\", NUM_THREAD)\n",
    "    logger.info(f\"{nb_to_translate} have been translated in english.\")\n",
    "    # df = correct_spelling_symspell(\n",
    "    #     df,\n",
    "    #     column_name=column_name,\n",
    "    #     batch_size=10000,\n",
    "    #     n_threads=NUM_THREAD,\n",
    "    #     max_edit_distance=2,\n",
    "    #     dictionary_path=\"../data/original/frequency_dictionary_en_82_765.txt\"\n",
    "    # )\n",
    "    df.write_csv(output_csv)\n",
    "    logger.info(f\"Cleaned Dataframe saved at {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea87689e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:43:18,752 - INFO - DataFrame data_accessiblego loaded : 2225 rows x 4 columns\n",
      "2025-11-18 12:43:18,752 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 12:43:18,763 - INFO - 329 duplicated reviews detected and cleaned.\n",
      "2025-11-18 12:43:18,819 - INFO - Numerical numbers converted to string numbers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_accessiblego.csv -> data_accessiblego_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:43:19,254 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1896/1896 [00:00<00:00, 2242.30it/s]\n",
      "2025-11-18 12:43:20,885 - INFO - 8 reviews are potentially not in english.\n",
      "Translating non-English reviews:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:00<00:00, 13.77it/s]\n",
      "2025-11-18 12:43:21,782 - INFO - 8 have been translated in english.\n",
      "2025-11-18 12:43:21,786 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_accessiblego_cleaned.csv\n",
      "2025-11-18 12:43:21,817 - INFO - DataFrame data_activities_reviews loaded : 46377 rows x 4 columns\n",
      "2025-11-18 12:43:21,817 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 12:43:21,828 - INFO - 503 duplicated reviews detected and cleaned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_activities_reviews.csv -> data_activities_reviews_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:43:22,225 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 12:43:26,160 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45874/45874 [00:35<00:00, 1310.37it/s]\n",
      "2025-11-18 12:44:02,531 - INFO - 3770 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 3770/3771 [02:06<00:00, 29.92it/s]\n",
      "2025-11-18 12:46:08,618 - INFO - 3770 have been translated in english.\n",
      "2025-11-18 12:46:08,626 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_activities_reviews_cleaned.csv\n",
      "2025-11-18 12:46:08,638 - INFO - DataFrame data_airline_reviews_1 loaded : 3701 rows x 20 columns\n",
      "2025-11-18 12:46:08,648 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 12:46:08,652 - INFO - 9 duplicated reviews detected and cleaned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_airline_reviews_1.csv -> data_airline_reviews_1_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:46:08,811 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 12:46:09,680 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3692/3692 [00:02<00:00, 1234.18it/s]\n",
      "2025-11-18 12:46:12,793 - INFO - 0 reviews are potentially not in english.\n",
      "Translating non-English reviews: 0it [00:00, ?it/s]\n",
      "2025-11-18 12:46:12,797 - INFO - 0 have been translated in english.\n",
      "2025-11-18 12:46:12,805 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_airline_reviews_1_cleaned.csv\n",
      "2025-11-18 12:46:12,845 - INFO - DataFrame data_airline_reviews_2 loaded : 8100 rows x 18 columns\n",
      "2025-11-18 12:46:12,845 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 12:46:12,845 - INFO - 1 duplicated reviews detected and cleaned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_airline_reviews_2.csv -> data_airline_reviews_2_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:46:13,168 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 12:46:14,957 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8099/8099 [00:06<00:00, 1189.38it/s]\n",
      "2025-11-18 12:46:21,915 - INFO - 5 reviews are potentially not in english.\n",
      "Translating non-English reviews:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:01<00:00,  2.66it/s]\n",
      "2025-11-18 12:46:23,434 - INFO - 5 have been translated in english.\n",
      "2025-11-18 12:46:23,448 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_airline_reviews_2_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_booking.csv -> data_booking_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:46:24,288 - INFO - DataFrame data_booking loaded : 1832776 rows x 17 columns\n",
      "2025-11-18 12:46:24,320 - INFO - 528473 missing reviews detected and cleaned.\n",
      "2025-11-18 12:46:24,443 - INFO - 2653 duplicated reviews detected and cleaned.\n",
      "2025-11-18 12:46:40,657 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 12:48:22,191 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1301650/1301650 [15:37<00:00, 1388.76it/s]\n",
      "2025-11-18 13:05:00,673 - INFO - 5596 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 5595/5596 [03:09<00:00, 29.49it/s]\n",
      "2025-11-18 13:08:11,310 - INFO - 5596 have been translated in english.\n",
      "2025-11-18 13:08:11,691 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_booking_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_european_hotel_reviews.csv -> data_european_hotel_reviews_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:08:12,045 - INFO - DataFrame data_european_hotel_reviews loaded : 515738 rows x 17 columns\n",
      "2025-11-18 13:08:12,058 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:08:12,117 - INFO - 16450 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:08:15,025 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:08:41,418 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499288/499288 [05:51<00:00, 1418.92it/s]\n",
      "2025-11-18 13:14:43,888 - INFO - 9052 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9052/9053 [10:06<00:00, 14.93it/s]\n",
      "2025-11-18 13:24:50,402 - INFO - 9052 have been translated in english.\n",
      "2025-11-18 13:24:50,577 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_european_hotel_reviews_cleaned.csv\n",
      "2025-11-18 13:24:50,612 - INFO - DataFrame data_european_restaurant_reviews loaded : 1502 rows x 7 columns\n",
      "2025-11-18 13:24:50,614 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:24:50,618 - INFO - 76 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:24:50,643 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:24:50,785 - INFO -  Special characters removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_european_restaurant_reviews.csv -> data_european_restaurant_reviews_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1426/1426 [00:01<00:00, 1263.17it/s]\n",
      "2025-11-18 13:24:51,968 - INFO - 5 reviews are potentially not in english.\n",
      "Translating non-English reviews:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  4.28it/s]\n",
      "2025-11-18 13:24:52,902 - INFO - 5 have been translated in english.\n",
      "2025-11-18 13:24:52,906 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_european_restaurant_reviews_cleaned.csv\n",
      "2025-11-18 13:24:52,969 - INFO - DataFrame data_hotel_reviews_1 loaded : 35912 rows x 20 columns\n",
      "2025-11-18 13:24:52,985 - INFO - 20 missing reviews detected and cleaned.\n",
      "2025-11-18 13:24:53,005 - INFO - 1493 duplicated reviews detected and cleaned.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_hotel_reviews_1.csv -> data_hotel_reviews_1_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:24:53,253 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:24:55,712 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34399/34399 [00:28<00:00, 1200.67it/s]\n",
      "2025-11-18 13:25:25,081 - INFO - 1599 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1614/1615 [01:57<00:00, 13.76it/s]\n",
      "2025-11-18 13:27:22,447 - INFO - 1599 have been translated in english.\n",
      "2025-11-18 13:27:22,481 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_hotel_reviews_1_cleaned.csv\n",
      "2025-11-18 13:27:22,572 - INFO - DataFrame data_hotel_reviews_2 loaded : 10000 rows x 26 columns\n",
      "2025-11-18 13:27:22,582 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:27:22,586 - INFO - 18 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:27:22,680 - INFO - Numerical numbers converted to string numbers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_hotel_reviews_2.csv -> data_hotel_reviews_2_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:27:23,575 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9982/9982 [00:06<00:00, 1507.19it/s]\n",
      "2025-11-18 13:27:30,648 - INFO - 107 reviews are potentially not in english.\n",
      "Translating non-English reviews:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 107/108 [00:08<00:00, 12.32it/s]\n",
      "2025-11-18 13:27:39,364 - INFO - 107 have been translated in english.\n",
      "2025-11-18 13:27:39,392 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_hotel_reviews_2_cleaned.csv\n",
      "2025-11-18 13:27:39,585 - INFO - DataFrame data_hotel_reviews_3 loaded : 10000 rows x 26 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_hotel_reviews_3.csv -> data_hotel_reviews_3_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:27:39,596 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:27:39,607 - INFO - 230 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:27:39,784 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:27:41,441 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9770/9770 [00:03<00:00, 2992.38it/s] \n",
      "2025-11-18 13:27:49,469 - INFO - 17 reviews are potentially not in english.\n",
      "Translating non-English reviews:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 16/17 [00:02<00:00,  5.72it/s]\n",
      "2025-11-18 13:27:52,298 - INFO - 17 have been translated in english.\n",
      "2025-11-18 13:27:52,353 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_hotel_reviews_3_cleaned.csv\n",
      "2025-11-18 13:27:52,369 - INFO - DataFrame data_restaurant_reviews_1 loaded : 1000 rows x 3 columns\n",
      "2025-11-18 13:27:52,369 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:27:52,375 - INFO - 4 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:27:52,380 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:27:52,400 - INFO -  Special characters removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_restaurant_reviews_1.csv -> data_restaurant_reviews_1_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 996/996 [00:00<00:00, 1513.67it/s]\n",
      "2025-11-18 13:27:53,100 - INFO - 27 reviews are potentially not in english.\n",
      "Translating non-English reviews:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 26/27 [00:05<00:00,  4.71it/s]\n",
      "2025-11-18 13:27:58,635 - INFO - 27 have been translated in english.\n",
      "2025-11-18 13:27:58,635 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_restaurant_reviews_1_cleaned.csv\n",
      "2025-11-18 13:27:58,667 - INFO - DataFrame data_restaurant_reviews_2 loaded : 10000 rows x 9 columns\n",
      "2025-11-18 13:27:58,672 - INFO - 45 missing reviews detected and cleaned.\n",
      "2025-11-18 13:27:58,675 - INFO - 591 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:27:58,786 - INFO - Numerical numbers converted to string numbers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_restaurant_reviews_2.csv -> data_restaurant_reviews_2_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:27:59,616 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9364/9364 [00:06<00:00, 1386.28it/s]\n",
      "2025-11-18 13:28:06,760 - INFO - 182 reviews are potentially not in english.\n",
      "Translating non-English reviews:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 183/184 [00:14<00:00, 12.84it/s]\n",
      "2025-11-18 13:28:21,041 - INFO - 182 have been translated in english.\n",
      "2025-11-18 13:28:21,045 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_restaurant_reviews_2_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_tripadvisor_hotel_reviews.csv -> data_tripadvisor_hotel_reviews_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:28:22,412 - INFO - DataFrame data_tripadvisor_hotel_reviews loaded : 878561 rows x 10 columns\n",
      "2025-11-18 13:28:22,419 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 13:28:22,500 - INFO - 988 duplicated reviews detected and cleaned.\n",
      "2025-11-18 13:28:44,842 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 13:31:47,397 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877573/877573 [13:26<00:00, 1088.62it/s]  \n",
      "2025-11-18 13:48:16,583 - INFO - 106928 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 106928/106929 [2:21:17<00:00, 12.61it/s]  \n",
      "2025-11-18 16:09:46,253 - INFO - 106928 have been translated in english.\n",
      "2025-11-18 16:09:50,220 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_tripadvisor_hotel_reviews_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_twitter.csv -> data_twitter_cleaned.csv | column: review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:09:51,742 - INFO - DataFrame data_twitter loaded : 10000 rows x 6 columns\n",
      "2025-11-18 16:09:51,789 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 16:09:51,820 - INFO - 0 duplicated reviews detected and cleaned.\n",
      "2025-11-18 16:09:52,146 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 16:09:58,525 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:39<00:00, 254.16it/s] \n",
      "2025-11-18 16:10:49,866 - INFO - 6 reviews are potentially not in english.\n",
      "Translating non-English reviews:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 5/6 [00:01<00:00,  4.23it/s]\n",
      "2025-11-18 16:10:51,141 - INFO - 6 have been translated in english.\n",
      "2025-11-18 16:10:51,165 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_twitter_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data_yelp_reviews.csv -> data_yelp_reviews_cleaned.csv | column: text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:11:14,004 - INFO - DataFrame data_yelp_reviews loaded : 6990280 rows x 9 columns\n",
      "2025-11-18 16:11:14,054 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 16:11:16,815 - INFO - 16153 duplicated reviews detected and cleaned.\n",
      "2025-11-18 16:17:04,046 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 16:52:19,842 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6974127/6974127 [2:10:13<00:00, 892.54it/s]  \n",
      "2025-11-18 19:30:50,642 - INFO - 7251 reviews are potentially not in english.\n",
      "Translating non-English reviews: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 7268/7269 [08:34<00:00, 14.13it/s]\n",
      "2025-11-18 19:39:52,756 - INFO - 7251 have been translated in english.\n",
      "2025-11-18 19:40:00,207 - INFO - Cleaned Dataframe saved at ..\\data\\processed\\data_clean\\data_yelp_reviews_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dossiers\n",
    "input_folder = Path(\"../data/original/dataset\")\n",
    "output_folder = Path(\"../data/processed/data_clean\")\n",
    "output_folder.mkdir(parents=True, exist_ok=True)  # crÃ©er le dossier si besoin\n",
    "\n",
    "# Lancer le pipeline sur tous les fichiers CSV\n",
    "for file_path in input_folder.glob(\"*.csv\"):\n",
    "    # Nom de fichier sans extension\n",
    "    stem = file_path.stem  # ex: data_airline_reviews_1\n",
    "    \n",
    "    # Choix de la colonne Ã  nettoyer\n",
    "    column_name = \"review\"\n",
    "    if \"yelp\" in stem.lower():\n",
    "        column_name = \"text\"\n",
    "    \n",
    "    output_file = output_folder / f\"{stem}_cleaned.csv\"\n",
    "    \n",
    "    preprocess_pipeline(str(file_path), column_name, str(output_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "004276f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 19:40:00,354 - INFO - DataFrame data_airline_reviews_1 loaded : 3701 rows x 20 columns\n",
      "2025-11-18 19:40:00,407 - INFO - 0 missing reviews detected and cleaned.\n",
      "2025-11-18 19:40:00,459 - INFO - 9 duplicated reviews detected and cleaned.\n",
      "2025-11-18 19:40:00,743 - INFO - Numerical numbers converted to string numbers.\n",
      "2025-11-18 19:40:02,299 - INFO -  Special characters removed.\n",
      "Language detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3692/3692 [00:05<00:00, 700.73it/s]\n",
      "2025-11-18 19:40:08,586 - INFO - 0 reviews are potentially not in english.\n",
      "Translating non-English reviews: 0it [00:00, ?it/s]\n",
      "2025-11-18 19:40:08,603 - INFO - 0 have been translated in english.\n",
      "2025-11-18 19:40:08,613 - INFO - Cleaned Dataframe saved at ../data/processed/data_clean/data_airline_reviews_1_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     preprocess_pipeline(\"../data/original/dataset/data_airline_reviews_1.csv\",\n",
    "                         \"review\",\n",
    "                         \"../data/processed/data_clean/data_airline_reviews_1_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
