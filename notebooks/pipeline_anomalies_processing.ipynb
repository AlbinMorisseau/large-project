{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2536e8",
   "metadata": {},
   "source": [
    "# Pipeline to manage anomalies processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba5131a",
   "metadata": {},
   "source": [
    "Before using any dataset, it is necessary to clean the data to obtain a clear, formatted, and complete dataset. This tedious step is the first step in data analysis.\n",
    "\n",
    "In this notebook, there are building blocks (functions) that can be used later to perform the identified preprocessing steps.\n",
    "\n",
    "- Remove missing values\n",
    "- Remove duplicates\n",
    "- Remove special characters\n",
    "- Convert numbers to letters\n",
    "- Identify the language of the review and translate it if necessary\n",
    "- Correct spelling errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7caac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from num2words import num2words\n",
    "import langid\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from deep_translator import GoogleTranslator\n",
    "from spellchecker import SpellChecker\n",
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98210f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "NUM_THREAD = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4736d",
   "metadata": {},
   "source": [
    "## Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735da6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows from a DataFrame where the specified column has missing values,\n",
    "    and return the cleaned DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to clean.\n",
    "        column_name (str): Column to check for missing values.\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Drop rows where the specified column is null\n",
    "    df_clean = df.drop_nulls(subset=[column_name])\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17059c65",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8caa955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pl.DataFrame, subset_columns: list) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from a DataFrame based on specified columns,\n",
    "    and return the cleaned DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): The DataFrame to clean.\n",
    "        subset_columns (list): List of columns to consider for duplicates.\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with duplicates removed.\n",
    "    \"\"\"\n",
    "    # Drop duplicates based on the subset of columns\n",
    "    df_clean = df.unique(subset=subset_columns)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373ce80",
   "metadata": {},
   "source": [
    "## Remove spacial characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a391adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(df: pl.DataFrame, column_name: str, keep: str = \"\") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove special characters from a specified text column using regex.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input Polars DataFrame.\n",
    "        column_name (str): Name of the text column to clean.\n",
    "        keep (str): Optional string of characters to preserve (e.g., \".,\" to keep dots and commas).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: New DataFrame with cleaned text in the specified column.\n",
    "    \"\"\"\n",
    "    # Build regex dynamically: allow alphanumeric, space, underscore, and chosen extra characters\n",
    "    pattern = rf\"[^\\w\\s{re.escape(keep)}]\"\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text  # ignore empty or non-string\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    df_cleaned = df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text).alias(column_name)\n",
    "    )\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f61e8e",
   "metadata": {},
   "source": [
    "## Convert numbers to letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbers_to_words(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert all numbers in a text column into words using num2words.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame.\n",
    "        column_name (str): Name of the text column to process.\n",
    "        lang (str): Language code (e.g., 'en' or 'fr').\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: New DataFrame with numbers replaced by words.\n",
    "    \"\"\"\n",
    "    def convert_numbers(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        # Replace every number with its text version\n",
    "        return re.sub(r'\\b\\d+\\b', lambda m: num2words(int(m.group())), text)\n",
    "\n",
    "    df_converted = df.with_columns(\n",
    "        pl.col(column_name).map_elements(convert_numbers).alias(column_name)\n",
    "    )\n",
    "\n",
    "    return df_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7955e",
   "metadata": {},
   "source": [
    "## Languages and translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b2e3d6",
   "metadata": {},
   "source": [
    "### Languages detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_parallel(df: pl.DataFrame, column_name: str, num_threads: int = 4) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect the language of a text column in a Polars DataFrame using langid in parallel.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame.\n",
    "        column_name (str): Name of the text column to process.\n",
    "        num_threads (int): Number of threads to use for parallel processing (default=4).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: New DataFrame with an added column 'detected_lang' containing language codes.\n",
    "    \"\"\"\n",
    "    def detect_lang(text: str) -> str:\n",
    "        \"\"\"Return the language code of a single text using langid.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return None\n",
    "        lang, _ = langid.classify(text)\n",
    "        return lang\n",
    "\n",
    "    # Convert the Polars column to a Python list\n",
    "    texts = df[column_name].to_list()\n",
    "\n",
    "    # Parallel processing with ThreadPoolExecutor\n",
    "    all_langs = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for result in tqdm(executor.map(detect_lang, texts), total=len(texts), desc=\"Language detection\"):\n",
    "            all_langs.append(result)\n",
    "\n",
    "    # Return new DataFrame with added column\n",
    "    df_result = df.with_columns(pl.Series(\"detected_lang\", all_langs))\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40e2c5",
   "metadata": {},
   "source": [
    "### Translation in english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_non_english_threadsafe(df: pl.DataFrame,\n",
    "                                     column_name: str,\n",
    "                                     detected_lang_col: str = \"detected_lang\",\n",
    "                                     num_threads: int = 4) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Thread-safe translation: one translator per thread.\n",
    "    \"\"\"\n",
    "    def translate_one(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        try:\n",
    "            translator = GoogleTranslator(source='auto', target='en')  # <— crée un traducteur local\n",
    "            return translator.translate(text)\n",
    "        except Exception as e:\n",
    "            return f\"[ERROR: {e}]\"\n",
    "\n",
    "    indices_to_translate = []\n",
    "    texts_to_translate = []\n",
    "    for i, (text, lang) in enumerate(zip(df[column_name].to_list(), df[detected_lang_col].to_list())):\n",
    "        if lang != 'en':\n",
    "            indices_to_translate.append(i)\n",
    "            texts_to_translate.append(text)\n",
    "\n",
    "    translated_texts = df[column_name].to_list()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for idx, translation in zip(\n",
    "            indices_to_translate,\n",
    "            tqdm(executor.map(translate_one, texts_to_translate),\n",
    "                 total=len(texts_to_translate),\n",
    "                 desc=\"Translating non-English (thread-safe)\")\n",
    "        ):\n",
    "            translated_texts[idx] = translation\n",
    "\n",
    "    return df.with_columns(pl.Series(column_name, translated_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e047051",
   "metadata": {},
   "source": [
    "## Correction of spelling error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a1770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling_symspell(\n",
    "    df: pl.DataFrame,\n",
    "    column_name: str,\n",
    "    batch_size: int = 10000,\n",
    "    n_threads: int = 4,\n",
    "    max_edit_distance: int = 2,\n",
    "    dictionary_path: str = \"frequency_dictionary_en_82_765.txt\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Correct spelling using SymSpell (fast) on a Polars DataFrame text column.\n",
    "    Supports batching and multithreading.\n",
    "\n",
    "    Args:\n",
    "        df: Polars DataFrame\n",
    "        column_name: Name of the text column\n",
    "        batch_size: Number of rows per batch\n",
    "        n_threads: Number of parallel threads\n",
    "        max_edit_distance: Max edit distance for corrections\n",
    "        dictionary_path: Path to SymSpell frequency dictionary\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with corrected text\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Initialize SymSpell ---\n",
    "    sym_spell = SymSpell(max_dictionary_edit_distance=max_edit_distance, prefix_length=7)\n",
    "    if not os.path.exists(dictionary_path):\n",
    "        raise FileNotFoundError(f\"Dictionary not found: {dictionary_path}\")\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "    # --- Function to correct a single review ---\n",
    "    def fix_text(text: str) -> str:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return text\n",
    "        corrected_words = []\n",
    "        for word in text.split():\n",
    "            suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance)\n",
    "            corrected_words.append(suggestions[0].term if suggestions else word)\n",
    "        return \" \".join(corrected_words)\n",
    "\n",
    "    # --- Split dataframe into batches ---\n",
    "    batches = [df.slice(i, batch_size) for i in range(0, len(df), batch_size)]\n",
    "    corrected_reviews = []\n",
    "\n",
    "    # --- Parallel batch processing ---\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        for batch_result in tqdm(\n",
    "            executor.map(lambda b: [fix_text(t) for t in b[column_name]], batches),\n",
    "            total=len(batches),\n",
    "            desc=\"Spell-checking with SymSpell\"\n",
    "        ):\n",
    "            corrected_reviews.extend(batch_result)\n",
    "\n",
    "    # --- Return DataFrame with corrected column ---\n",
    "    df_corrected = df.with_columns(\n",
    "        pl.Series(name=column_name, values=corrected_reviews)\n",
    "    )\n",
    "\n",
    "    return df_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e5420",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9116c0",
   "metadata": {},
   "source": [
    "This script automatically runs the entire pipeline on a .csv file and saves the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(input_csv: str, column_name: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Apply the full preprocessing pipeline to the given CSV file.\n",
    "    \"\"\"\n",
    "    df = pl.read_csv(input_csv)\n",
    "    df = clean_missing_values(df, column_name)\n",
    "    df = remove_duplicates(df, column_name)\n",
    "    df = numbers_to_words(df, column_name)\n",
    "    df = remove_special_characters(df, column_name)\n",
    "    df = detect_language_parallel(df, column_name, NUM_THREAD)\n",
    "    df = translate_non_english_threadsafe(df, column_name, \"detected_lang\", NUM_THREAD)\n",
    "    df = correct_spelling_symspell(\n",
    "        df,\n",
    "        column_name=column_name,\n",
    "        batch_size=10000,\n",
    "        n_threads=NUM_THREAD,\n",
    "        max_edit_distance=2,\n",
    "        dictionary_path=\"../data/original/frequency_dictionary_en_82_765.txt\"\n",
    "    )\n",
    "    df.write_csv(output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004276f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     preprocess_pipeline(\"../data/original/Booking/val.csv\",\n",
    "                         \"review_positive\",\n",
    "                         \"../data/processed/val_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
