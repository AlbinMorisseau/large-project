{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3fdb94",
   "metadata": {},
   "source": [
    "# Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106525e",
   "metadata": {},
   "source": [
    "Once you have managed the anomalies and created a clean dataset, you now need to create a pipeline that allows you to extract three datasets based on content from a total dataset:\n",
    "- pets dataset\n",
    "- children dataset\n",
    "- disability dataset\n",
    "\n",
    "To do this, several steps must be carried out:\n",
    "- stop word removal\n",
    "- tokenize the text\n",
    "- lemmatize the text\n",
    "- extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4eb448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafc18ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/processed/all_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d447",
   "metadata": {},
   "source": [
    "## Stop word removal andd tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a93b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "def remove_stopwords(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given text column in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing text data.\n",
    "    column_name : str\n",
    "        Name of the column containing the text to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A new DataFrame with stopwords removed from the specified column.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text, return_dtype=pl.Utf8).alias(column_name)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf2477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of use\n",
    "#df_clean = remove_stopwords(df, \"review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985458e7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2788d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger SpaCy (désactiver les composants inutiles pour plus de vitesse)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def lemmatize_texts(\n",
    "    texts: List[str], \n",
    "    batch_size: int = 1000, \n",
    "    n_process: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize a list of texts using spaCy with multiprocessing.\n",
    "    \"\"\"\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        lemmatized.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def lemmatize_column_fast(\n",
    "    df: pl.DataFrame, \n",
    "    col_name: str, \n",
    "    new_col_name: str = None, \n",
    "    chunk_size: int = 5000, \n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lemmatize a Polars DataFrame column efficiently in batches with multiprocessing.\n",
    "    \"\"\"\n",
    "    new_col_name = new_col_name or f\"{col_name}_lemmatized\"\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    lemmatized_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_texts(chunk, n_process=n_process))\n",
    "\n",
    "    return df.with_columns(pl.Series(name=new_col_name, values=lemmatized_chunks))\n",
    "\n",
    "\n",
    "def lemmatize_categories(\n",
    "    categories: Dict[str, List[str]]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Lemmatize all keywords in category dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        category: lemmatize_texts(keywords, batch_size=100, n_process=1)\n",
    "        for category, keywords in categories.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9ebe",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def extract_all_categories(\n",
    "    df: pl.DataFrame,\n",
    "    col_name: str,\n",
    "    categories: Dict[str, List[str]],\n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract reviews matching any category keywords using regex filtering.\n",
    "    Runs extraction for all categories in parallel.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        The input dataframe.\n",
    "    col_name : str\n",
    "        Column name containing the (lemmatized) text.\n",
    "    categories : dict[str, list[str]]\n",
    "        Dictionary of category → list of keywords.\n",
    "    n_process : int\n",
    "        Number of threads to use.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        DataFrame with columns [\"text\", \"keywords_found\", \"category\"].\n",
    "    \"\"\"\n",
    "\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "\n",
    "    # --- Fonction interne pour traiter UNE catégorie ---\n",
    "    def process_category(category, keywords):\n",
    "        regex = r\"\\b(\" + \"|\".join(\n",
    "            re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\") \n",
    "            for kw in keywords\n",
    "        ) + r\")\\b\"\n",
    "\n",
    "        results = []\n",
    "        for text in texts:\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "            if re.search(regex, text, flags=re.IGNORECASE):\n",
    "                found = [\n",
    "                    kw for kw in keywords \n",
    "                    if re.search(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\", text, flags=re.IGNORECASE)\n",
    "                ]\n",
    "                results.append((text, \", \".join(found), category))\n",
    "        return results\n",
    "\n",
    "    # --- Lancer en parallèle sur toutes les catégories ---\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_process) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_category, cat, kws): cat\n",
    "            for cat, kws in categories.items()\n",
    "        }\n",
    "        for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Keyword extraction\"):\n",
    "            cat_results = fut.result()\n",
    "            all_results.extend(cat_results)\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No matches found for any category.\")\n",
    "        return pl.DataFrame(schema={\"text\": pl.Utf8, \"keywords_found\": pl.Utf8, \"category\": pl.Utf8})\n",
    "\n",
    "    # --- Convertir en DataFrame Polars ---\n",
    "    df_filtered = pl.DataFrame(\n",
    "        {\n",
    "            \"text\": [r[0] for r in all_results],\n",
    "            \"keywords_found\": [r[1] for r in all_results],\n",
    "            \"category\": [r[2] for r in all_results],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Extracted {df_filtered.shape[0]} matching reviews across {len(categories)} categories.\")\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c933f",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ea3906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Lemmatized Categories ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing review: 100%|██████████| 1/1 [00:36<00:00, 36.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DataFrame with Lemmatized Texts ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword extraction: 100%|██████████| 3/3 [00:00<00:00, 1389.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 116 matching reviews across 3 categories.\n",
      "\n",
      "=== Filtered Reviews ===\n",
      "shape: (116, 3)\n",
      "┌─────────────────────────────────┬────────────────┬──────────┐\n",
      "│ text                            ┆ keywords_found ┆ category │\n",
      "│ ---                             ┆ ---            ┆ ---      │\n",
      "│ str                             ┆ str            ┆ str      │\n",
      "╞═════════════════════════════════╪════════════════╪══════════╡\n",
      "│ walk dog go pueta vallarta rea… ┆ dog            ┆ pets     │\n",
      "│ awful buy year old birthday th… ┆ animal         ┆ pets     │\n",
      "│ loud loud loud take listen buy… ┆ dog            ┆ pets     │\n",
      "│ drink hot chocolate watch snow… ┆ dog            ┆ pets     │\n",
      "│ gentle leader great product fi… ┆ dog            ┆ pets     │\n",
      "│ …                               ┆ …              ┆ …        │\n",
      "│ eat pizza ice cream family      ┆ family         ┆ children │\n",
      "│ lid fold back way lid lift imm… ┆ lift           ┆ handicap │\n",
      "│ soccer ride bike weight lift    ┆ lift           ┆ handicap │\n",
      "│ still excellent old tooth prep… ┆ adapt          ┆ handicap │\n",
      "│ commercial bullshit cd mistake… ┆ adapt          ┆ handicap │\n",
      "└─────────────────────────────────┴────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pl.read_csv(\"../data/processed/all_reviews.csv\")\n",
    "    df = df.head(1000)\n",
    "    nb_process = 4\n",
    "    name_column = \"review\"\n",
    "\n",
    "    categories = {\n",
    "        \"handicap\": [\n",
    "            \"handicap\", \"wheelchair\", \"accessible\", \"braille\", \"ramp\", \"lift\", \"elevator\",\n",
    "            \"disabled\", \"barrier-free\", \"accessible toilet\", \"toilet accessible\",\n",
    "            \"mobility aid\", \"adapted\", \"hearing aid\", \"visual impairment\", \"accessible entrance\"\n",
    "        ],\n",
    "        \"pets\": [\n",
    "            \"dog\", \"cat\", \"pet\", \"animal\", \"rabbit\", \"hamster\", \"ferret\", \"bird\",\n",
    "            \"pet-friendly\", \"animals allowed\", \"dog-friendly\", \"cat-friendly\",\n",
    "            \"pet welcome\", \"pup\", \"dog bowl\"\n",
    "        ],\n",
    "        \"children\": [\n",
    "            \"child\", \"baby\", \"kid\", \"stroller\", \"son\", \"daughter\", \"toddler\",\n",
    "            \"infant\", \"playground\", \"high chair\", \"changing table\", \"family-friendly\",\n",
    "            \"childcare\", \"kids menu\", \"baby seat\", \"family\",\"baby bed\", \"cot\", \"crib\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_clean = remove_stopwords(df, name_column)\n",
    "\n",
    "    lemmatized_categories = lemmatize_categories(categories)\n",
    "    print(\"\\n=== Lemmatized Categories ===\")\n",
    "\n",
    "    df_lem = lemmatize_column_fast(df_clean, name_column, n_process=nb_process)\n",
    "    print(\"\\n=== DataFrame with Lemmatized Texts ===\")\n",
    "\n",
    "    df_keywords = extract_all_categories(\n",
    "        df_lem, \n",
    "        col_name = f\"{name_column}_lemmatized\",\n",
    "        categories=lemmatized_categories,\n",
    "        n_process=nb_process  \n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Filtered Reviews ===\")\n",
    "    print(df_keywords)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
