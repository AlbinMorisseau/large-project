{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3fdb94",
   "metadata": {},
   "source": [
    "# Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106525e",
   "metadata": {},
   "source": [
    "Once you have managed the anomalies and created a clean dataset, you now need to create a pipeline that allows you to extract three datasets based on content from a total dataset:\n",
    "- pets dataset\n",
    "- children dataset\n",
    "- disability dataset\n",
    "\n",
    "To do this, several steps must be carried out:\n",
    "- stop word removal\n",
    "- tokenize the text\n",
    "- lemmatize the text\n",
    "- extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eb448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import concurrent.futures\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0685adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "print(NUM_THREAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d447",
   "metadata": {},
   "source": [
    "## Stop word removal and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a93b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "NEGATIONS = {\"not\", \"no\", \"never\", \"none\", \"cannot\", \"can't\", \"don't\", \n",
    "             \"doesn't\", \"isn't\", \"wasn't\", \"weren't\", \"wouldn't\", \"shouldn't\", \"couldn't\"}\n",
    "\n",
    "\n",
    "def remove_stopwords(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given text column in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing text data.\n",
    "    column_name : str\n",
    "        Name of the column containing the text to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A new DataFrame with stopwords removed from the specified column.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered = [word for word in tokens if re.match(r\"^[A-Za-z-]+$\", word) \n",
    "                    and (word not in stop_words or word in NEGATIONS)]\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text, return_dtype=pl.Utf8).alias(column_name)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985458e7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2788d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy (disable unnecessary components for faster performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def lemmatize_and_clean_texts(\n",
    "    texts: List[str],\n",
    "    batch_size: int = 2000,\n",
    "    n_process: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize a list of texts using spaCy with multiprocessing\n",
    "    and clean them for keyword matching.\n",
    "    \n",
    "    Cleaning:\n",
    "    - Replace \" - \" with \"-\" to handle multi-word keywords like \"pet-friendly\".\n",
    "    - Strip leading/trailing whitespace.\n",
    "    \"\"\"\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        text = \" \".join([token.lemma_ for token in doc])\n",
    "        text = text.replace(\" - \", \"-\").strip()\n",
    "        lemmatized.append(text)\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def lemmatize_column_fast(\n",
    "    df: pl.DataFrame, \n",
    "    col_name: str, \n",
    "    new_col_name: str = None, \n",
    "    chunk_size: int = 5000, \n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lemmatize a Polars DataFrame column efficiently in batches with multiprocessing.\n",
    "    \"\"\"\n",
    "    new_col_name = new_col_name or f\"{col_name}_lemmatized\"\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    lemmatized_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_and_clean_texts(chunk, n_process=n_process))\n",
    "\n",
    "    return df.with_columns(pl.Series(name=new_col_name, values=lemmatized_chunks))\n",
    "\n",
    "\n",
    "def lemmatize_categories(\n",
    "    categories: Dict[str, List[str]]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Lemmatize all keywords in category dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        category: lemmatize_and_clean_texts(keywords, batch_size=100, n_process=1)\n",
    "        for category, keywords in categories.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9ebe",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ff92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import concurrent.futures\n",
    "from typing import Dict, List\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_all_categories(\n",
    "    df: pl.DataFrame,\n",
    "    col_name: str,\n",
    "    categories: Dict[str, List[str]],\n",
    "    exclusions: Dict[str, List[str]] = None,\n",
    "    n_process: int = 4,\n",
    "    id_col: str = \"id\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract reviews matching category keywords, keeping reviews if at least one keyword remains\n",
    "    after temporarily removing exclusion phrases.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): Input dataframe.\n",
    "        col_name (str): Column containing the text.\n",
    "        categories (Dict[str, List[str]]): {category: [lemmatized keywords]}.\n",
    "        exclusions (Dict[str, List[str]]): {category: [phrases to exclude]}.\n",
    "        n_process (int): Number of threads.\n",
    "        id_col (str): Column containing unique IDs.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with columns [id, review, keywords_found, category].\n",
    "    \"\"\"\n",
    "\n",
    "    texts = df.select([id_col, col_name]).to_pandas()\n",
    "    exclusions = exclusions or {}\n",
    "\n",
    "    def normalize_keyword(kw: str) -> str:\n",
    "        return kw.strip().replace(\" - \", \"-\")\n",
    "\n",
    "    def make_regex(kw: str) -> str:\n",
    "        kw = normalize_keyword(kw)\n",
    "        if \" \" in kw or \"-\" in kw:\n",
    "            return re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\")\n",
    "        else:\n",
    "            return r\"\\b\" + re.escape(kw) + r\"\\b\"\n",
    "\n",
    "    def process_category(category: str, keywords: List[str], excluded_phrases: List[str]):\n",
    "        results = []\n",
    "        for _, row in texts.iterrows():\n",
    "            text = row[col_name]\n",
    "            review_id = row[id_col]\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            temp_text = text\n",
    "            # Temporarily remove the exclusion phrases\n",
    "            if excluded_phrases:\n",
    "                for ex in excluded_phrases:\n",
    "                    temp_text = re.sub(make_regex(ex), \" \", temp_text, flags=re.IGNORECASE)\n",
    "\n",
    "            # Check if at least one keyword remains\n",
    "            matched_keywords = [kw for kw in keywords if re.search(make_regex(kw), temp_text, flags=re.IGNORECASE)]\n",
    "\n",
    "            if matched_keywords:\n",
    "                results.append((review_id, text, \", \".join(matched_keywords), category))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Parallel processing\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_process) as executor:\n",
    "        futures = {executor.submit(process_category, cat, kws, exclusions.get(cat, [])): cat\n",
    "                   for cat, kws in categories.items()}\n",
    "        for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Keyword extraction\"):\n",
    "            all_results.extend(fut.result())\n",
    "\n",
    "    if not all_results:\n",
    "        return pl.DataFrame(schema={\n",
    "            id_col: pl.Int64,\n",
    "            \"review\": pl.Utf8,\n",
    "            \"keywords_found\": pl.Utf8,\n",
    "            \"category\": pl.Utf8\n",
    "        })\n",
    "\n",
    "    df_filtered = pl.DataFrame({\n",
    "        id_col: [r[0] for r in all_results],\n",
    "        \"review\": [r[1] for r in all_results],\n",
    "        \"keywords_found\": [r[2] for r in all_results],\n",
    "        \"category\": [r[3] for r in all_results]\n",
    "    })\n",
    "\n",
    "    print(f\"Extracted {df_filtered.shape[0]} matching reviews across {len(categories)} categories.\")\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c933f",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea3906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['handicap', 'wheelchair', 'accessible', 'braille', 'ramp', 'disabled', 'barrier-free', 'accessible toilet', 'toilet accessible', 'mobility aid', 'adapted', 'hearing aid', 'visual impairment', 'accessible entrance', 'accessible entrance', 'disablement', 'disenable', 'impairment', 'incapacitate', 'invalid']\n",
      "['hot dog', 'dogecoin', 'corn dog', 'dog day', 'pet project', 'cat scan', 'pet scan', 'pet peeve', 'pet hate', 'pet bottle', 'bulldog clip', 'dog-ear', 'dog-iron', 'dog bark', 'bark dog', 'dog barking', 'dogs barking', 'bird displays', 'early bird', 'early birds', 'party animal', 'screech of birds', 'bird-eye', 'bird BBQ', 'half bird', 'chicken bird', 'hotdog', 'street dog', 'street dogs', 'data dog', 'datadog']\n",
      "\n",
      "=== Lemmatized Categories and Exclusions ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing review:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pl.read_csv(\"../data/original/dataset/data_accessiblego.csv\")\n",
    "    nb_process = NUM_THREAD\n",
    "    name_column = \"review\"\n",
    "    output_path = \"../data/processed/key_words_data_accessiblego.csv\"\n",
    "\n",
    "    \n",
    "    # Load categories\n",
    "    with open(\"../data/categories.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        categories = json.load(f)\n",
    "\n",
    "    # Load exclusions\n",
    "    with open(\"../data/exclusions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        exclusions = json.load(f)\n",
    "\n",
    "    # Verification\n",
    "    print(categories[\"handicap\"])\n",
    "    print(exclusions[\"pet\"])\n",
    "\n",
    "    df_clean = remove_stopwords(df, name_column)\n",
    "\n",
    "    lemmatized_categories = lemmatize_categories(categories)\n",
    "    lemmatized_exclusions = lemmatize_categories(exclusions)\n",
    "    print(\"\\n=== Lemmatized Categories and Exclusions ===\")\n",
    "\n",
    "    df_lem = lemmatize_column_fast(df_clean, name_column, n_process=nb_process)\n",
    "    print(\"\\n=== DataFrame with Lemmatized Texts ===\")\n",
    "\n",
    "    df_keywords = extract_all_categories(\n",
    "        df_lem, \n",
    "        col_name = f\"{name_column}_lemmatized\",\n",
    "        categories=lemmatized_categories,\n",
    "        exclusions=lemmatized_exclusions,\n",
    "        n_process=nb_process\n",
    "    )\n",
    "         \n",
    "    print(\"\\n=== Filtered Reviews ===\")\n",
    "    print(df_keywords.head(10))\n",
    "\n",
    "    # Save\n",
    "    df_keywords.write_csv(output_path)\n",
    "    print(f\"DataFrame saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
