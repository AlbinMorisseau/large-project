{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3fdb94",
   "metadata": {},
   "source": [
    "# Pipeline preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106525e",
   "metadata": {},
   "source": [
    "Once you have managed the anomalies and created a clean dataset, you now need to create a pipeline that allows you to extract three datasets based on content from a total dataset:\n",
    "- pets dataset\n",
    "- children dataset\n",
    "- disability dataset\n",
    "\n",
    "To do this, several steps must be carried out:\n",
    "- stop word removal\n",
    "- tokenize the text\n",
    "- lemmatize the text\n",
    "- extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import polars as pl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0685adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "\n",
    "NUM_THREADS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6d447",
   "metadata": {},
   "source": [
    "## Stop word removal and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def remove_stopwords(df: pl.DataFrame, column_name: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from a given text column in a Polars DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame containing text data.\n",
    "    column_name : str\n",
    "        Name of the column containing the text to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        A new DataFrame with stopwords removed from the specified column.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        filtered = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "        return \" \".join(filtered)\n",
    "\n",
    "    return df.with_columns(\n",
    "        pl.col(column_name).map_elements(clean_text, return_dtype=pl.Utf8).alias(column_name)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985458e7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy (disable unnecessary components for faster performance)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def lemmatize_texts(\n",
    "    texts: List[str], \n",
    "    batch_size: int = 2000, \n",
    "    n_process: int = 4\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize a list of texts using spaCy with multiprocessing.\n",
    "    \"\"\"\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    lemmatized = []\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        lemmatized.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "def lemmatize_column_fast(\n",
    "    df: pl.DataFrame, \n",
    "    col_name: str, \n",
    "    new_col_name: str = None, \n",
    "    chunk_size: int = 5000, \n",
    "    n_process: int = 4\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Lemmatize a Polars DataFrame column efficiently in batches with multiprocessing.\n",
    "    \"\"\"\n",
    "    new_col_name = new_col_name or f\"{col_name}_lemmatized\"\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    lemmatized_chunks = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i + chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_texts(chunk, n_process=n_process))\n",
    "\n",
    "    return df.with_columns(pl.Series(name=new_col_name, values=lemmatized_chunks))\n",
    "\n",
    "\n",
    "def lemmatize_categories(\n",
    "    categories: Dict[str, List[str]]\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Lemmatize all keywords in category dictionary.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        category: lemmatize_texts(keywords, batch_size=100, n_process=1)\n",
    "        for category, keywords in categories.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cea9ebe",
   "metadata": {},
   "source": [
    "## Keywords extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_categories(\n",
    "    df: pl.DataFrame,\n",
    "    col_name: str,\n",
    "    categories: Dict[str, List[str]],\n",
    "    n_process: int = 4,\n",
    "    id_col: str = \"id\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract reviews matching any category keywords using regex filtering.\n",
    "    Returns a DataFrame with the original id, text, lemmatized text, keywords found and category.\n",
    "    \"\"\"\n",
    "\n",
    "    # Conversion en pandas pour manipulation plus facile\n",
    "    texts = df.select([id_col, col_name]).to_pandas()\n",
    "\n",
    "    # Process for one category\n",
    "    def process_category(category, keywords):\n",
    "        regex = r\"\\b(\" + \"|\".join(\n",
    "            re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\") \n",
    "            for kw in keywords\n",
    "        ) + r\")\\b\"\n",
    "\n",
    "        results = []\n",
    "        for _, row in texts.iterrows():\n",
    "            text = row[col_name]\n",
    "            review_id = row[id_col]\n",
    "            if not isinstance(text, str):\n",
    "                continue\n",
    "            if re.search(regex, text, flags=re.IGNORECASE):\n",
    "                found = [\n",
    "                    kw for kw in keywords \n",
    "                    if re.search(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\", text, flags=re.IGNORECASE)\n",
    "                ]\n",
    "                # Ajout de la colonne lemmatized ici\n",
    "                results.append((review_id, text, \", \".join(found), category))\n",
    "        return results\n",
    "\n",
    "    # Parallelisation\n",
    "    all_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=n_process) as executor:\n",
    "        futures = {executor.submit(process_category, cat, kws): cat for cat, kws in categories.items()}\n",
    "        for fut in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Keyword extraction\"):\n",
    "            cat_results = fut.result()\n",
    "            all_results.extend(cat_results)\n",
    "\n",
    "    if not all_results:\n",
    "        print(\"No matches found for any category.\")\n",
    "        return pl.DataFrame(schema={\n",
    "            id_col: pl.Int64,\n",
    "            \"review\": pl.Utf8,\n",
    "            \"keywords_found\": pl.Utf8,\n",
    "            \"category\": pl.Utf8\n",
    "        })\n",
    "\n",
    "    # Conversion finale Polars\n",
    "    df_filtered = pl.DataFrame({\n",
    "        id_col: [r[0] for r in all_results],\n",
    "        \"review\": [r[1] for r in all_results],\n",
    "        \"keywords_found\": [r[2] for r in all_results],\n",
    "        \"category\": [r[3] for r in all_results]\n",
    "    })\n",
    "\n",
    "    print(f\"Extracted {df_filtered.shape[0]} matching reviews across {len(categories)} categories.\")\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466c933f",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pl.read_csv(\"../data/processed/val_cleaned.csv\")\n",
    "    nb_process = NUM_THREADS\n",
    "    name_column = \"review_positive\"\n",
    "    output_path = \"../data/processed/key_word_test_a_supprimer.csv\"\n",
    "\n",
    "    categories = {\n",
    "        \"handicap\": [\n",
    "            \"handicap\", \"wheelchair\", \"accessible\", \"braille\", \"ramp\", \"lift\", \"elevator\",\n",
    "            \"disabled\", \"barrier-free\", \"accessible toilet\", \"toilet accessible\",\n",
    "            \"mobility aid\", \"adapted\", \"hearing aid\", \"visual impairment\", \"accessible entrance\"\n",
    "        ],\n",
    "        \"pet\": [\n",
    "            \"dog\", \"cat\", \"pet\", \"animal\", \"rabbit\", \"hamster\", \"ferret\", \"bird\",\n",
    "            \"pet-friendly\", \"animals allowed\", \"dog-friendly\", \"cat-friendly\",\n",
    "            \"pet welcome\", \"pup\", \"dog bowl\"\n",
    "        ],\n",
    "        \"child\": [\n",
    "            \"child\", \"baby\", \"kid\", \"stroller\", \"son\", \"daughter\", \"toddler\",\n",
    "            \"infant\", \"playground\", \"high chair\", \"changing table\", \"family-friendly\",\n",
    "            \"childcare\", \"kids menu\", \"baby seat\", \"family\",\"baby bed\", \"cot\", \"crib\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    df_clean = remove_stopwords(df, name_column)\n",
    "\n",
    "    lemmatized_categories = lemmatize_categories(categories)\n",
    "    print(\"\\n=== Lemmatized Categories ===\")\n",
    "\n",
    "    df_lem = lemmatize_column_fast(df_clean, name_column, n_process=4)\n",
    "    print(\"\\n=== DataFrame with Lemmatized Texts ===\")\n",
    "\n",
    "    df_keywords = extract_all_categories(\n",
    "        df_lem, \n",
    "        col_name = f\"{name_column}_lemmatized\",\n",
    "        categories=lemmatized_categories,\n",
    "        n_process=nb_process\n",
    "    )\n",
    "         \n",
    "    print(\"\\n=== Filtered Reviews ===\")\n",
    "    print(df_keywords.head(10))\n",
    "\n",
    "    # Sauvegarde\n",
    "    df_keywords.write_csv(output_path)\n",
    "    print(f\"DataFrame saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
