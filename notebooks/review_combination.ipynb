{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc5e87d",
   "metadata": {},
   "source": [
    "# Reviews combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468816b6",
   "metadata": {},
   "source": [
    "This notebook allows you to combine all the reviews present in the various useful datasets identified to create a single dataset of reviews that follows the following structure:\n",
    "- id_review\n",
    "- review\n",
    "- original_dataset\n",
    "- original_id\n",
    "- service_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739233f3",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3748130",
   "metadata": {},
   "source": [
    "Preprocessing is necessary on the European hotelreview and booking datasets because the reviews are separated into two categories: positive and negative. In our initial approach, we do not need to segment the reviews into two groups. The decision has therefore been made to combine the content of these two cells into one.\n",
    "To make the pipeline easier to read, the two new dataframes thus created will be saved in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"../data/processed/dataset/data_booking.csv\")\n",
    "\n",
    "# Merge the review_positive and review_negative columns\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"review_positive\") + \" \" + pl.col(\"review_negative\")).alias(\"review\")\n",
    ")\n",
    "\n",
    "# Remove the original columns\n",
    "df = df.drop([\"review_positive\", \"review_negative\"])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Save in CSV\n",
    "df.write_csv(\"../data/processed/data_booking.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e22ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"../data/processed/dataset/data_european_hotel_reviews.csv\")\n",
    "\n",
    "# Merge the Negative_Review and Positive_Review columns\n",
    "df = df.with_columns(\n",
    "    (pl.col(\"Negative_Review\") + \" \" + pl.col(\"Positive_Review\")).alias(\"review\")\n",
    ")\n",
    "\n",
    "# Remove the original columns\n",
    "df = df.drop([\"Negative_Review\", \"Positive_Review\"])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Save in CSV\n",
    "df.write_csv(\"../data/processed/data_european_hotel_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0891d71",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "def load_dataset(path: str, review_col: str, id_col: str, dataset_name: str, service_type: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a dataset and formats it in memory in standard format:\n",
    "    - review: comment text\n",
    "    - original_dataset: name of the original dataset\n",
    "    - original_id: original identifier\n",
    "    - service_type: type of service (e.g., \"accommodation\", \"restauration\", etc.)\n",
    "    \"\"\"\n",
    "    # Read\n",
    "    df = pl.read_csv(path, encoding=\"latin1\", ignore_errors=True)\n",
    "\n",
    "    # Conversion to target format\n",
    "    df = (\n",
    "        df\n",
    "        .rename({review_col: \"review\", id_col: \"original_id\"})\n",
    "        .select([\"review\", \"original_id\"])\n",
    "        .with_columns([\n",
    "            pl.lit(dataset_name).alias(\"original_dataset\"),\n",
    "            pl.lit(service_type).alias(\"service_type\")\n",
    "        ])\n",
    "        .select([\"review\", \"original_dataset\", \"original_id\", \"service_type\"])\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multiple datasets into memory\n",
    "dfs = [\n",
    "    load_dataset(\"../data/processed/dataset/data_activities_reviews.csv\", \"Text\", \"id\", \"data_activities_reviews\", \"leisure\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_airline_reviews_1.csv\", \"ReviewBody\", \"id\", \"data_airline_reviews_1\", \"transportation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_airline_reviews_2.csv\", \"Reviews\", \"id\", \"data_airline_reviews_2\", \"transportation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_european_restaurant_reviews.csv\", \"Review\", \"id\", \"data_european_restaurant_reviews\", \"restauration\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_hotel_reviews_1.csv\", \"reviews.text\", \"id\", \"data_hotel_reviews_1\", \"accommodation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_hotel_reviews_2.csv\", \"reviews.text\", \"id\", \"data_hotel_reviews_2\", \"accommodation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_hotel_reviews_3.csv\", \"reviews.text\", \"id\", \"data_hotel_reviews_3\", \"accommodation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_restaurant_reviews_1.csv\", \"Review\", \"id\", \"data_restaurant_reviews_1\", \"restauration\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_restaurant_reviews_2.csv\", \"Review\", \"id\", \"data_restaurant_reviews_2\", \"restauration\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_tripadvisor_hotel_reviews.csv\", \"text\", \"id\", \"data_tripadvisor_hotel_reviews\", \"accommodation\"),\n",
    "    load_dataset(\"../data/processed/dataset/data_twitter.csv\", \"Text\", \"id\", \"data_twitter\", \"undetermined\"),\n",
    "    load_dataset(\"../data/processed/data_booking.csv\", \"review\", \"id\", \"data_booking\", \"accommodation\"),\n",
    "    load_dataset(\"../data/processed/data_european_hotel_reviews.csv\", \"review\", \"id\", \"data_european_hotel_reviews\", \"accommodation\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437c056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating all DataFrames\n",
    "df_all = pl.concat(dfs, rechunk=True)\n",
    "\n",
    "# Adding an artificial id\n",
    "id_column = pl.Series(\"id_review\", range(df_all.height))  \n",
    "df_all = pl.concat([id_column.to_frame(), df_all], how=\"horizontal\")\n",
    "\n",
    "# Manage Yelp\n",
    "#df_yelp = load_dataset(\"../data/processed/dataset/data_yelp_reviews.csv\", \"text\", \"id\", \"data_yelp_reviews\", \"accommodation\")\n",
    "\n",
    "#df_all = pl.concat(df_yelp, rechunk=True)\n",
    "# Sauvegarde du dataset global\n",
    "df_all.write_csv(\"../data/processed/all_reviews.csv\")\n",
    "\n",
    "print(\"The global dataset has been created: data/processed/all_reviews.csv\")\n",
    "print(df_all.shape)\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc91fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt\n",
    "\n",
    "# Yelp processing\n",
    "import polars as pl\n",
    "df = pl.read_ndjson('../data/original/yelp_dataset/yelp_academic_dataset_review.json')\n",
    "print(df.head())\n",
    "\n",
    "df_yelp = (\n",
    "    df\n",
    "    .rename({\"text\": \"review\", \"review_id\": \"original_id\"}).select([\"review\", \"original_id\"])\n",
    "    .with_columns([\n",
    "        pl.lit(\"data_yelp_reviews\").alias(\"original_dataset\"),\n",
    "        pl.lit(\"accommodation\").alias(\"service_type\")\n",
    "    ])\n",
    "    .select([\"review\", \"original_dataset\", \"original_id\", \"service_type\"])\n",
    ")\n",
    "\n",
    "id_column = pl.Series(\"id_review\", range(3363667, 3363667 + df_yelp.height)) \n",
    "df_yelp = pl.concat([id_column.to_frame(), df_yelp], how=\"horizontal\")\n",
    "\n",
    "print(df_yelp.head())\n",
    "df_yelp.write_csv(\"../data/processed/all_reviews_yelp.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#df_all = pl.concat([df, df_yelp], rechunk=True)\n",
    "#df_all.write_csv(\"../data/processed/all_reviews_new.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ead71",
   "metadata": {},
   "source": [
    "### Concatenate the two CSV files in streaming mode to avoid memory overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f87ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Read in streaming mode\n",
    "df1 = pl.scan_csv(\"../data/processed/all_reviews.csv\")\n",
    "df2 = pl.scan_csv(\"../data/processed/all_reviews_yelp.csv\")\n",
    "\n",
    "df1 = df1.with_columns(pl.col(\"original_id\").cast(pl.Utf8))\n",
    "df2 = df2.with_columns(pl.col(\"original_id\").cast(pl.Utf8))\n",
    "\n",
    "# Lazy concatenation\n",
    "merged = pl.concat([df1, df2])\n",
    "\n",
    "# Writing to a new file, still streaming\n",
    "merged.sink_csv(\"../data/processed/all_merge_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1699b1d",
   "metadata": {},
   "source": [
    "## Merge only good reviews by categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5833183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed category 'child' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_child.csv\n",
      "Processed category 'pet' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_pet.csv\n",
      "Processed category 'handicap' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_handicap.csv\n",
      "Processed category 'child' for dataset 'data_restaurant_reviews_1' -> processed_data_restaurant_reviews_1_child.csv\n"
     ]
    },
    {
     "ename": "SchemaError",
     "evalue": "datatypes of join keys don't match - `id`: str on left does not match `id`: i64 on right (and no other type was available to cast to)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSchemaError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m categories:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[43mprocess_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_kw_path_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_final_path_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path_template\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprocess_category\u001b[39m\u001b[34m(category_name, dataset_name, df_kw_path_template, df_final_path_template, output_path_template)\u001b[39m\n\u001b[32m     24\u001b[39m df_kw = pl.read_csv(df_kw_path)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Join sur 'id'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m df = \u001b[43mdf_final\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_kw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Rename colonne\u001b[39;00m\n\u001b[32m     30\u001b[39m df = df.rename({\u001b[33m\"\u001b[39m\u001b[33mreview_right\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mreview_cleaned\u001b[39m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\polars\\dataframe\\frame.py:8104\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how, left_on, right_on, suffix, validate, nulls_equal, coalesce, maintain_order)\u001b[39m\n\u001b[32m   8086\u001b[39m require_same_type(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[32m   8088\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazyframe\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[32m   8090\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m   8091\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8092\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   8093\u001b[39m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m=\u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8094\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8095\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8096\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8098\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8099\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnulls_equal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnulls_equal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8102\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaintain_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaintain_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   8103\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m8104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQueryOptFlags\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   8105\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\polars\\_utils\\deprecation.py:97\u001b[39m, in \u001b[36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33min-memory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\polars\\lazyframe\\opt_flags.py:328\u001b[39m, in \u001b[36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m         optflags = cb(optflags, kwargs.pop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[32m    327\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33moptimizations\u001b[39m\u001b[33m\"\u001b[39m] = optflags\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\polars\\lazyframe\\frame.py:2415\u001b[39m, in \u001b[36mLazyFrame.collect\u001b[39m\u001b[34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[39m\n\u001b[32m   2413\u001b[39m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[32m   2414\u001b[39m callback = _kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mpost_opt_callback\u001b[39m\u001b[33m\"\u001b[39m, callback)\n\u001b[32m-> \u001b[39m\u001b[32m2415\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mSchemaError\u001b[39m: datatypes of join keys don't match - `id`: str on left does not match `id`: i64 on right (and no other type was available to cast to)"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def process_category(category_name: str, dataset_name: str, df_kw_path_template: str,\n",
    "                     df_final_path_template: str, output_path_template: str):\n",
    "    \"\"\"\n",
    "    Process reviews for a given category and dataset:\n",
    "    - load the final reviews\n",
    "    - join with keywords\n",
    "    - rename the cleaned review column\n",
    "    - remove 'original_index'\n",
    "    - add 'original_dataset' and 'service_type'\n",
    "    - filter by category\n",
    "    - write to CSV\n",
    "    \"\"\"\n",
    "    # Construire les chemins\n",
    "    df_kw_path = df_kw_path_template.format(dataset=dataset_name)\n",
    "    df_final_path = df_final_path_template.format(category=category_name, dataset=dataset_name)\n",
    "    output_path = output_path_template.format(category=category_name, dataset=dataset_name)\n",
    "\n",
    "    # Load final reviews\n",
    "    df_final = pl.read_csv(df_final_path)\n",
    "    \n",
    "    # Load keywords\n",
    "    df_kw = pl.read_csv(df_kw_path)\n",
    "    \n",
    "    # Join sur 'id'\n",
    "    df = df_final.join(df_kw, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Rename colonne\n",
    "    df = df.rename({\"review_right\": \"review_cleaned\"})\n",
    "    \n",
    "    # Retirer colonne 'original_index' si elle existe\n",
    "    if \"original_index\" in df.columns:\n",
    "        df = df.drop(\"original_index\")\n",
    "    \n",
    "    # Ajouter colonne 'original_dataset'\n",
    "    df = df.with_columns(pl.lit(dataset_name).alias(\"original_dataset\"))\n",
    "    \n",
    "    # Déterminer le type de service\n",
    "    dataset_lower = dataset_name.lower()\n",
    "    if \"hotel\" in dataset_lower or \"booking\" in dataset_lower:\n",
    "        service_type = \"accommodation\"\n",
    "    elif \"restaurant\" in dataset_lower:\n",
    "        service_type = \"restauration\"\n",
    "    elif \"airline\" in dataset_lower:\n",
    "        service_type = \"transportation\"\n",
    "    else:\n",
    "        service_type = \"unknown\"\n",
    "    \n",
    "    df = df.with_columns(pl.lit(service_type).alias(\"service_type\"))\n",
    "    \n",
    "    # Filtrer par catégorie\n",
    "    df = df.filter(pl.col(\"category\") == category_name)\n",
    "    \n",
    "    # Écrire CSV\n",
    "    df.write_csv(output_path)\n",
    "    print(f\"Processed category '{category_name}' for dataset '{dataset_name}' -> {output_path}\")\n",
    "\n",
    "\n",
    "# Paramètres généraux\n",
    "categories = [\"child\", \"pet\", \"handicap\"]\n",
    "datasets = [\"data_hotel_reviews_1\", \"data_restaurant_reviews_1\"]  # exemple\n",
    "df_kw_path_template = \"../data/processed/Bazarre/key_words_{dataset}.csv\"\n",
    "df_final_path_template = \"../data/processed/final/{category}/validated_{dataset}_{category}_good.csv\"\n",
    "output_path_template = \"processed_{dataset}_{category}.csv\"\n",
    "\n",
    "# Exécution\n",
    "for dataset in datasets:\n",
    "    for category in categories:\n",
    "        process_category(category, dataset, df_kw_path_template, df_final_path_template, output_path_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d6f314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed category 'child' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_child.csv\n",
      "Processed category 'pet' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_pet.csv\n",
      "Processed category 'handicap' for dataset 'data_hotel_reviews_1' -> processed_data_hotel_reviews_1_handicap.csv\n",
      "Processed category 'child' for dataset 'data_restaurant_reviews_1' -> processed_data_restaurant_reviews_1_child.csv\n",
      "No reviews for category 'pet' in dataset 'data_restaurant_reviews_1', skipping...\n",
      "File not found: ../data/processed/final/handicap/validated_data_restaurant_reviews_1_handicap_good.csv, skipping...\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def process_category(category_name: str, dataset_name: str, df_kw_path_template: str,\n",
    "                     df_final_path_template: str, output_path_template: str):\n",
    "    \"\"\"\n",
    "    Process reviews for a given category and dataset safely:\n",
    "    - load the final reviews (skip if empty)\n",
    "    - join with keywords\n",
    "    - rename the cleaned review column\n",
    "    - remove 'original_index'\n",
    "    - add 'original_dataset' and 'service_type'\n",
    "    - filter by category\n",
    "    - write to CSV\n",
    "    \"\"\"\n",
    "    # Construire les chemins\n",
    "    df_kw_path = df_kw_path_template.format(dataset=dataset_name)\n",
    "    df_final_path = df_final_path_template.format(category=category_name, dataset=dataset_name)\n",
    "    output_path = output_path_template.format(category=category_name, dataset=dataset_name)\n",
    "\n",
    "    # Load final reviews\n",
    "    try:\n",
    "        df_final = pl.read_csv(df_final_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {df_final_path}, skipping...\")\n",
    "        return\n",
    "\n",
    "    # Vérifier si le CSV n'est pas vide (seulement l'en-tête)\n",
    "    if df_final.is_empty():\n",
    "        print(f\"No reviews for category '{category_name}' in dataset '{dataset_name}', skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Load keywords\n",
    "    df_kw = pl.read_csv(df_kw_path)\n",
    "    \n",
    "    # Join sur 'id'\n",
    "    df = df_final.join(df_kw, on=\"id\", how=\"left\")\n",
    "    \n",
    "    # Rename colonne\n",
    "    df = df.rename({\"review_right\": \"review_cleaned\"})\n",
    "    \n",
    "    # Retirer colonne 'original_index' si elle existe\n",
    "    if \"original_index\" in df.columns:\n",
    "        df = df.drop(\"original_index\")\n",
    "    \n",
    "    # Ajouter colonne 'original_dataset'\n",
    "    df = df.with_columns(pl.lit(dataset_name).alias(\"original_dataset\"))\n",
    "    \n",
    "    # Déterminer le type de service\n",
    "    dataset_lower = dataset_name.lower()\n",
    "    if \"hotel\" in dataset_lower or \"booking\" in dataset_lower:\n",
    "        service_type = \"accommodation\"\n",
    "    elif \"restaurant\" in dataset_lower:\n",
    "        service_type = \"restauration\"\n",
    "    elif \"airline\" in dataset_lower:\n",
    "        service_type = \"transportation\"\n",
    "    else:\n",
    "        service_type = \"unknown\"\n",
    "    \n",
    "    df = df.with_columns(pl.lit(service_type).alias(\"service_type\"))\n",
    "    \n",
    "    # Filtrer par catégorie\n",
    "    df = df.filter(pl.col(\"category\") == category_name)\n",
    "    \n",
    "    # Vérifier si le filtrage n'a pas tout supprimé\n",
    "    if df.is_empty():\n",
    "        print(f\"No matching reviews for category '{category_name}' after filtering in dataset '{dataset_name}', skipping...\")\n",
    "        return\n",
    "    \n",
    "    # Écrire CSV\n",
    "    df.write_csv(output_path)\n",
    "    print(f\"Processed category '{category_name}' for dataset '{dataset_name}' -> {output_path}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "categories = [\"child\", \"pet\", \"handicap\"]\n",
    "datasets = [\"data_hotel_reviews_1\", \"data_restaurant_reviews_1\"]\n",
    "df_kw_path_template = \"../data/processed/Bazarre/key_words_{dataset}.csv\"\n",
    "df_final_path_template = \"../data/processed/final/{category}/validated_{dataset}_{category}_good.csv\"\n",
    "output_path_template = \"processed_{dataset}_{category}.csv\"\n",
    "\n",
    "for dataset in datasets:\n",
    "    for category in categories:\n",
    "        process_category(category, dataset, df_kw_path_template, df_final_path_template, output_path_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65507a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 2 files into child_all.csv\n",
      "Deleted intermediate files for category 'child'\n",
      "Merged 1 files into pet_all.csv\n",
      "Deleted intermediate files for category 'pet'\n",
      "Merged 1 files into handicap_all.csv\n",
      "Deleted intermediate files for category 'handicap'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import glob\n",
    "import os\n",
    "\n",
    "categories = [\"child\", \"pet\", \"handicap\"]\n",
    "\n",
    "for category in categories:\n",
    "    # Récupérer tous les CSV pour cette catégorie\n",
    "    pattern = f\"processed_*_{category}.csv\"\n",
    "    csv_files = glob.glob(pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No processed CSVs found for category '{category}', skipping merge.\")\n",
    "        continue\n",
    "    \n",
    "    # Lire et concaténer tous les CSV\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pl.read_csv(f)\n",
    "        dfs.append(df)\n",
    "    \n",
    "    merged_df = pl.concat(dfs)\n",
    "    \n",
    "    # Écrire le CSV final pour la catégorie\n",
    "    final_output = f\"{category}_all.csv\"\n",
    "    merged_df.write_csv(final_output)\n",
    "    print(f\"Merged {len(csv_files)} files into {final_output}\")\n",
    "    \n",
    "    # Supprimer les CSV intermédiaires\n",
    "    for f in csv_files:\n",
    "        os.remove(f)\n",
    "    print(f\"Deleted intermediate files for category '{category}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc73e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
