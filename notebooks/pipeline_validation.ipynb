{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59208fa0",
   "metadata": {},
   "source": [
    "# Pipeline Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88158dc9",
   "metadata": {},
   "source": [
    "We created a pipeline that combines the results of three methods to have a \"cross validation\" on our reveiws classifications:\n",
    "\n",
    "- the result of the keywords extractionn that classified a review in the theme related to the keyword\n",
    "- the result of a finetunned BERT model on the review enabling better classification\n",
    "- the result of a small LLM on the review enabling better context understanding\n",
    "\n",
    "If a review gets classified the same way by each of these 3 methods it is considered valid. \n",
    "Either way, it has to be submitted to human validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4161ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d0dc4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-19 21:18:49,323 - INFO - NUM_THREAD fixed to 12\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dd27fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CATEGORIES = ['handicap', 'pet', 'child']\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_THREADS = 4\n",
    "BERT_PATH = \"../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"bert/best_weights.pth\"\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Utilisation du device: {device}\")\n",
    "\n",
    "# Création des dossiers de sortie\n",
    "Path(\"../data/processed/data_validated/good\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../data/processed/data_validated/rejected\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "69571b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utile ?\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'idx': idx\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7d2faab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8066e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_review_chunks(review_text, max_length=128):\n",
    "    \"\"\"Divise une review en chunks de max_length tokens\"\"\"\n",
    "    words = review_text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk).split()) >= max_length - 20:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks if chunks else [review_text]\n",
    "\n",
    "\n",
    "def predict_bert_chunks(reviews, model, tokenizer, threshold=0.95, batch_size=32):\n",
    "    \"\"\"Prédit les catégories pour une liste de reviews avec chunking\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch_reviews = reviews[i:i + batch_size]\n",
    "            \n",
    "            # Encoder le batch\n",
    "            encoded = tokenizer(\n",
    "                batch_reviews,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Prédiction\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "            pred_bin = (pred > threshold).astype(int)\n",
    "            \n",
    "            all_predictions.extend(pred_bin.tolist())\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification via Ollama\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\n",
    "             \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "             f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "             f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "             \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "         )},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=messages)\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4029a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. Chargement des données\n",
    "    print(\"Chargement des données...\")\n",
    "    keywords_df = pl.read_csv(\"../data/processed/data_categorized/key_words_data_accessiblego.csv\")\n",
    "    original_df = pl.read_csv(\"../data/original/dataset/data_accessiblego.csv\")\n",
    "    \n",
    "    # 2. Fusion pour récupérer les reviews originales\n",
    "    keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "    original_reviews = original_df.select([\"id\", \"review\"])\n",
    "    df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "    df = df.group_by('id').agg(\n",
    "        pl.col('review').first(), \n",
    "        pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "    )\n",
    "\n",
    "    # 3. Chargement du modèle BERT\n",
    "    print(\"Chargement du tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "    print(\"Chargement du modèle BERT...\")\n",
    "    model = BertMultiLabelClassifier(n_classes=len(CATEGORIES))\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 4. Prédiction BERT sur chunks\n",
    "    print(\"Prédiction BERT avec chunking...\")\n",
    "    bert_predictions = []\n",
    "    \n",
    "    for review in tqdm(df['review'], desc=\"Traitement des reviews\"):\n",
    "        chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "        chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "        # Agrégation: si un chunk est positif, toute la review l'est\n",
    "        final_pred = [0, 0, 0]\n",
    "        for pred in chunk_preds:\n",
    "            for i in range(3):\n",
    "                if pred[i]:\n",
    "                    final_pred[i] = 1\n",
    "        \n",
    "        bert_predictions.append(final_pred)\n",
    "    \n",
    "    # 5. Conversion des catégories keywords en format binaire\n",
    "    keyword_preds = []\n",
    "    for category in df['category']:\n",
    "        pred = [0, 0, 0]\n",
    "        if 'handicap' in str(category).lower():\n",
    "            pred[0] = 1\n",
    "        if 'pet' in str(category).lower():\n",
    "            pred[1] = 1\n",
    "        if 'child' in str(category).lower():\n",
    "            pred[2] = 1\n",
    "        keyword_preds.append(pred)\n",
    "    \n",
    "    # 6. Ajout des prédictions au DataFrame\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_child\", [p[2] for p in bert_predictions]),\n",
    "        pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_child\", [p[2] for p in keyword_preds])\n",
    "    ])\n",
    "    \n",
    "    # 7. Comparaison et filtrage\n",
    "    print(\"Comparaison des prédictions...\")\n",
    "    validated_rows = []\n",
    "    disputed_rows = []\n",
    "    \n",
    "    rows = iter(df.iter_rows(named=True))\n",
    "    for row in tqdm(rows, total=len(df), desc=\"Validation\"):\n",
    "        bert_preds = [row['bert_handicap'], row['bert_pet'], row['bert_child']]\n",
    "        kw_preds = [row['kw_handicap'], row['kw_pet'], row['kw_child']]\n",
    "        \n",
    "        if bert_preds == kw_preds:\n",
    "            # Accord parfait\n",
    "            validated_rows.append({\n",
    "                **row,\n",
    "                'validation_status': 'agreed',\n",
    "                'llm_handicap': None,\n",
    "                'llm_pet': None,\n",
    "                'llm_child': None\n",
    "            })\n",
    "        else:\n",
    "            # Désaccord: on demande au LLM\n",
    "            llm_preds = [None, None, None]\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "                futures = {}\n",
    "                for i, cat in enumerate(CATEGORIES):\n",
    "                    if bert_preds[i] != kw_preds[i]:\n",
    "                        futures[executor.submit(classify_review_ollama, row['review'], cat)] = i\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    cat_idx = futures[future]\n",
    "                    llm_preds[cat_idx] = future.result()\n",
    "            \n",
    "            # Vérifier si le LLM valide au moins une des prédictions\n",
    "            agrees_with_bert = any(llm_preds[i] == bert_preds[i] for i in range(3) if llm_preds[i] is not None)\n",
    "            agrees_with_kw = any(llm_preds[i] == kw_preds[i] for i in range(3) if llm_preds[i] is not None)\n",
    "            \n",
    "            if agrees_with_bert or agrees_with_kw:\n",
    "                validated_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'llm_validated',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "            else:\n",
    "                disputed_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'disputed',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "    \n",
    "    # 8. Sauvegarde des résultats\n",
    "    print(\"Sauvegarde des résultats...\")\n",
    "    validated_df = pl.DataFrame(validated_rows)\n",
    "    disputed_df = pl.DataFrame(disputed_rows)\n",
    "    \n",
    "    validated_df.write_csv(\"../data/processed/data_validated/good/validated_reviews.csv\")\n",
    "    disputed_df.write_csv(\"../data/processed/data_validated/rejected/disputed_reviews.csv\")\n",
    "    \n",
    "    print(f\"\\n✓ Pipeline terminée:\")\n",
    "    print(f\"  - Reviews validées: {len(validated_rows)}\")\n",
    "    print(f\"  - Reviews à vérifier: {len(disputed_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "43be7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Chargement du tokenizer...\n",
      "Chargement du modèle BERT...\n",
      "Prédiction BERT avec chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des reviews: 100%|██████████| 1487/1487 [00:15<00:00, 97.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison des prédictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/1487 [00:00<?, ?it/s]2025-11-19 21:44:38,603 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Validation:   0%|          | 2/1487 [00:00<06:58,  3.55it/s]2025-11-19 21:44:39,086 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Validation:   0%|          | 4/1487 [00:01<06:22,  3.88it/s]2025-11-19 21:44:39,599 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Validation:   0%|          | 5/1487 [00:01<08:14,  3.00it/s]2025-11-19 21:44:40,126 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Validation:   0%|          | 7/1487 [00:02<07:26,  3.31it/s]2025-11-19 21:44:41,953 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "Validation:   0%|          | 7/1487 [00:03<13:47,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[196]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[195]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m bert_preds[i] != kw_preds[i]:\n\u001b[32m     92\u001b[39m         futures[executor.submit(classify_review_ollama, row[\u001b[33m'\u001b[39m\u001b[33mreview\u001b[39m\u001b[33m'\u001b[39m], cat)] = i\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcat_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcat_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n\u001b[32m    246\u001b[39m     finished = waiter.finished_futures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:622\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    620\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = iter(df.iter_rows(named=True))\n",
    "for row in tqdm(rows, total=len(df), desc=\"Validation\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39737e6e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033b594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données...\n",
      "Chargement du tokenizer...\n",
      "Chargement du modèle BERT...\n",
      "Prédiction BERT avec chunking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traitement des reviews: 100%|██████████| 1487/1487 [00:15<00:00, 98.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sauvegarde des résultats...\n",
      "Terminé!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "BERT_PATH = \"../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"bert/best_weights.pth\"\n",
    "MAX_SEQ_LEN = 128\n",
    "THRESHOLD = 0.95\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "classes = [\"handicap\", \"pet\", \"child\"]\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1) Définition du modèle\n",
    "# ===============================\n",
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "def split_review_chunks(review_text, max_length=128):\n",
    "    \"\"\"Divise une review en chunks de max_length tokens\"\"\"\n",
    "    words = review_text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk).split()) >= max_length - 20:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks if chunks else [review_text]\n",
    "\n",
    "\n",
    "def predict_bert_chunks(reviews, model, tokenizer, threshold=0.95, batch_size=32):\n",
    "    \"\"\"Prédit les catégories pour une liste de reviews avec chunking\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch_reviews = reviews[i:i + batch_size]\n",
    "            \n",
    "            # Encoder le batch\n",
    "            encoded = tokenizer(\n",
    "                batch_reviews,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Prédiction\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "            pred_bin = (pred > threshold).astype(int)\n",
    "            \n",
    "            all_predictions.extend(pred_bin.tolist())\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. Chargement des données\n",
    "    print(\"Chargement des données...\")\n",
    "    keywords_df = pl.read_csv(\"../data/processed/data_categorized/key_words_data_accessiblego.csv\")\n",
    "    original_df = pl.read_csv(\"../data/original/dataset/data_accessiblego.csv\")\n",
    "    \n",
    "    # 2. Fusion pour récupérer les reviews originales\n",
    "    keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "    original_reviews = original_df.select([\"id\", \"review\"])\n",
    "    df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "    df = df.group_by('id').agg(\n",
    "        pl.col('review').first(), \n",
    "        pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "    )\n",
    "\n",
    "    # 3. Chargement du modèle BERT\n",
    "    print(\"Chargement du tokenizer...\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "    \n",
    "    print(\"Chargement du modèle BERT...\")\n",
    "    model = BertMultiLabelClassifier(n_classes=len(CATEGORIES))\n",
    "    model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # 4. Prédiction BERT sur chunks\n",
    "    print(\"Prédiction BERT avec chunking...\")\n",
    "    bert_predictions = []\n",
    "    \n",
    "    for review in tqdm(df['review'], desc=\"Traitement des reviews\"):\n",
    "        chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "        chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "        # Agrégation: si un chunk est positif, toute la review l'est\n",
    "        final_pred = [0, 0, 0]\n",
    "        for pred in chunk_preds:\n",
    "            for i in range(3):\n",
    "                if pred[i]:\n",
    "                    final_pred[i] = 1\n",
    "        \n",
    "        bert_predictions.append(final_pred)\n",
    "    \n",
    "    # 5. Conversion des catégories keywords en format binaire\n",
    "    keyword_preds = []\n",
    "    for category in df['category']:\n",
    "        pred = [0, 0, 0]\n",
    "        if 'handicap' in str(category).lower():\n",
    "            pred[0] = 1\n",
    "        if 'pet' in str(category).lower():\n",
    "            pred[1] = 1\n",
    "        if 'child' in str(category).lower():\n",
    "            pred[2] = 1\n",
    "        keyword_preds.append(pred)\n",
    "    \n",
    "    # 6. Ajout des prédictions au DataFrame\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_child\", [p[2] for p in bert_predictions]),\n",
    "        pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_child\", [p[2] for p in keyword_preds])\n",
    "    ])\n",
    "\n",
    "    print(\"Sauvegarde des résultats...\")\n",
    "    df.write_csv(\"test.csv\")\n",
    "    print(\"Terminé!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ab3d3188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv(\"test.csv\")\n",
    "somme_bert_pet = df.select(pl.col(\"bert_pet\").sum()).item()\n",
    "\n",
    "print(somme_bert_pet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
