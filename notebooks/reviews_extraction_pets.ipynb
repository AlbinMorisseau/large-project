{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47a3f39",
   "metadata": {},
   "source": [
    "# Pets reviews extraction \n",
    "\n",
    "The objective is to extract relevant reviews to understand the profiles of travelers with pets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d2b5f8",
   "metadata": {},
   "source": [
    "## Word regex extraction\n",
    "\n",
    "The keywords list is not exhaustive but tend to extract the main part of the targeted reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23f3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfd06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df_booking = pl.read_csv('../data/processed/data_totale_booking.csv')\n",
    "df_yelp = pl.read_ndjson('../data/original/yelp_dataset/yelp_academic_dataset_review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08e30138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# key words\n",
    "categories = {\n",
    "    \"pets\": [\n",
    "        \"dog\", \"cat\", \"pet\", \"animal\", \"rabbit\", \"hamster\", \"ferret\", \"bird\", \"pet-friendly\",\n",
    "        \"animals allowed\", \"dog-friendly\", \"cat-friendly\", \"pet welcome\", \"pup\", \"dog bowl\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92037f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "# Load the english model of SpaCy\n",
    "# spacy.prefer_gpu()            # To uncomment to run on GPU\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lemmatization via SpaCy\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text.lower())  # minuscules + NLP\n",
    "    return \" \".join(token.lemma_ for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8617dde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pets': ['dog', 'cat', 'pet', 'animal', 'rabbit', 'hamster', 'ferret', 'bird', 'pet - friendly', 'animal allow', 'dog - friendly', 'cat - friendly', 'pet welcome', 'pup', 'dog bowl']}\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize categories so that words can match each other.\n",
    "lemmatized_categories = {\n",
    "    category: [lemmatize_text(kw) for kw in keywords]\n",
    "    for category, keywords in categories.items()\n",
    "}\n",
    "\n",
    "print(lemmatized_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7381958",
   "metadata": {},
   "source": [
    "### Booking dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56eb65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_booking.head(100000)\n",
    "\n",
    "df_lemmatized = df_test.with_columns([\n",
    "    pl.col(\"review_positive\").map_elements(lemmatize_text).alias(\"review_positive\"),\n",
    "    pl.col(\"review_negative\").map_elements(lemmatize_text).alias(\"review_negative\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebf235",
   "metadata": {},
   "source": [
    "## Lemmatization parallelised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3a70e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatizing review_positive:   0%|          | 0/10 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# --- 4. Exemple avec tes colonnes ---\u001b[39;00m\n\u001b[32m     29\u001b[39m df_test = df_booking.head(\u001b[32m50000\u001b[39m)\n\u001b[32m     31\u001b[39m df_lemmatized = df_test.with_columns([\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[43mlemmatize_column_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreview_positive\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     33\u001b[39m     lemmatize_column_fast(df_test, \u001b[33m\"\u001b[39m\u001b[33mreview_negative\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m ])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mlemmatize_column_fast\u001b[39m\u001b[34m(df, col_name, chunk_size)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), chunk_size), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLemmatizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     23\u001b[39m     chunk = texts[i:i+chunk_size]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     lemmatized_chunks.extend(\u001b[43mlemmatize_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pl.Series(name=col_name, values=lemmatized_chunks)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mlemmatize_texts\u001b[39m\u001b[34m(texts, batch_size, n_process)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# SpaCy ne supporte pas None → filtrer / remplacer\u001b[39;00m\n\u001b[32m     12\u001b[39m clean_texts = [(t \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_process\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_process\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlemmatized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemma_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lemmatized\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\spacy\\language.py:1622\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1620\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1621\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1622\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\spacy\\language.py:1688\u001b[39m, in \u001b[36mLanguage._multiprocessing_pipe\u001b[39m\u001b[34m(self, texts, pipes, n_process, batch_size)\u001b[39m\n\u001b[32m   1674\u001b[39m procs = [\n\u001b[32m   1675\u001b[39m     mp.Process(\n\u001b[32m   1676\u001b[39m         target=_apply_pipes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1685\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m rch, sch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(texts_q, bytedocs_send_ch)\n\u001b[32m   1686\u001b[39m ]\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m proc \u001b[38;5;129;01min\u001b[39;00m procs:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m     \u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[38;5;66;03m# Close writing-end of channels. This is needed to avoid that reading\u001b[39;00m\n\u001b[32m   1691\u001b[39m \u001b[38;5;66;03m# from the channel blocks indefinitely when the worker closes the\u001b[39;00m\n\u001b[32m   1692\u001b[39m \u001b[38;5;66;03m# channel.\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tx \u001b[38;5;129;01min\u001b[39;00m bytedocs_send_ch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\context.py:337\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\popen_spawn_win32.py:97\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     96\u001b[39m     reduction.dump(prep_data, to_child)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[43mreduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     99\u001b[39m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\multiprocessing\\reduction.py:60\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, file, protocol)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdump\u001b[39m(obj, file, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     59\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Charger SpaCy ---\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "# --- 2. Fonction pour lemmatiser une liste de textes ---\n",
    "def lemmatize_texts(texts, batch_size=1000, n_process=4):\n",
    "    lemmatized = []\n",
    "    # SpaCy ne supporte pas None → filtrer / remplacer\n",
    "    clean_texts = [(t if isinstance(t, str) else \"\") for t in texts]\n",
    "    for doc in nlp.pipe(clean_texts, batch_size=batch_size, n_process=n_process):\n",
    "        lemmatized.append(\" \".join([token.lemma_ for token in doc]))\n",
    "    return lemmatized\n",
    "\n",
    "# --- 3. Fonction pour traiter une colonne Polars ---\n",
    "def lemmatize_column_fast(df, col_name, chunk_size=5000):\n",
    "    lemmatized_chunks = []\n",
    "    texts = df.select(col_name).to_series().to_list()\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size), desc=f\"Lemmatizing {col_name}\"):\n",
    "        chunk = texts[i:i+chunk_size]\n",
    "        lemmatized_chunks.extend(lemmatize_texts(chunk))\n",
    "    \n",
    "    return pl.Series(name=col_name, values=lemmatized_chunks)\n",
    "\n",
    "# --- 4. Exemple avec tes colonnes ---\n",
    "df_test = df_booking.head(50000)\n",
    "\n",
    "df_lemmatized = df_test.with_columns([\n",
    "    lemmatize_column_fast(df_test, \"review_positive\"),\n",
    "    lemmatize_column_fast(df_test, \"review_negative\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16997ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Count ===\n",
      "\n",
      "pets: 1953\n",
      "\n",
      "=== Keyword Details ===\n",
      "\n",
      "dog: 1048\n",
      "pet: 303\n",
      "bird: 233\n",
      "animal: 161\n",
      "cat: 159\n",
      "rabbit: 16\n",
      "pet - friendly: 10\n",
      "pup: 9\n",
      "dog - friendly: 8\n",
      "dog bowl: 5\n",
      "pet welcome: 1\n",
      "animal allow: 0\n",
      "cat - friendly: 0\n",
      "ferret: 0\n",
      "hamster: 0\n",
      "\n",
      "=== Preview of Filtered DataFrame ===\n",
      "Size of dataset 1465\n"
     ]
    }
   ],
   "source": [
    "# Concatenate both review columns into a single text column for counting/filtering\n",
    "df_lemmatized = df_lemmatized.with_columns(\n",
    "    (pl.col(\"review_positive\") + \" \" + pl.col(\"review_negative\")).alias(\"text\")\n",
    ")\n",
    "\n",
    "# Initialize dictionaries for counts\n",
    "keyword_summary = {}\n",
    "category_summary = {}\n",
    "\n",
    "# List to store filtered rows\n",
    "filtered_rows = []\n",
    "\n",
    "for category, keywords in lemmatized_categories.items():\n",
    "    category_total = 0\n",
    "    # Create a regex for the category\n",
    "    regex = r\"\\b(\" + \"|\".join(re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\") for kw in keywords) + r\")\\b\"\n",
    "\n",
    "    # Filter rows containing at least one keyword\n",
    "    matches = df_lemmatized.filter(pl.col(\"text\").str.contains(regex))\n",
    "\n",
    "    # Function to identify which keywords are found in each comment\n",
    "    def find_keywords(text):\n",
    "        found = [kw for kw in keywords if re.search(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\", text)]\n",
    "        return \", \".join(found)\n",
    "\n",
    "    # Add a new column with the keywords found\n",
    "    matches = matches.with_columns(\n",
    "        pl.col(\"text\").map_elements(find_keywords).alias(\"keywords_found\")\n",
    "    )\n",
    "\n",
    "    # Append to the list of filtered rows\n",
    "    filtered_rows.append(matches)\n",
    "\n",
    "    # Count keyword occurrences\n",
    "    for kw in keywords:\n",
    "        count = (\n",
    "            df_lemmatized\n",
    "            .select(pl.col(\"text\").str.count_matches(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\").sum().alias(\"total\"))\n",
    "            .item()\n",
    "        )\n",
    "        keyword_summary[kw] = int(count)\n",
    "        category_total += count\n",
    "    category_summary[category] = int(category_total)\n",
    "\n",
    "# Concatenate all filtered rows into a new DataFrame\n",
    "df_filtered = pl.concat(filtered_rows)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Category Count ===\\n\")\n",
    "for cat, count in category_summary.items():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "print(\"\\n=== Keyword Details ===\\n\")\n",
    "for kw, count in sorted(keyword_summary.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print(f\"{kw}: {count}\")\n",
    "\n",
    "print(\"\\n=== Preview of Filtered DataFrame ===\")\n",
    "print(\"Size of dataset\", df_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3179cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the new dataset\n",
    "df_filtered.write_csv(\"../data/processed/data_pet_booking.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb54a7",
   "metadata": {},
   "source": [
    "### Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "253e83b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_yelp.head(10000)\n",
    "\n",
    "df_lemmatized = df_test.with_columns(\n",
    "    pl.col(\"text\").map_elements(lemmatize_text).alias(\"text\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b243d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Category Count ===\n",
      "\n",
      "pets: 511\n",
      "\n",
      "=== Keyword Details ===\n",
      "\n",
      "dog: 287\n",
      "pet: 56\n",
      "bird: 47\n",
      "cat: 46\n",
      "animal: 34\n",
      "rabbit: 27\n",
      "pup: 11\n",
      "dog - friendly: 3\n",
      "animal allow: 0\n",
      "cat - friendly: 0\n",
      "dog bowl: 0\n",
      "ferret: 0\n",
      "hamster: 0\n",
      "pet - friendly: 0\n",
      "pet welcome: 0\n",
      "\n",
      "=== Preview of Filtered DataFrame ===\n",
      "Size of dataset 277\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries for counts\n",
    "keyword_summary = {}\n",
    "category_summary = {}\n",
    "\n",
    "# List to store filtered rows\n",
    "filtered_rows = []\n",
    "\n",
    "for category, keywords in lemmatized_categories.items():\n",
    "    category_total = 0\n",
    "    # Create a regex for the category\n",
    "    regex = r\"\\b(\" + \"|\".join(re.escape(kw).replace(\"\\\\-\", \"[-\\\\s]\").replace(\"\\\\ \", \"\\\\s+\") for kw in keywords) + r\")\\b\"\n",
    "\n",
    "    # Filter rows containing at least one keyword\n",
    "    matches = df_lemmatized.filter(pl.col(\"text\").str.contains(regex))\n",
    "\n",
    "    # Function to identify which keywords are found in each comment\n",
    "    def find_keywords(text):\n",
    "        found = [kw for kw in keywords if re.search(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\", text)]\n",
    "        return \", \".join(found)\n",
    "\n",
    "    # Add a new column with the keywords found\n",
    "    matches = matches.with_columns(\n",
    "        pl.col(\"text\").map_elements(find_keywords).alias(\"keywords_found\")\n",
    "    )\n",
    "\n",
    "    # Append to the list of filtered rows\n",
    "    filtered_rows.append(matches)\n",
    "\n",
    "    # Count keyword occurrences\n",
    "    for kw in keywords:\n",
    "        count = (\n",
    "            df_lemmatized\n",
    "            .select(pl.col(\"text\").str.count_matches(r\"\\b\" + re.escape(kw).replace(\"\\\\ \", \"\\\\s+\") + r\"\\b\").sum().alias(\"total\"))\n",
    "            .item()\n",
    "        )\n",
    "        keyword_summary[kw] = int(count)\n",
    "        category_total += count\n",
    "    category_summary[category] = int(category_total)\n",
    "\n",
    "# Concatenate all filtered rows into a new DataFrame\n",
    "df_filtered = pl.concat(filtered_rows)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== Category Count ===\\n\")\n",
    "for cat, count in category_summary.items():\n",
    "    print(f\"{cat}: {count}\")\n",
    "\n",
    "print(\"\\n=== Keyword Details ===\\n\")\n",
    "for kw, count in sorted(keyword_summary.items(), key=lambda x: (-x[1], x[0])):\n",
    "    print(f\"{kw}: {count}\")\n",
    "\n",
    "print(\"\\n=== Preview of Filtered DataFrame ===\")\n",
    "print(\"Size of dataset\", df_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "650d5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the new dataset\n",
    "df_filtered.write_csv(\"../data/processed/data_pet_yelp.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
