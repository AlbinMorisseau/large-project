{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "866423b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3532b584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import torch\n",
    "\n",
    "# --- Choisir ton moteur de correction mot à mot ---\n",
    "# Option 1 : Hunspell\n",
    "try:\n",
    "    import hunspell\n",
    "    HUNSPELL_AVAILABLE = True\n",
    "    h_en = hunspell.HunSpell('/usr/share/hunspell/en_US.dic', '/usr/share/hunspell/en_US.aff')\n",
    "    h_fr = hunspell.HunSpell('/usr/share/hunspell/fr_FR.dic', '/usr/share/hunspell/fr_FR.aff')\n",
    "except:\n",
    "    HUNSPELL_AVAILABLE = False\n",
    "\n",
    "# Option 2 : PyEnchant\n",
    "if not HUNSPELL_AVAILABLE:\n",
    "    import enchant\n",
    "    d_en = enchant.Dict(\"en_US\")\n",
    "\n",
    "\n",
    "# --- Fonction mot par mot ---\n",
    "def correct_word(word, lang='en'):\n",
    "    if HUNSPELL_AVAILABLE:\n",
    "        h = h_en if lang == 'en' else h_fr\n",
    "        if h.spell(word):\n",
    "            return word\n",
    "        suggestions = h.suggest(word)\n",
    "        return suggestions[0] if suggestions else word\n",
    "    else:\n",
    "        d = d_en if lang == 'en' else d_fr\n",
    "        if d.check(word):\n",
    "            return word\n",
    "        suggestions = d.suggest(word)\n",
    "        return suggestions[0] if suggestions else word\n",
    "\n",
    "\n",
    "def correct_review_fast(review, lang='en'):\n",
    "    words = review.split()\n",
    "    return ' '.join([correct_word(w, lang) for w in words])\n",
    "\n",
    "\n",
    "# --- Découper les textes longs ---\n",
    "def split_text_into_chunks(text, max_chars=500):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [text]\n",
    "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "    for s in sentences:\n",
    "        if len(current) + len(s) <= max_chars:\n",
    "            current += \" \" + s if current else s\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "            current = s\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Correction contextuelle avec Transformer léger ---\n",
    "def init_transformer(model_path=\"../../models/grammar_correcter\"):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    from transformers import pipeline\n",
    "    corrector = pipeline(\"text2text-generation\", model=model_path, device=device)\n",
    "    return corrector\n",
    "\n",
    "\n",
    "def correct_text_transformer(text, corrector, max_length=1024, chunk_size=500):\n",
    "    chunks = split_text_into_chunks(text, chunk_size)\n",
    "    results = corrector(chunks, max_length=max_length)\n",
    "    corrected = \" \".join([r[\"generated_text\"] for r in results])\n",
    "    return corrected\n",
    "\n",
    "\n",
    "# --- Pipeline hybride ---\n",
    "def correct_reviews_hybrid(\n",
    "    df: pl.DataFrame,\n",
    "    column_name: str,\n",
    "    lang='en',\n",
    "    fast_threshold=500,  # review < fast_threshold chars -> correction rapide\n",
    "    batch_size_fast=5000,\n",
    "    batch_size_transformer=8,\n",
    "    transformer_model_path=\"./models/grammar_correcter\",\n",
    "    max_length=1024,\n",
    "    chunk_size=500,\n",
    "    n_threads=8\n",
    "):\n",
    "    # 1️⃣ Pré-correction rapide (Hunspell / PyEnchant) sur toutes les reviews\n",
    "    print(\"Step 1: Fast word-level correction (CPU, multithreaded)...\")\n",
    "    reviews = df[column_name].to_list()\n",
    "    \n",
    "    def fast_worker(batch):\n",
    "        return [correct_review_fast(r, lang) for r in batch]\n",
    "    \n",
    "    corrected_fast = []\n",
    "    with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "        for batch_start in tqdm(range(0, len(reviews), batch_size_fast), desc=\"Fast CPU correction\"):\n",
    "            batch = reviews[batch_start:batch_start + batch_size_fast]\n",
    "            corrected_fast.extend(executor.submit(fast_worker, batch).result())\n",
    "    \n",
    "    df = df.with_columns(pl.Series(name=column_name, values=corrected_fast))\n",
    "    \n",
    "    # 2️⃣ Correction contextuelle avec Transformer sur textes longs\n",
    "    print(\"Step 2: Transformer correction for long reviews (GPU)...\")\n",
    "    corrector = init_transformer(transformer_model_path)\n",
    "    \n",
    "    final_corrected = corrected_fast.copy()\n",
    "    \n",
    "    # Filtrer les reviews longues\n",
    "    long_indices = [i for i, r in enumerate(corrected_fast) if len(r) > fast_threshold]\n",
    "    \n",
    "    for i in tqdm(range(0, len(long_indices), batch_size_transformer), desc=\"Transformer GPU correction\"):\n",
    "        batch_indices = long_indices[i:i+batch_size_transformer]\n",
    "        for idx in batch_indices:\n",
    "            final_corrected[idx] = correct_text_transformer(\n",
    "                corrected_fast[idx],\n",
    "                corrector,\n",
    "                max_length=max_length,\n",
    "                chunk_size=chunk_size\n",
    "            )\n",
    "    \n",
    "    df_corrected = df.with_columns(pl.Series(name=column_name, values=final_corrected))\n",
    "    return df_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae3abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Fast word-level correction (CPU, multithreaded)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast CPU correction:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv(\"../../data/original/dataset/data_accessiblego.csv\")\n",
    "\n",
    "df_corrected = correct_reviews_hybrid(df, \"review\")\n",
    "\n",
    "df_corrected.write_csv(\"../../data/original/dataset/test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
