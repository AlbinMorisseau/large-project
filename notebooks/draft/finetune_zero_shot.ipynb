{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3f9b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 04:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I am looking for a product for my child\n",
      "Predicted class: child\n",
      "Scores: [0.9959827065467834, 0.00175605365075171, 0.001264249556697905, 0.000997065333649516]\n",
      "\n",
      "Text: I want to adopt a cat\n",
      "Predicted class: pet\n",
      "Scores: [0.9968515038490295, 0.001390138640999794, 0.000978102209046483, 0.0007802379550412297]\n",
      "\n",
      "Text: I need assistance for a disabled person\n",
      "Predicted class: handicap\n",
      "Scores: [0.9981399774551392, 0.000879904895555228, 0.0005158438580110669, 0.00046425481559708714]\n",
      "\n",
      "Text: I just want something else\n",
      "Predicted class: other\n",
      "Scores: [0.8058078289031982, 0.1796957403421402, 0.012700246647000313, 0.0017961935373023152]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1️⃣ Load dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"test_fine_tune.json\", \"test\": \"test_fine_tune.json\"}, field=\"train\")\n",
    "\n",
    "# 2️⃣ Load tokenizer and model\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 3️⃣ Preprocessing\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 4️⃣ Remove unused columns for Trainer\n",
    "dataset = dataset.remove_columns([\"premise\", \"hypothesis\"])\n",
    "\n",
    "# 5️⃣ Fine-tuning with Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 6️⃣ Test fine-tuned model with zero-shot pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_texts = [\n",
    "    \"I am looking for a product for my child\",\n",
    "    \"I want to adopt a cat\",\n",
    "    \"I need assistance for a disabled person\",\n",
    "    \"I just want something else\"\n",
    "]\n",
    "\n",
    "candidate_labels = [\"child\", \"handicap\", \"pet\", \"other\"]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classifier(text, candidate_labels)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Predicted class:\", result['labels'][0])\n",
    "    print(\"Scores:\", result['scores'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d68a7",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cce211a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d696eca729a4f17bd1c1e32ed786ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cb13a530ae4183ade2c7506933834d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8fce3e54bb4489b28d0ac8976a40e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5354d9c02b684ad28b712a098a5516f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x0000019AB9659940>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724b1a51d6d04fc2a499387f73f62c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967a5d70308d41318917fdee6e463a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m outputs = model(**inputs, labels=labels)\n\u001b[32m     77\u001b[39m loss = outputs.loss\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m optimizer.step()\n\u001b[32m     81\u001b[39m lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, GPT2Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ==== PARAMÈTRES ====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"  \n",
    "NUM_LABELS = 4  # adapte selon ton dataset\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 2e-5\n",
    "MAX_LEN = 512  \n",
    "\n",
    "# ==== DATA ====\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"finetune_dataset.csv\")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"review\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# PyTorch format\n",
    "label_cols = [\"handicap\", \"pet\", \"child\", \"other\"]\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "\n",
    "# ==== MODEL ====\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# ==== TRAIN ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = EPOCHS * len(train_ds)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def compute_f1(y_true, y_pred):\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    return f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "    for batch in loop:\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ==== EVAL ====\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "for batch in DataLoader(val_ds, batch_size=BATCH_SIZE):\n",
    "    labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    preds.append(sigmoid(logits.cpu()).numpy())\n",
    "    trues.append(labels)\n",
    "\n",
    "import numpy as np\n",
    "f1 = compute_f1(np.vstack(trues), np.vstack(preds))\n",
    "print(\"Validation F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c6d39",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from transformers import AdamW\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==== PARAMÈTRES ====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "NUM_LABELS = 4\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 512\n",
    "\n",
    "# ==== DATA ====\n",
    "df = pd.read_csv(\"finetune_dataset.csv\")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# === TOKENIZER ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # pour éviter les warnings\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"review\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "label_cols = [\"handicap\", \"pet\", \"child\", \"other\"]\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "\n",
    "# ==== MODEL LoRA ====\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                        # rang faible\n",
    "    lora_alpha=32,               # scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # couches attention\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"MULTILABEL_CLASSIFICATION\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# ==== DEVICE ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ==== OPTIMIZER + SCHEDULER ====\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = EPOCHS * len(train_ds) // BATCH_SIZE\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# ==== FONCTIONS UTILITAIRES ====\n",
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def compute_f1(y_true, y_pred):\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    return f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "# ==== TRAIN ====\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "    for batch in loop:\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ==== EVAL ====\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "for batch in DataLoader(val_ds, batch_size=BATCH_SIZE):\n",
    "    labels = torch.stack([batch[col] for col in label_cols], dim=1).numpy()\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    preds.append(sigmoid(logits.cpu()).numpy())\n",
    "    trues.append(labels)\n",
    "\n",
    "f1 = compute_f1(np.vstack(trues), np.vstack(preds))\n",
    "print(\"Validation F1:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
