{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3f9b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     23\u001b[39m training_args = TrainingArguments(\n\u001b[32m     24\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     push_to_hub=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m trainer = Trainer(\n\u001b[32m     35\u001b[39m     model=model,\n\u001b[32m     36\u001b[39m     args=training_args,\n\u001b[32m     37\u001b[39m     train_dataset=dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     38\u001b[39m     eval_dataset=dataset[\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     39\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 6️⃣ Test fine-tuned model with zero-shot pipeline\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\trainer.py:3845\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3843\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3845\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3847\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1️⃣ Load dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"test_fine_tune.json\", \"test\": \"test_fine_tune.json\"}, field=\"train\")\n",
    "\n",
    "# 2️⃣ Load tokenizer and model\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 3️⃣ Preprocessing\n",
    "def preprocess(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# 4️⃣ Remove unused columns for Trainer\n",
    "dataset = dataset.remove_columns([\"premise\", \"hypothesis\"])\n",
    "\n",
    "# 5️⃣ Fine-tuning with Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 6️⃣ Test fine-tuned model with zero-shot pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "test_texts = [\n",
    "    \"I am looking for a product for my child\",\n",
    "    \"I want to adopt a cat\",\n",
    "    \"I need assistance for a disabled person\",\n",
    "    \"I just want something else\"\n",
    "]\n",
    "\n",
    "candidate_labels = [\"child\", \"handicap\", \"pet\", \"other\"]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = classifier(text, candidate_labels)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Predicted class:\", result['labels'][0])\n",
    "    print(\"Scores:\", result['scores'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeddf71",
   "metadata": {},
   "source": [
    "# Jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7266cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les datasets\n",
    "df_keywords = pd.read_csv(\"key_words_data_accessiblego.csv\")  # contient id, review, keywords_found, category\n",
    "df_data = pd.read_csv(\"data_accessiblego.csv\")          # contient id, review\n",
    "\n",
    "# Choisir la catégorie\n",
    "category_to_filter = \"handicap\"  # remplace par la catégorie souhaitée\n",
    "\n",
    "# Filtrer sur la catégorie\n",
    "filtered_ids = df_keywords[df_keywords[\"category\"] == category_to_filter][\"id\"]\n",
    "\n",
    "# Récupérer les reviews correspondantes dans data.csv\n",
    "reviews_filtered = df_data[df_data[\"id\"].isin(filtered_ids)]\n",
    "\n",
    "reviews_only = reviews_filtered[[\"review\"]]\n",
    "\n",
    "\n",
    "# Exporter dans un CSV\n",
    "reviews_only.to_csv(\"filtered_reviews_handicap_accessiblego.csv\", index=False, header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0d68a7",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cce211a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc0c6487b6c4d73939da24ff22566d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b56bd3b70d451eb13975eaa1c6ac59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44676d3c2c1a4f12afb016c38fbd96a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5318a43b4c4c1caf9d4c881755f268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Exception ignored in: <function tqdm.__del__ at 0x000002EA89D21940>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     76\u001b[39m outputs = model(**inputs, labels=labels)\n\u001b[32m     77\u001b[39m loss = outputs.loss\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m optimizer.step()\n\u001b[32m     81\u001b[39m lr_scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, GPT2Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# ==== PARAMÈTRES ====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"  \n",
    "NUM_LABELS = 4  # adapte selon ton dataset\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 512  \n",
    "\n",
    "# ==== DATA ====\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"finetune_dataset.csv\")\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Hugging Face dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"review\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# PyTorch format\n",
    "label_cols = [\"handicap\", \"pet\", \"child\", \"other\"]\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "\n",
    "# ==== MODEL ====\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# ==== TRAIN ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = EPOCHS * len(train_ds)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "def sigmoid(x): return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "def compute_f1(y_true, y_pred):\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    return f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "    for batch in loop:\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# ==== EVAL ====\n",
    "model.eval()\n",
    "preds, trues = [], []\n",
    "for batch in DataLoader(val_ds, batch_size=BATCH_SIZE):\n",
    "    labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    preds.append(sigmoid(logits.cpu()).numpy())\n",
    "    trues.append(labels)\n",
    "\n",
    "import numpy as np\n",
    "f1 = compute_f1(np.vstack(trues), np.vstack(preds))\n",
    "print(\"Validation F1:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d90f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ed01edc7a54739831d6aaa671bf52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dffc6142ca440b990fe145e4356afea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043e06a470e142f8aa8d87c07adc4d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Évaluation avant fine-tuning ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000018695B59800>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0000\n",
      "precision: 0.2500\n",
      "recall: 1.0000\n",
      "f1: 0.4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92d7f6631ec4a4d96d6ab65ff293b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.6525 | Acc: 0.0000 | Prec: 0.2500 | Rec: 1.0000 | F1: 0.4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff67f1465a14a60a7d297ed85610c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.5877 | Acc: 0.0000 | Prec: 0.2667 | Rec: 0.8000 | F1: 0.4000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e679f146aeed4836a7910a141f76c1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m labels = torch.stack([batch[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_cols], dim=\u001b[32m1\u001b[39m).float().to(device)\n\u001b[32m    104\u001b[39m inputs = {k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m loss = outputs.loss\n\u001b[32m    107\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1079\u001b[39m, in \u001b[36mDebertaV2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1071\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1075\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1076\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1077\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:786\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    776\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    778\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    779\u001b[39m     input_ids=input_ids,\n\u001b[32m    780\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    784\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:659\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    657\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    669\u001b[39m         all_attentions = all_attentions + (attn_weights,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         logger.warning(message)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:438\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    431\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    436\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    437\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    447\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:371\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    364\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    380\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:251\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    250\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    256\u001b[39m     attention_scores = attention_scores + rel_att\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:304\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_attention_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    301\u001b[39m rel_embeddings = rel_embeddings[\u001b[32m0\u001b[39m : att_span * \u001b[32m2\u001b[39m, :].unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.share_att_key:\n\u001b[32m    303\u001b[39m     pos_query_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.num_attention_heads\n\u001b[32m    305\u001b[39m     ).repeat(query_layer.size(\u001b[32m0\u001b[39m) // \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    306\u001b[39m     pos_key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.key_proj(rel_embeddings), \u001b[38;5;28mself\u001b[39m.num_attention_heads).repeat(\n\u001b[32m    307\u001b[39m         query_layer.size(\u001b[32m0\u001b[39m) // \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m\n\u001b[32m    308\u001b[39m     )\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, DebertaV2Tokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==== PARAMÈTRES ====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "NUM_LABELS = 4\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 128\n",
    "label_cols = [\"handicap\", \"pet\", \"child\", \"other\"]\n",
    "\n",
    "# ==== DATA ====\n",
    "df = pd.read_csv(\"finetune_dataset.csv\").sample(n=50, random_state=42)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ==== TOKENIZER ====\n",
    "\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"review\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "\n",
    "# ==== MODEL ====\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "# ==== DEVICE ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ==== OPTIMIZER + SCHEDULER ====\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = EPOCHS * len(train_ds) // BATCH_SIZE\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# ==== FONCTION D'ÉVALUATION ====\n",
    "def evaluate(model, dataset, label_cols, batch_size=4, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).numpy()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds.append(probs)\n",
    "        trues.append(labels)\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    preds_bin = (preds > 0.3).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(trues, preds_bin),\n",
    "        \"precision\": precision_score(trues, preds_bin, average=\"micro\", zero_division=0),\n",
    "        \"recall\": recall_score(trues, preds_bin, average=\"micro\", zero_division=0),\n",
    "        \"f1\": f1_score(trues, preds_bin, average=\"micro\", zero_division=0)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ==== ÉVALUATION AVANT FINE-TUNING ====\n",
    "print(\"=== Évaluation avant fine-tuning ===\")\n",
    "pretrain_metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "for k, v in pretrain_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ==== TRAIN + SUIVI DES MÉTRIQUES ====\n",
    "history = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "    running_loss = 0\n",
    "    for batch in loop:\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Moyenne de la perte sur l'epoch\n",
    "    avg_loss = running_loss / len(loop)\n",
    "\n",
    "    # Évaluation sur validation\n",
    "    metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    history[\"loss\"].append(avg_loss)\n",
    "    history[\"accuracy\"].append(metrics[\"accuracy\"])\n",
    "    history[\"precision\"].append(metrics[\"precision\"])\n",
    "    history[\"recall\"].append(metrics[\"recall\"])\n",
    "    history[\"f1\"].append(metrics[\"f1\"])\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f} | Acc: {metrics['accuracy']:.4f} | \"\n",
    "          f\"Prec: {metrics['precision']:.4f} | Rec: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "# ==== PLOT DES MÉTRIQUES ====\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(EPOCHS), history[\"loss\"], label=\"Loss\")\n",
    "plt.plot(range(EPOCHS), history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.plot(range(EPOCHS), history[\"precision\"], label=\"Precision\")\n",
    "plt.plot(range(EPOCHS), history[\"recall\"], label=\"Recall\")\n",
    "plt.plot(range(EPOCHS), history[\"f1\"], label=\"F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Valeur\")\n",
    "plt.title(\"Évolution des métriques pendant le fine-tuning\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==== ÉVALUATION FINALE APRÈS FINE-TUNING ====\n",
    "print(\"\\n=== Évaluation finale après fine-tuning ===\")\n",
    "final_metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62cbdc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probs: [[0.2139775  0.3614397  0.26746103 0.2835042 ]\n",
      " [0.20355363 0.3627451  0.2876741  0.2838162 ]\n",
      " [0.21516015 0.37406728 0.2520137  0.2873677 ]\n",
      " [0.27090776 0.37614912 0.2789748  0.29411465]]\n",
      "Labels: tensor([[0, 1, 0, 0],\n",
      "        [0, 0, 1, 0],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(DataLoader(val_ds, batch_size=4)))\n",
    "inputs = {k: v.to(device) for k,v in batch.items() if k in [\"input_ids\",\"attention_mask\"]}\n",
    "labels = torch.stack([batch[col] for col in label_cols], dim=1)\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "probs = torch.sigmoid(logits).cpu().numpy()\n",
    "print(\"Probs:\", probs)\n",
    "print(\"Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c6d39",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44b36adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faca0c30eeb14ef2ba47a3d4e49497c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/286 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046ad8e27b4e434284f87af7e0e874fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96aa7487730c402795553bf21e757e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Évaluation avant fine-tuning ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x0000021C58339760>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"c:\\Users\\emma\\Desktop\\project\\large-project\\venv\\Lib\\site-packages\\tqdm\\notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0000\n",
      "precision: 0.2273\n",
      "recall: 0.5128\n",
      "f1: 0.3150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2db95491174e62bbe16a583a199e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.6845 | Acc: 0.0556 | Prec: 0.2424 | Rec: 0.2051 | F1: 0.2222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3d2d920ac14463a73f22ba8cf25d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.6428 | Acc: 0.0556 | Prec: 0.4000 | Rec: 0.0513 | F1: 0.0909\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa9270b5b4043ac812ca48b78f164f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.6286 | Acc: 0.0278 | Prec: 0.3333 | Rec: 0.0256 | F1: 0.0476\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Évaluation finale après fine-tuning ===\n",
      "accuracy: 0.0278\n",
      "precision: 0.3333\n",
      "recall: 0.0256\n",
      "f1: 0.0476\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler, GPT2Tokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== PARAMÈTRES ====\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "NUM_LABELS = 4\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "MAX_LEN = 256\n",
    "label_cols = [\"handicap\", \"pet\", \"child\", \"other\"]\n",
    "\n",
    "# ==== DATA ====\n",
    "df = pd.read_csv(\"finetune_dataset.csv\")\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "# ==== TOKENIZER ====\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"review\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"] + label_cols)\n",
    "\n",
    "# ==== MODEL LoRA ====\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=None,  # détecte automatiquement les bonnes couches\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# ==== DEVICE ====\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ==== OPTIMIZER + SCHEDULER ====\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "num_training_steps = EPOCHS * len(train_ds) // BATCH_SIZE\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# ==== FONCTION D'ÉVALUATION ====\n",
    "def evaluate(model, dataset, label_cols, batch_size=4, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    for batch in DataLoader(dataset, batch_size=batch_size):\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).numpy()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds.append(probs)\n",
    "        trues.append(labels)\n",
    "    \n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    preds_bin = (preds > 0.5).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(trues, preds_bin),\n",
    "        \"precision\": precision_score(trues, preds_bin, average=\"micro\", zero_division=0),\n",
    "        \"recall\": recall_score(trues, preds_bin, average=\"micro\", zero_division=0),\n",
    "        \"f1\": f1_score(trues, preds_bin, average=\"micro\", zero_division=0)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ==== ÉVALUATION AVANT FINE-TUNING ====\n",
    "print(\"=== Évaluation avant fine-tuning ===\")\n",
    "pretrain_metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "for k, v in pretrain_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ==== TRAIN + SUIVI DES MÉTRIQUES ====\n",
    "history = {\"loss\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    loop = tqdm(DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True))\n",
    "    running_loss = 0\n",
    "    for batch in loop:\n",
    "        labels = torch.stack([batch[col] for col in label_cols], dim=1).float().to(device)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Moyenne de la loss sur l'epoch\n",
    "    avg_loss = running_loss / len(loop)\n",
    "    \n",
    "    # Évaluation sur validation\n",
    "    metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "\n",
    "    history[\"loss\"].append(avg_loss)\n",
    "    history[\"accuracy\"].append(metrics[\"accuracy\"])\n",
    "    history[\"precision\"].append(metrics[\"precision\"])\n",
    "    history[\"recall\"].append(metrics[\"recall\"])\n",
    "    history[\"f1\"].append(metrics[\"f1\"])\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {avg_loss:.4f} | Acc: {metrics['accuracy']:.4f} | \"\n",
    "          f\"Prec: {metrics['precision']:.4f} | Rec: {metrics['recall']:.4f} | F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "# ==== PLOT DES MÉTRIQUES ====\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(EPOCHS), history[\"loss\"], label=\"Loss\")\n",
    "plt.plot(range(EPOCHS), history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.plot(range(EPOCHS), history[\"precision\"], label=\"Precision\")\n",
    "plt.plot(range(EPOCHS), history[\"recall\"], label=\"Recall\")\n",
    "plt.plot(range(EPOCHS), history[\"f1\"], label=\"F1\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Valeur\")\n",
    "plt.title(\"Évolution des métriques pendant le fine-tuning\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ==== ÉVALUATION FINALE APRÈS FINE-TUNING ====\n",
    "print(\"\\n=== Évaluation finale après fine-tuning ===\")\n",
    "final_metrics = evaluate(model, val_ds, label_cols, batch_size=BATCH_SIZE, device=device)\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
