{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd9b8a0",
   "metadata": {},
   "source": [
    "# Evaluation classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe941f",
   "metadata": {},
   "source": [
    "This notebook aims to evaluate on the same 200 labeeled reviews the 3 following methods of classification used ithin our pipelines :\n",
    "\n",
    "- Keywords extractions\n",
    "- BERT model finetunned\n",
    "- LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "443a5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re \n",
    "import ollama\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff5dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filterign HTTP logging\n",
    "class HttpStatusFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        message = record.getMessage()\n",
    "        if 'HTTP/1.1 200' not in message:\n",
    "            record.levelname = \"WARNING\"\n",
    "            record.levelno = logging.WARNING\n",
    "        return 'HTTP/1.1 200' not in message\n",
    "    \n",
    "logging.getLogger(\"httpx\").addFilter(HttpStatusFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61551618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 20:30:10,452 - INFO - NUM_THREAD fixed to 8\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b18db2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 18:43:43,574 - INFO - Loading test data...\n"
     ]
    }
   ],
   "source": [
    "# Categories\n",
    "classes = [\"handicap\", \"pet\", \"child\"]\n",
    "\n",
    "# Test data loading\n",
    "logger.info(\"Loading test data...\")\n",
    "df = pd.read_csv(\"../../data/original/fine_tunning/data_test.csv\")\n",
    "y_true = df[classes].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571fc82",
   "metadata": {},
   "source": [
    "### Evaluation of Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff6b65ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9600\n",
      " Precision: 1.0000\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9298\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9500\n",
      " Precision: 0.8732\n",
      " Recall   : 0.9841\n",
      " F1-score : 0.9254\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.8615\n",
      " Recall   : 1.0000\n",
      " F1-score : 0.9256\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9048, Recall: 0.9500, F1: 0.9268\n",
      " Macro Precision: 0.9116, Recall: 0.9510, F1: 0.9269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# --- Load datasets ---\n",
    "df_truth = df\n",
    "df_kw = pd.read_csv(\"../../data/processed/data_categorized/key_words_data_test.csv\")\n",
    "\n",
    "classes = [\"handicap\", \"pet\", \"child\"]\n",
    "\n",
    "# Initialize prediciton at 0\n",
    "df_pred = pd.DataFrame(0, index=df_truth.index, columns=classes)\n",
    "df_pred[\"id\"] = df_truth[\"id\"]\n",
    "\n",
    "# Prediction\n",
    "# Each row in df_kw corresponds to one detected keyword for a review\n",
    "# We can have multiple rows for the same review if multiple keywords/categories\n",
    "for idx, row in df_kw.iterrows():\n",
    "    review_id = row[\"id\"]\n",
    "    cat = str(row[\"category\"]).strip().lower()\n",
    "    if cat in classes:\n",
    "        # Find the corresponding index in df_pred\n",
    "        pred_idx = df_pred.index[df_pred[\"id\"] == review_id].tolist()\n",
    "        if pred_idx:\n",
    "            df_pred.at[pred_idx[0], cat] = 1\n",
    "\n",
    "# Extract numpy arrays\n",
    "y_true = df_truth[classes].values\n",
    "y_pred = df_pred[classes].values\n",
    "\n",
    "# Compute metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab70e73",
   "metadata": {},
   "source": [
    "### Evaluation of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1921e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 18:05:10,474 - INFO - Loading tokenizer...\n",
      "2025-11-24 18:05:10,512 - INFO - Loading model...\n",
      "2025-11-24 18:05:12,088 - INFO - Predicting...\n",
      "2025-11-24 18:05:14,066 - INFO - Metrics multilabel\n",
      "2025-11-24 18:05:14,095 - INFO - Global metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      "Accuracy : 0.8600\n",
      "Precision: 0.9714\n",
      "Recall   : 0.5574\n",
      "F1-score : 0.7083\n",
      "\n",
      "Label: pet\n",
      "Accuracy : 0.9300\n",
      "Precision: 0.8551\n",
      "Recall   : 0.9365\n",
      "F1-score : 0.8939\n",
      "\n",
      "Label: child\n",
      "Accuracy : 0.9300\n",
      "Precision: 0.8500\n",
      "Recall   : 0.9107\n",
      "F1-score : 0.8793\n",
      "\n",
      "Micro Precision: 0.8780, Recall: 0.8000, F1: 0.8372\n",
      "Macro Precision: 0.8922, Recall: 0.8015, F1: 0.8272\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "BERT_PATH = \"../../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"../bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"../bert/best_weights.pth\"\n",
    "MAX_SEQ_LEN = 128\n",
    "threshold = 0.95\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model definition\n",
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "logger.info(\"Loading model...\")\n",
    "model = BertMultiLabelClassifier(n_classes=len(classes))\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encodign function\n",
    "def encode_batch(sentences):\n",
    "    encoded = tokenizer(\n",
    "        list(sentences),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Prediction\n",
    "logger.info(\"Predicting...\")\n",
    "input_ids, attention_mask = encode_batch(df[\"review\"])\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "\n",
    "y_pred_bin = (pred > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Metrics\n",
    "logger.info(\"Metrics multilabel\")\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred_bin[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "rec_micro = recall_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "rec_macro = recall_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "\n",
    "logger.info(\"Global metrics\")\n",
    "print(f\"Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\"Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1e8df",
   "metadata": {},
   "source": [
    "### Evaluation of LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a24c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.9815\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9217\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9350\n",
      " Precision: 0.8906\n",
      " Recall   : 0.9048\n",
      " F1-score : 0.8976\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9250\n",
      " Precision: 0.8596\n",
      " Recall   : 0.8750\n",
      " F1-score : 0.8673\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9086, Recall: 0.8833, F1: 0.8958\n",
      " Macro Precision: 0.9106, Recall: 0.8829, F1: 0.8955\n"
     ]
    }
   ],
   "source": [
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification via Ollama\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\n",
    "             \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "             f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "             f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "             \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "         )},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=messages,options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "def classify_all_categories(review):\n",
    "    return [classify_review_ollama(review, category) for category in classes]\n",
    "\n",
    "# Prediction\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREAD) as executor:\n",
    "    y_pred = list(executor.map(classify_all_categories, df[\"review\"]))\n",
    "\n",
    "# Convertir en matrice numpy si besoin :\n",
    "import numpy as np\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
