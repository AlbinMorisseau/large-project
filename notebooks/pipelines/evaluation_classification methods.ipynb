{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd9b8a0",
   "metadata": {},
   "source": [
    "# Evaluation classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe941f",
   "metadata": {},
   "source": [
    "This notebook aims to evaluate on the same 200 labeeled reviews the 3 following methods of classification used ithin our pipelines :\n",
    "\n",
    "- Keywords extractions\n",
    "- BERT model finetunned\n",
    "- LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443a5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re \n",
    "import ollama\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff5dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filterign HTTP logging\n",
    "class HttpStatusFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        message = record.getMessage()\n",
    "        if 'HTTP/1.1 200' not in message:\n",
    "            record.levelname = \"WARNING\"\n",
    "            record.levelno = logging.WARNING\n",
    "        return 'HTTP/1.1 200' not in message\n",
    "    \n",
    "logging.getLogger(\"httpx\").addFilter(HttpStatusFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61551618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:44:33,230 - INFO - NUM_THREAD fixed to 8\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18db2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:44:36,309 - INFO - Loading test data...\n"
     ]
    }
   ],
   "source": [
    "# Categories\n",
    "classes = [\"handicap\", \"pet\", \"child\"]\n",
    "\n",
    "# Test data loading\n",
    "logger.info(\"Loading test data...\")\n",
    "df = pd.read_csv(\"../../data/original/fine_tunning/data_test.csv\")\n",
    "y_true = df[classes].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571fc82",
   "metadata": {},
   "source": [
    "### Evaluation of Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f354659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9600\n",
      " Precision: 1.0000\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9298\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.8732\n",
      " Recall   : 1.0000\n",
      " F1-score : 0.9323\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9600\n",
      " Precision: 0.8769\n",
      " Recall   : 1.0000\n",
      " F1-score : 0.9344\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9101, Recall: 0.9556, F1: 0.9322\n",
      " Macro Precision: 0.9167, Recall: 0.9563, F1: 0.9322\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_truth = df \n",
    "df_kw = pd.read_csv(\"../../data/processed/data_categorized/key_words_data_test.csv\")\n",
    "\n",
    "# Combine keywords per review (each review now has a unique id)\n",
    "df_kw = (\n",
    "    df_kw.groupby(\"id\", as_index=False)\n",
    "         .agg({\n",
    "             \"review\": \"first\",\n",
    "             \"category\": lambda x: \" \".join(x.astype(str))\n",
    "         })\n",
    ")\n",
    "\n",
    "# Initialize predictions at 0\n",
    "df_pred = pd.DataFrame(0, index=df_truth.index, columns=classes)\n",
    "df_pred[\"id\"] = df_truth[\"id\"]\n",
    "\n",
    "# Prediction using keywords\n",
    "df_pred = df_pred.set_index(\"id\")\n",
    "\n",
    "for _, row in df_kw.iterrows():\n",
    "    review_id = row[\"id\"]\n",
    "    cat_list = str(row[\"category\"]).strip().lower().split()  # split in case multiple categories concatenated\n",
    "    for cat in cat_list:\n",
    "        if cat in classes and review_id in df_pred.index:\n",
    "            df_pred.at[review_id, cat] = 1\n",
    "\n",
    "df_pred = df_pred.reset_index()  # restore id column\n",
    "\n",
    "# Align truth and prediction\n",
    "df_truth = df_truth.sort_values(\"id\").reset_index(drop=True)\n",
    "df_pred  = df_pred.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "y_true = df_truth[classes].values\n",
    "y_pred = df_pred[classes].values\n",
    "\n",
    "\n",
    "# Compute metrics per class\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab70e73",
   "metadata": {},
   "source": [
    "### Evaluation of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1921e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 18:29:45,635 - INFO - Loading tokenizer...\n",
      "2025-11-25 18:29:45,654 - INFO - Loading model...\n",
      "2025-11-25 18:29:46,283 - INFO - Predicting...\n",
      "2025-11-25 18:29:47,770 - INFO - Metrics multilabel\n",
      "2025-11-25 18:29:47,787 - INFO - Global metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      "Accuracy : 0.9300\n",
      "Precision: 0.9608\n",
      "Recall   : 0.8033\n",
      "F1-score : 0.8750\n",
      "\n",
      "Label: pet\n",
      "Accuracy : 0.9300\n",
      "Precision: 0.8636\n",
      "Recall   : 0.9194\n",
      "F1-score : 0.8906\n",
      "\n",
      "Label: child\n",
      "Accuracy : 0.9550\n",
      "Precision: 0.9138\n",
      "Recall   : 0.9298\n",
      "F1-score : 0.9217\n",
      "\n",
      "Micro Precision: 0.9086, Recall: 0.8833, F1: 0.8958\n",
      "Macro Precision: 0.9127, Recall: 0.8842, F1: 0.8958\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "BERT_PATH = \"../../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"../bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"../bert/best_weights_v3.pth\"\n",
    "MAX_SEQ_LEN = 256\n",
    "threshold = 0.95\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model definition\n",
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "logger.info(\"Loading model...\")\n",
    "model = BertMultiLabelClassifier(n_classes=len(classes))\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encodign function\n",
    "def encode_batch(sentences):\n",
    "    encoded = tokenizer(\n",
    "        list(sentences),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Prediction\n",
    "logger.info(\"Predicting...\")\n",
    "input_ids, attention_mask = encode_batch(df[\"review\"])\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "\n",
    "y_pred_bin = (pred > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Metrics\n",
    "logger.info(\"Metrics multilabel\")\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred_bin[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "rec_micro = recall_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "rec_macro = recall_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "\n",
    "logger.info(\"Global metrics\")\n",
    "print(f\"Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\"Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1e8df",
   "metadata": {},
   "source": [
    "### Evaluation of LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a24c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.9815\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9217\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9400\n",
      " Precision: 0.8906\n",
      " Recall   : 0.9194\n",
      " F1-score : 0.9048\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9300\n",
      " Precision: 0.8772\n",
      " Recall   : 0.8772\n",
      " F1-score : 0.8772\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9143, Recall: 0.8889, F1: 0.9014\n",
      " Macro Precision: 0.9164, Recall: 0.8885, F1: 0.9012\n"
     ]
    }
   ],
   "source": [
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification via Ollama\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\n",
    "             \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "             f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "             f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "             \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "         )},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=messages,options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "def classify_all_categories(review):\n",
    "    return [classify_review_ollama(review, category) for category in classes]\n",
    "\n",
    "# Prediction\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREAD) as executor:\n",
    "    y_pred = list(executor.map(classify_all_categories, df[\"review\"]))\n",
    "\n",
    "# Convertir en matrice numpy si besoin :\n",
    "import numpy as np\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45f7ec",
   "metadata": {},
   "source": [
    "# Evaluation prompt few shot mistral small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e45a4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9750\n",
      " Precision: 0.9667\n",
      " Recall   : 0.9508\n",
      " F1-score : 0.9587\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9650\n",
      " Precision: 0.9365\n",
      " Recall   : 0.9516\n",
      " F1-score : 0.9440\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9200\n",
      " Precision: 0.8475\n",
      " Recall   : 0.8772\n",
      " F1-score : 0.8621\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9176, Recall: 0.9278, F1: 0.9227\n",
      " Macro Precision: 0.9169, Recall: 0.9265, F1: 0.9216\n"
     ]
    }
   ],
   "source": [
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification with Ollama \"\"\"\n",
    "    \n",
    "    if category == \"child\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                  \"You are a strict family-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with children. Especially, you need to determine\"\n",
    "                 \" if these children have a high chance to be under 18 years old.\"\n",
    "                 \"Respond strictly with 'yes' if the review indicates people travelling with children, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"We traveled with our kids and loved the family-friendly pool.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The hotel was great, but we went alone as a couple.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I got there at 6:30, and a kid that apparently worked there (no id/uniform) was scrambling to set everything up\\\" -> no\\n\"\n",
    "                 \"Review: \\\"My Grand kids loved the pool\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I travelled to Dakota to see my son graduatation\\\" -> no\\n\"\n",
    "                 \"Review: \\\"This family owned business has a welcoming staff which made us feel right at home\\\" -> no\\n\"\n",
    "                 \"Review: \\\"If I had to ask one thing of Best Western, please replace the mattresses or box springs every time our kids moved at night\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"pet\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict pet-friendly-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with pets. \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates they travel with pets, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Thanks again Cat!\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I only booked this hotel because it was dog friendly\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I wanted to see if I could bring my service dog with me but they told me it was impossible at the front desk.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The bedsheets were smelling cat urine. Horrible !\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Perfect for travelers with cats or dogs.\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"handicap\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict business-travel-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) have any type of handicap or if the reviews contains a specific needs\"\n",
    "                 \"associated with a disability (transporations, amenities, etc.) \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates a handicaped traveler or a special need related to handicap travelling, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Plant to go to London in September Need information about Accessible Van in London airport\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The room was great, big enough to move around in my power chair in both the bedroom and bathroom\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I would like to sell my wheelchair.please contact me\\\" -> no\\n\"\n",
    "                 \"Review: \\\"It's new digital travel magazine targeted exclusively for travelers with disabilities.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Nice roll-in shower with a pull-down bench, but the amenities were again too high\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown category: {category}\")\n",
    "    \n",
    "    # Ici tu peux directement envoyer `messages` √† Ollama\n",
    "    response = ollama.chat(model=model, messages=messages, options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "\n",
    "def classify_all_categories(review):\n",
    "    return [classify_review_ollama(review, category) for category in classes]\n",
    "\n",
    "# Prediction\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREAD) as executor:\n",
    "    y_pred = list(executor.map(classify_all_categories, df[\"review\"]))\n",
    "\n",
    "# Convertir en matrice numpy si besoin :\n",
    "import numpy as np\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b224ee",
   "metadata": {},
   "source": [
    "## Evluation using Human validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fcff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Traitement cat√©gorie : child\n",
      "üìÇ Traitement cat√©gorie : handicap\n",
      "üìÇ Traitement cat√©gorie : pet\n",
      "\n",
      "========== STATISTIQUES GLOBALES ==========\n",
      "{'agreed': 1609, 'llm_validated': 1261, 'disputed': 50}\n",
      "\n",
      "--- Par subset (good/rejected) ---\n",
      "validation_status  agreed  disputed  llm_validated\n",
      "subset                                            \n",
      "good                 1016        31            819\n",
      "rejected              593        19            442\n",
      "\n",
      "========== STATISTIQUES PAR CAT√âGORIE ==========\n",
      "validation_status  agreed  disputed  llm_validated\n",
      "category                                          \n",
      "child                 346        26           1025\n",
      "handicap              664         0             91\n",
      "pet                   599        24            145\n",
      "\n",
      "========== STATISTIQUES PAR CAT√âGORIE ET SUBSET ==========\n",
      "validation_status  agreed  disputed  llm_validated\n",
      "category subset                                   \n",
      "child    good         228        17            661\n",
      "         rejected     118         9            364\n",
      "handicap good         465         0             70\n",
      "         rejected     199         0             21\n",
      "pet      good         323        14             88\n",
      "         rejected     276        10             57\n",
      "\n",
      "========== STATISTIQUES PAR DATASET ==========\n",
      "validation_status                                   agreed  disputed  \\\n",
      "dataset_file                                                           \n",
      "validated__data_booking_handicap_good.csv               67         0   \n",
      "validated__data_booking_handicap_rejected.csv           33         0   \n",
      "validated__data_booking_pet_good.csv                    91         1   \n",
      "validated__data_booking_pet_rejected.csv                30         2   \n",
      "validated_data_accessiblego_child_good.csv               0         0   \n",
      "...                                                    ...       ...   \n",
      "validated_data_restaurant_reviews_2_handicap_re...       1         0   \n",
      "validated_data_restaurant_reviews_2_pet_good.csv         1         0   \n",
      "validated_data_restaurant_reviews_2_pet_rejecte...       0         0   \n",
      "validated_data_twitter_child_rejected.csv                9         0   \n",
      "validated_data_twitter_pet_rejected.csv                148         3   \n",
      "\n",
      "validation_status                                   llm_validated  \n",
      "dataset_file                                                       \n",
      "validated__data_booking_handicap_good.csv                      12  \n",
      "validated__data_booking_handicap_rejected.csv                   4  \n",
      "validated__data_booking_pet_good.csv                           16  \n",
      "validated__data_booking_pet_rejected.csv                        4  \n",
      "validated_data_accessiblego_child_good.csv                     12  \n",
      "...                                                           ...  \n",
      "validated_data_restaurant_reviews_2_handicap_re...              0  \n",
      "validated_data_restaurant_reviews_2_pet_good.csv                0  \n",
      "validated_data_restaurant_reviews_2_pet_rejecte...              2  \n",
      "validated_data_twitter_child_rejected.csv                      78  \n",
      "validated_data_twitter_pet_rejected.csv                        21  \n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_DIR = Path(\"../../data/processed/final\")\n",
    "ORIGINAL_DIR = Path(\"../../data/processed/data_validated\")\n",
    "\n",
    "CATEGORIES = [\"child\", \"handicap\", \"pet\"]\n",
    "\n",
    "\n",
    "def load_original_dataset(category):\n",
    "    \"\"\"Charge le CSV original de la cat√©gorie.\"\"\"\n",
    "    original_path = ORIGINAL_DIR / category\n",
    "    # on cherche le ou les fichiers du dataset original\n",
    "    files = list(original_path.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouv√© pour la cat√©gorie : {category}\")\n",
    "    \n",
    "    # S'il y a plusieurs fichiers, on les concat√®ne\n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_validated_files(category):\n",
    "    \"\"\"Charge les fichiers good/rejected d'une cat√©gorie.\"\"\"\n",
    "    folder = BASE_DIR / category\n",
    "    files = list(folder.glob(\"*.csv\"))\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_category(category):\n",
    "    original_df = load_original_dataset(category)\n",
    "    validated_files = load_validated_files(category)\n",
    "\n",
    "    all_records = []\n",
    "\n",
    "    for file in validated_files:\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # D√©terminer s'il s'agit d'un fichier good ou rejected\n",
    "        status_type = \"good\" if \"good\" in file.stem.lower() else \"rejected\"\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            orig_idx = int(row[\"original_index\"]) + 2  # d√©calage demand√©\n",
    "            \n",
    "            if orig_idx >= len(original_df):\n",
    "                print(f\"‚ö†Ô∏è Index hors limite dans {file.name} ({orig_idx})\")\n",
    "                continue\n",
    "            \n",
    "            orig_row = original_df.iloc[orig_idx]\n",
    "\n",
    "            all_records.append({\n",
    "                \"dataset_file\": file.name,\n",
    "                \"category\": category,\n",
    "                \"subset\": status_type,  # good / rejected\n",
    "                \"validation_status\": orig_row.get(\"validation_status\", None),\n",
    "                \"llm_child\": orig_row.get(\"llm_child\", None),\n",
    "                \"llm_pet\": orig_row.get(\"llm_pet\", None),\n",
    "                \"llm_handicap\": orig_row.get(\"llm_handicap\", None)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "\n",
    "def compute_statistics(df):\n",
    "    stats = {}\n",
    "\n",
    "    # Statistiques globales\n",
    "    stats[\"global_counts\"] = df[\"validation_status\"].value_counts().to_dict()\n",
    "    stats[\"global_by_subset\"] = df.groupby(\"subset\")[\"validation_status\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Statistiques par cat√©gorie\n",
    "    stats[\"by_category\"] = df.groupby(\"category\")[\"validation_status\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Statistiques cat√©gorie √ó subset\n",
    "    stats[\"by_category_and_subset\"] = df.groupby([\"category\", \"subset\"])[\"validation_status\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Statistiques par dataset (fichier)\n",
    "    stats[\"by_dataset\"] = df.groupby(\"dataset_file\")[\"validation_status\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "# MAIN WORKFLOW\n",
    "all_categories_df = []\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    print(f\"Traitement cat√©gorie : {category}\")\n",
    "    df_cat = process_category(category)\n",
    "    all_categories_df.append(df_cat)\n",
    "\n",
    "final_df = pd.concat(all_categories_df, ignore_index=True)\n",
    "\n",
    "stats = compute_statistics(final_df)\n",
    "\n",
    "# Affichage final\n",
    "print(\"\\n========== STATISTIQUES GLOBALES ==========\")\n",
    "print(stats[\"global_counts\"])\n",
    "print(\"\\n--- Par subset (good/rejected) ---\")\n",
    "print(stats[\"global_by_subset\"])\n",
    "\n",
    "print(\"\\n========== STATISTIQUES PAR CAT√âGORIE ==========\")\n",
    "print(stats[\"by_category\"])\n",
    "\n",
    "print(\"\\n========== STATISTIQUES PAR CAT√âGORIE ET SUBSET ==========\")\n",
    "print(stats[\"by_category_and_subset\"])\n",
    "\n",
    "print(\"\\n========== STATISTIQUES PAR DATASET ==========\")\n",
    "print(stats[\"by_dataset\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3db3ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== PIPELINE VALIDATION METRICS ======\n",
      "\n",
      "agreement_rate: 0.5510273972602739\n",
      "\n",
      "disputed_rate: 0.017123287671232876\n",
      "\n",
      "accuracy_agreed: {'child': np.float64(0.0), 'pet': np.float64(0.0), 'handicap': np.float64(0.0), 'global_mean': np.float64(0.0)}\n",
      "\n",
      "bert_added_value: 0.8424657534246576\n",
      "\n",
      "llm_validates_keywords: 0.0\n",
      "\n",
      "llm_validates_bert: 0.8424657534246576\n",
      "\n",
      "confusion_keywords_vs_llm: {'child': '‚ö†Ô∏è No valid data for confusion matrix', 'handicap': '‚ö†Ô∏è No valid data for confusion matrix', 'pet': '‚ö†Ô∏è No valid data for confusion matrix'}\n",
      "\n",
      "confusion_bert_vs_llm: {'child': (array([[1420,    0],\n",
      "       [1032,  468]]), ['0', '1']), 'handicap': (array([[1001,    0],\n",
      "       [ 245, 1674]]), ['0', '1']), 'pet': (array([[2122,    0],\n",
      "       [ 103,  695]]), ['0', '1'])}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "BASE_DIR = Path(\"../../data/processed/final\")\n",
    "ORIGINAL_DIR = Path(\"../../data/processed/data_validated\")\n",
    "\n",
    "CATEGORIES = [\"child\", \"handicap\", \"pet\"]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Chargement des fichiers originaux et des fichiers valid√©s\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def load_original_dataset(category):\n",
    "    \"\"\"Charge le CSV original de la cat√©gorie.\"\"\"\n",
    "    original_path = ORIGINAL_DIR / category\n",
    "    files = list(original_path.glob(\"*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Aucun fichier trouv√© pour la cat√©gorie : {category}\")\n",
    "    \n",
    "    dfs = [pd.read_csv(f) for f in files]\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def load_validated_files(category):\n",
    "    folder = BASE_DIR / category\n",
    "    return list(folder.glob(\"*.csv\"))\n",
    "\n",
    "\n",
    "def process_category(category):\n",
    "    original_df = load_original_dataset(category)\n",
    "    validated_files = load_validated_files(category)\n",
    "\n",
    "    all_records = []\n",
    "\n",
    "    for file in validated_files:\n",
    "        df = pd.read_csv(file)\n",
    "        subset = \"good\" if \"good\" in file.stem.lower() else \"rejected\"\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            orig_idx = int(row[\"original_index\"]) + 2\n",
    "            if orig_idx >= len(original_df):\n",
    "                continue\n",
    "\n",
    "            orig_row = original_df.iloc[orig_idx]\n",
    "\n",
    "            all_records.append({\n",
    "                \"dataset_file\": file.name,\n",
    "                \"category\": category,\n",
    "                \"subset\": subset,\n",
    "\n",
    "                # Pipeline outputs\n",
    "                \"validation_status\": orig_row.get(\"validation_status\", None),\n",
    "\n",
    "                # Predictions\n",
    "                \"keywords_child\": orig_row.get(\"keywords_child\", None),\n",
    "                \"bert_child\": orig_row.get(\"bert_child\", None),\n",
    "                \"llm_child\": orig_row.get(\"llm_child\", None),\n",
    "\n",
    "                \"keywords_pet\": orig_row.get(\"keywords_pet\", None),\n",
    "                \"bert_pet\": orig_row.get(\"bert_pet\", None),\n",
    "                \"llm_pet\": orig_row.get(\"llm_pet\", None),\n",
    "\n",
    "                \"keywords_handicap\": orig_row.get(\"keywords_handicap\", None),\n",
    "                \"bert_handicap\": orig_row.get(\"bert_handicap\", None),\n",
    "                \"llm_handicap\": orig_row.get(\"llm_handicap\", None),\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(all_records)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Fusion globale des cat√©gories\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "all_categories_df = []\n",
    "for category in CATEGORIES:\n",
    "    df_cat = process_category(category)\n",
    "    all_categories_df.append(df_cat)\n",
    "\n",
    "df = pd.concat(all_categories_df, ignore_index=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Fonctions de m√©triques\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def agreement_rate(df):\n",
    "    return (df.validation_status == \"agreed\").mean()\n",
    "\n",
    "\n",
    "def disputed_rate(df):\n",
    "    return (df.validation_status == \"disputed\").mean()\n",
    "\n",
    "\n",
    "def accuracy_agreed(df):\n",
    "    \"\"\"On v√©rifie si agreed signifie r√©ellement coh√©rence keywords=BERT=LLM.\"\"\"\n",
    "    agreed_df = df[df.validation_status == \"agreed\"]\n",
    "\n",
    "    acc_child = (agreed_df[\"keywords_child\"] == agreed_df[\"llm_child\"]).mean()\n",
    "    acc_pet = (agreed_df[\"keywords_pet\"] == agreed_df[\"llm_pet\"]).mean()\n",
    "    acc_handicap = (agreed_df[\"keywords_handicap\"] == agreed_df[\"llm_handicap\"]).mean()\n",
    "\n",
    "    return {\n",
    "        \"child\": acc_child,\n",
    "        \"pet\": acc_pet,\n",
    "        \"handicap\": acc_handicap,\n",
    "        \"global_mean\": (acc_child + acc_pet + acc_handicap) / 3\n",
    "    }\n",
    "\n",
    "\n",
    "def bert_added_value(df):\n",
    "    \"\"\"Cas o√π le LLM valide BERT mais pas keywords.\"\"\"\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    for cat in CATEGORIES:\n",
    "        kw = df[f\"keywords_{cat}\"]\n",
    "        bert = df[f\"bert_{cat}\"]\n",
    "        llm = df[f\"llm_{cat}\"]\n",
    "\n",
    "        mask = (bert == llm) & (kw != llm)\n",
    "        count += mask.sum()\n",
    "        total += len(df)\n",
    "\n",
    "    return count / total\n",
    "\n",
    "\n",
    "def llm_validating_keywords(df):\n",
    "    \"\"\"LLM d'accord avec keywords quand keywords != BERT.\"\"\"\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    for cat in CATEGORIES:\n",
    "        kw = df[f\"keywords_{cat}\"]\n",
    "        bert = df[f\"bert_{cat}\"]\n",
    "        llm = df[f\"llm_{cat}\"]\n",
    "\n",
    "        mask = (kw == llm) & (kw != bert)\n",
    "        count += mask.sum()\n",
    "        total += len(df)\n",
    "\n",
    "    return count / total\n",
    "\n",
    "\n",
    "def llm_validating_bert(df):\n",
    "    \"\"\"LLM d'accord avec BERT quand keywords != BERT.\"\"\"\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    for cat in CATEGORIES:\n",
    "        kw = df[f\"keywords_{cat}\"]\n",
    "        bert = df[f\"bert_{cat}\"]\n",
    "        llm = df[f\"llm_{cat}\"]\n",
    "\n",
    "        mask = (bert == llm) & (kw != bert)\n",
    "        count += mask.sum()\n",
    "        total += len(df)\n",
    "\n",
    "    return count / total\n",
    "\n",
    "\n",
    "def clean_labels(series):\n",
    "    \"\"\"Nettoie et normalise les labels.\"\"\"\n",
    "    return (\n",
    "        series\n",
    "        .fillna(\"unknown\")\n",
    "        .replace(\"\", \"unknown\")\n",
    "        .replace(\"none\", \"unknown\")\n",
    "        .replace(\"None\", \"unknown\")\n",
    "        .astype(str)\n",
    "    )\n",
    "\n",
    "def confusion(df, model_col, cat):\n",
    "    \"\"\"Matrix de confusion model vs LLM (avec nettoyage robuste).\"\"\"\n",
    "    gold = clean_labels(df[f\"llm_{cat}\"])\n",
    "    pred = clean_labels(df[f\"{model_col}_{cat}\"])\n",
    "\n",
    "    # Filtrer les cas o√π au moins une valeur est 'unknown'\n",
    "    valid_mask = (gold != \"unknown\") & (pred != \"unknown\")\n",
    "    gold_clean = gold[valid_mask]\n",
    "    pred_clean = pred[valid_mask]\n",
    "\n",
    "    if len(gold_clean) == 0:\n",
    "        return \"‚ö†Ô∏è No valid data for confusion matrix\"\n",
    "\n",
    "    labels = sorted(set(gold_clean) | set(pred_clean))\n",
    "    return confusion_matrix(gold_clean, pred_clean, labels=labels), labels\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Calcul global des m√©triques\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "results = {\n",
    "    \"agreement_rate\": agreement_rate(df),\n",
    "    \"disputed_rate\": disputed_rate(df),\n",
    "    \"accuracy_agreed\": accuracy_agreed(df),\n",
    "    \"bert_added_value\": bert_added_value(df),\n",
    "    \"llm_validates_keywords\": llm_validating_keywords(df),\n",
    "    \"llm_validates_bert\": llm_validating_bert(df),\n",
    "    \"confusion_keywords_vs_llm\": {cat: confusion(df, \"keywords\", cat) for cat in CATEGORIES},\n",
    "    \"confusion_bert_vs_llm\": {cat: confusion(df, \"bert\", cat) for cat in CATEGORIES},\n",
    "}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Affichage synth√©tique\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n====== PIPELINE VALIDATION METRICS ======\\n\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
