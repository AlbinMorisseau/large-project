{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd9b8a0",
   "metadata": {},
   "source": [
    "# Evaluation classification methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fe941f",
   "metadata": {},
   "source": [
    "This notebook aims to evaluate on the same 200 labeeled reviews the 3 following methods of classification used ithin our pipelines :\n",
    "\n",
    "- Keywords extractions\n",
    "- BERT model finetunned\n",
    "- LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443a5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re \n",
    "import ollama\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score,accuracy_score\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff5dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filterign HTTP logging\n",
    "class HttpStatusFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        message = record.getMessage()\n",
    "        if 'HTTP/1.1 200' not in message:\n",
    "            record.levelname = \"WARNING\"\n",
    "            record.levelno = logging.WARNING\n",
    "        return 'HTTP/1.1 200' not in message\n",
    "    \n",
    "logging.getLogger(\"httpx\").addFilter(HttpStatusFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61551618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:53:11,164 - INFO - NUM_THREAD fixed to 10\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b18db2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:53:17,603 - INFO - Loading test data...\n"
     ]
    }
   ],
   "source": [
    "# Categories\n",
    "classes = [\"handicap\", \"pet\", \"child\"]\n",
    "\n",
    "# Test data loading\n",
    "logger.info(\"Loading test data...\")\n",
    "df = pd.read_csv(\"../../data/original/fine_tunning/data_test.csv\")\n",
    "y_true = df[classes].values.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a571fc82",
   "metadata": {},
   "source": [
    "### Evaluation of Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f354659a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9600\n",
      " Precision: 1.0000\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9298\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.8732\n",
      " Recall   : 1.0000\n",
      " F1-score : 0.9323\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9600\n",
      " Precision: 0.8769\n",
      " Recall   : 1.0000\n",
      " F1-score : 0.9344\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9101, Recall: 0.9556, F1: 0.9322\n",
      " Macro Precision: 0.9167, Recall: 0.9563, F1: 0.9322\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "df_truth = df \n",
    "df_kw = pd.read_csv(\"../../data/processed/data_categorized/key_words_data_test.csv\")\n",
    "\n",
    "# Combine keywords per review (each review now has a unique id)\n",
    "df_kw = (\n",
    "    df_kw.groupby(\"id\", as_index=False)\n",
    "         .agg({\n",
    "             \"review\": \"first\",\n",
    "             \"category\": lambda x: \" \".join(x.astype(str))\n",
    "         })\n",
    ")\n",
    "\n",
    "# Initialize predictions at 0\n",
    "df_pred = pd.DataFrame(0, index=df_truth.index, columns=classes)\n",
    "df_pred[\"id\"] = df_truth[\"id\"]\n",
    "\n",
    "# Prediction using keywords\n",
    "df_pred = df_pred.set_index(\"id\")\n",
    "\n",
    "for _, row in df_kw.iterrows():\n",
    "    review_id = row[\"id\"]\n",
    "    cat_list = str(row[\"category\"]).strip().lower().split()  # split in case multiple categories concatenated\n",
    "    for cat in cat_list:\n",
    "        if cat in classes and review_id in df_pred.index:\n",
    "            df_pred.at[review_id, cat] = 1\n",
    "\n",
    "df_pred = df_pred.reset_index()  # restore id column\n",
    "\n",
    "# Align truth and prediction\n",
    "df_truth = df_truth.sort_values(\"id\").reset_index(drop=True)\n",
    "df_pred  = df_pred.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "y_true = df_truth[classes].values\n",
    "y_pred = df_pred[classes].values\n",
    "\n",
    "\n",
    "# Compute metrics per class\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab70e73",
   "metadata": {},
   "source": [
    "### Evaluation of BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1921e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-02 11:55:14,304 - INFO - Loading tokenizer...\n",
      "2025-12-02 11:55:14,363 - INFO - Loading model...\n",
      "2025-12-02 11:55:16,530 - INFO - Predicting...\n",
      "2025-12-02 11:55:19,185 - INFO - Metrics multilabel\n",
      "2025-12-02 11:55:19,273 - INFO - Global metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      "Accuracy : 0.9300\n",
      "Precision: 0.9796\n",
      "Recall   : 0.7869\n",
      "F1-score : 0.8727\n",
      "\n",
      "Label: pet\n",
      "Accuracy : 0.9350\n",
      "Precision: 0.8769\n",
      "Recall   : 0.9194\n",
      "F1-score : 0.8976\n",
      "\n",
      "Label: child\n",
      "Accuracy : 0.9550\n",
      "Precision: 0.9138\n",
      "Recall   : 0.9298\n",
      "F1-score : 0.9217\n",
      "\n",
      "Micro Precision: 0.9186, Recall: 0.8778, F1: 0.8977\n",
      "Macro Precision: 0.9234, Recall: 0.8787, F1: 0.8974\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "BERT_PATH = \"../../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"../bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"../bert/best_weights_v3.pth\"\n",
    "MAX_SEQ_LEN = 256\n",
    "threshold = 0.97\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model definition\n",
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "\n",
    "logger.info(\"Loading model...\")\n",
    "model = BertMultiLabelClassifier(n_classes=len(classes))\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Encodign function\n",
    "def encode_batch(sentences):\n",
    "    encoded = tokenizer(\n",
    "        list(sentences),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Prediction\n",
    "logger.info(\"Predicting...\")\n",
    "input_ids, attention_mask = encode_batch(df[\"review\"])\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "\n",
    "y_pred_bin = (pred > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Metrics\n",
    "logger.info(\"Metrics multilabel\")\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred_bin[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall   : {rec:.4f}\")\n",
    "    print(f\"F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "rec_micro = recall_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "f1_micro = f1_score(y_true, y_pred_bin, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "rec_macro = recall_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(y_true, y_pred_bin, average=\"macro\", zero_division=0)\n",
    "\n",
    "logger.info(\"Global metrics\")\n",
    "print(f\"Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\"Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1e8df",
   "metadata": {},
   "source": [
    "### Evaluation of LLM Mistral Small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a24c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9550\n",
      " Precision: 0.9815\n",
      " Recall   : 0.8689\n",
      " F1-score : 0.9217\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9400\n",
      " Precision: 0.9032\n",
      " Recall   : 0.9032\n",
      " F1-score : 0.9032\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9300\n",
      " Precision: 0.8772\n",
      " Recall   : 0.8772\n",
      " F1-score : 0.8772\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9191, Recall: 0.8833, F1: 0.9008\n",
      " Macro Precision: 0.9206, Recall: 0.8831, F1: 0.9007\n"
     ]
    }
   ],
   "source": [
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification via Ollama\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\n",
    "             \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "             f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "             f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "             \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "         )},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=messages,options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "def classify_all_categories(review):\n",
    "    return [classify_review_ollama(review, category) for category in classes]\n",
    "\n",
    "# Prediction\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREAD) as executor:\n",
    "    y_pred = list(executor.map(classify_all_categories, df[\"review\"]))\n",
    "\n",
    "# Convertir en matrice numpy si besoin :\n",
    "import numpy as np\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef45f7ec",
   "metadata": {},
   "source": [
    "# Evaluation prompt few shot mistral small 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45a4123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: handicap\n",
      " Accuracy : 0.9800\n",
      " Precision: 0.9831\n",
      " Recall   : 0.9508\n",
      " F1-score : 0.9667\n",
      "\n",
      "Label: pet\n",
      " Accuracy : 0.9650\n",
      " Precision: 0.9365\n",
      " Recall   : 0.9516\n",
      " F1-score : 0.9440\n",
      "\n",
      "Label: child\n",
      " Accuracy : 0.9200\n",
      " Precision: 0.8475\n",
      " Recall   : 0.8772\n",
      " F1-score : 0.8621\n",
      "\n",
      "Global metrics:\n",
      " Micro Precision: 0.9227, Recall: 0.9278, F1: 0.9252\n",
      " Macro Precision: 0.9223, Recall: 0.9265, F1: 0.9242\n"
     ]
    }
   ],
   "source": [
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification with Ollama \"\"\"\n",
    "    \n",
    "    if category == \"child\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                  \"You are a strict family-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with children. Especially, you need to determine\"\n",
    "                 \" if these children have a high chance to be under 18 years old.\"\n",
    "                 \"Respond strictly with 'yes' if the review indicates people travelling with children, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"We traveled with our kids and loved the family-friendly pool.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The hotel was great, but we went alone as a couple.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I got there at 6:30, and a kid that apparently worked there (no id/uniform) was scrambling to set everything up\\\" -> no\\n\"\n",
    "                 \"Review: \\\"My Grand kids loved the pool\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I travelled to Dakota to see my son graduatation\\\" -> no\\n\"\n",
    "                 \"Review: \\\"This family owned business has a welcoming staff which made us feel right at home\\\" -> no\\n\"\n",
    "                 \"Review: \\\"If I had to ask one thing of Best Western, please replace the mattresses or box springs every time our kids moved at night\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"pet\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict pet-friendly-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with pets. \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates they travel with pets, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Thanks again Cat!\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I only booked this hotel because it was dog friendly\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I wanted to see if I could bring my service dog with me but they told me it was impossible at the front desk.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The bedsheets were smelling cat urine. Horrible !\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Perfect for travelers with cats or dogs.\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"handicap\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict business-travel-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) have any type of handicap or if the reviews contains a specific needs\"\n",
    "                 \"associated with a disability (transporations, amenities, etc.) \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates a handicaped traveler or a special need related to handicap travelling, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Plant to go to London in September Need information about Accessible Van in London airport\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The room was great, big enough to move around in my power chair in both the bedroom and bathroom\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I would like to sell my wheelchair.please contact me\\\" -> no\\n\"\n",
    "                 \"Review: \\\"It's new digital travel magazine targeted exclusively for travelers with disabilities.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Nice roll-in shower with a pull-down bench, but the amenities were again too high\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown category: {category}\")\n",
    "    \n",
    "    # Ici tu peux directement envoyer `messages` à Ollama\n",
    "    response = ollama.chat(model=model, messages=messages, options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "\n",
    "def classify_all_categories(review):\n",
    "    return [classify_review_ollama(review, category) for category in classes]\n",
    "\n",
    "# Prediction\n",
    "with ThreadPoolExecutor(max_workers=NUM_THREAD) as executor:\n",
    "    y_pred = list(executor.map(classify_all_categories, df[\"review\"]))\n",
    "\n",
    "# Convertir en matrice numpy si besoin :\n",
    "import numpy as np\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# Metrics\n",
    "for i, label in enumerate(classes):\n",
    "    acc = accuracy_score(y_true[:, i], y_pred[:, i])\n",
    "    prec = precision_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    rec = recall_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "    f1 = f1_score(y_true[:, i], y_pred[:, i], zero_division=0)\n",
    "\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\" Accuracy : {acc:.4f}\")\n",
    "    print(f\" Precision: {prec:.4f}\")\n",
    "    print(f\" Recall   : {rec:.4f}\")\n",
    "    print(f\" F1-score : {f1:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Global metrics\n",
    "prec_micro = precision_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "rec_micro  = recall_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "f1_micro   = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
    "\n",
    "prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Global metrics:\")\n",
    "print(f\" Micro Precision: {prec_micro:.4f}, Recall: {rec_micro:.4f}, F1: {f1_micro:.4f}\")\n",
    "print(f\" Macro Precision: {prec_macro:.4f}, Recall: {rec_macro:.4f}, F1: {f1_macro:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b224ee",
   "metadata": {},
   "source": [
    "## Evluation using Human validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validated_data_accessiblego_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_accessiblego_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_1_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_1_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_restaurant_reviews_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_restaurant_reviews_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_1_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_1_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_2_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_2_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_child_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_twitter_child_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_accessiblego_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_accessiblego_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_restaurant_reviews_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_2_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_2_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_pet_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_twitter_pet_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_accessiblego_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_accessiblego_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_activities_reviews_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_1_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_1_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_airline_reviews_2_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_booking_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_hotel_reviews_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_european_restaurant_reviews_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_1_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_2_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_hotel_reviews_3_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_restaurant_reviews_2_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_handicap_good.csv -> ['original_index', 'review', 'id']\n",
      "\n",
      "validated_data_tripadvisor_hotel_reviews_handicap_rejected.csv -> ['original_index', 'review', 'id']\n",
      "NB REVIEWS : 3300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "FINAL_PATH = Path(\"../../data/processed/final\")\n",
    "VALIDATED_PATH = Path(\"../../data/processed/data_validated\")\n",
    "CATEGORIES = [\"child\", \"pet\", \"handicap\"]\n",
    "\n",
    "def load_human_labels():\n",
    "    rows = []\n",
    "    for cat in CATEGORIES:\n",
    "        cat_dir = FINAL_PATH / cat\n",
    "        for file in cat_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            if \"review\" not in df.columns.tolist():\n",
    "                print(file.name)\n",
    "            truth = 1 if \"good\" in file.stem else 0\n",
    "            df[\"human_truth\"] = truth\n",
    "            df[\"category\"] = cat\n",
    "            rows.append(df[[\"id\",\"review\",\"human_truth\",\"category\"]])\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "def load_validated_data():\n",
    "    dfs = []\n",
    "    for cat in CATEGORIES:\n",
    "        for file in (VALIDATED_PATH / cat).glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"category\"] = cat\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "human_df = load_human_labels()\n",
    "validated_df = load_validated_data()\n",
    "\n",
    "merged = human_df.merge(validated_df, on=[\"id\", \"category\"], how=\"left\")\n",
    "merged_unique = merged.drop_duplicates(subset=[\"id\", \"review_x\",\"category\"])\n",
    "merged_unique = merged_unique.rename(columns={\"review_x\": \"review\"})\n",
    "merged_unique = merged_unique.drop(columns=[\"review_y\"])\n",
    "\n",
    "final_cols = [\n",
    "    \"id\", \"review\", \"category\", \"human_truth\", \"validation_status\",\n",
    "    \"llm_child\", \"llm_pet\", \"llm_handicap\",\n",
    "    \"bert_child\", \"bert_pet\", \"bert_handicap\",\n",
    "    \"kw_child\", \"kw_pet\", \"kw_handicap\"\n",
    "]\n",
    "\n",
    "final_dataset = merged_unique[final_cols]\n",
    "final_dataset[\"review\"] = final_dataset[\"review\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "final_dataset[\"review\"] = final_dataset[\"review\"].str.replace(\"\\r\", \" \", regex=False)\n",
    "\n",
    "print(f\"NB REVIEWS : {len(final_dataset)}\")\n",
    "\n",
    "#final_dataset.to_csv(\"merged_final_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4497e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(df: pd.DataFrame):\n",
    "    \n",
    "    stats = {}\n",
    "\n",
    "    # 1️⃣ Nombre total de reviews\n",
    "    stats[\"total_reviews\"] = len(df)\n",
    "\n",
    "    # 2️⃣ Nombre de reviews par catégorie + proportion\n",
    "    cat_counts = df[\"category\"].value_counts()\n",
    "    cat_props = df[\"category\"].value_counts(normalize=True)\n",
    "    stats[\"reviews_per_category\"] = pd.DataFrame({\n",
    "        \"count\": cat_counts,\n",
    "        \"proportion\": cat_props\n",
    "    })\n",
    "\n",
    "    # 3️⃣ Nombre de reviews par validation_status + proportion\n",
    "    val_counts = df[\"validation_status\"].value_counts()\n",
    "    val_props = df[\"validation_status\"].value_counts(normalize=True)\n",
    "    stats[\"reviews_per_validation_status\"] = pd.DataFrame({\n",
    "        \"count\": val_counts,\n",
    "        \"proportion\": val_props\n",
    "    })\n",
    "\n",
    "    # 4️⃣ Nombre de reviews par validation_status pour chaque catégorie + proportion par catégorie\n",
    "    stats[\"reviews_per_status_per_category\"] = (\n",
    "        df.groupby([\"category\", \"validation_status\"])\n",
    "          .size()\n",
    "          .to_frame(\"count\")\n",
    "          .groupby(level=0)\n",
    "          .apply(lambda x: x.assign(proportion=x[\"count\"]/x[\"count\"].sum()))\n",
    "    )\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0bad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_statistics(stats: dict):\n",
    "    print(\"=== STATISTICS ===\\n\")\n",
    "    \n",
    "    # Nombre total de reviews\n",
    "    print(f\"Total Reviews: {stats['total_reviews']}\\n\")\n",
    "    \n",
    "    # Reviews par catégorie\n",
    "    print(\"Reviews per Category:\")\n",
    "    display(stats['reviews_per_category'].sort_index())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Reviews par validation_status\n",
    "    print(\"Reviews per Validation Status:\")\n",
    "    display(stats['reviews_per_validation_status'].sort_index())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Reviews par validation_status par catégorie\n",
    "    print(\"Reviews per Validation Status per Category:\")\n",
    "    display(stats['reviews_per_status_per_category'])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e08d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STATISTICS ===\n",
      "\n",
      "Total Reviews: 3300\n",
      "\n",
      "Reviews per Category:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>child</th>\n",
       "      <td>1575</td>\n",
       "      <td>0.477273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>handicap</th>\n",
       "      <td>877</td>\n",
       "      <td>0.265758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pet</th>\n",
       "      <td>848</td>\n",
       "      <td>0.256970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count  proportion\n",
       "category                   \n",
       "child      1575    0.477273\n",
       "handicap    877    0.265758\n",
       "pet         848    0.256970"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reviews per Validation Status:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>agreed</th>\n",
       "      <td>2437</td>\n",
       "      <td>0.738485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disputed</th>\n",
       "      <td>92</td>\n",
       "      <td>0.027879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_validated</th>\n",
       "      <td>771</td>\n",
       "      <td>0.233636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count  proportion\n",
       "validation_status                   \n",
       "agreed              2437    0.738485\n",
       "disputed              92    0.027879\n",
       "llm_validated        771    0.233636"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Reviews per Validation Status per Category:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th>category</th>\n",
       "      <th>validation_status</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">child</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">child</th>\n",
       "      <th>agreed</th>\n",
       "      <td>1196</td>\n",
       "      <td>0.759365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disputed</th>\n",
       "      <td>23</td>\n",
       "      <td>0.014603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_validated</th>\n",
       "      <td>356</td>\n",
       "      <td>0.226032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">handicap</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">handicap</th>\n",
       "      <th>agreed</th>\n",
       "      <td>573</td>\n",
       "      <td>0.653364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disputed</th>\n",
       "      <td>34</td>\n",
       "      <td>0.038769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_validated</th>\n",
       "      <td>270</td>\n",
       "      <td>0.307868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">pet</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">pet</th>\n",
       "      <th>agreed</th>\n",
       "      <td>668</td>\n",
       "      <td>0.787736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disputed</th>\n",
       "      <td>35</td>\n",
       "      <td>0.041274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm_validated</th>\n",
       "      <td>145</td>\n",
       "      <td>0.170991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     count  proportion\n",
       "category category validation_status                   \n",
       "child    child    agreed              1196    0.759365\n",
       "                  disputed              23    0.014603\n",
       "                  llm_validated        356    0.226032\n",
       "handicap handicap agreed               573    0.653364\n",
       "                  disputed              34    0.038769\n",
       "                  llm_validated        270    0.307868\n",
       "pet      pet      agreed               668    0.787736\n",
       "                  disputed              35    0.041274\n",
       "                  llm_validated        145    0.170991"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "statistics = compute_statistics(final_dataset)\n",
    "display_statistics(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pipeline_metrics(df: pd.DataFrame):\n",
    "    metrics = {}\n",
    "\n",
    "    total_reviews = len(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1️⃣ Premier layer : Keywords\n",
    "    # -------------------------\n",
    "    kw_correct = (\n",
    "        ((df[\"kw_child\"] == df[\"llm_child\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth\"] == 1))\n",
    "    )\n",
    "    metrics[\"keywords_precision\"] = kw_correct.sum() / total_reviews\n",
    "    metrics[\"keywords_coverage\"] = total_reviews / total_reviews\n",
    "\n",
    "    # -------------------------\n",
    "    # 2️⃣ Second layer : BERT validation\n",
    "    # -------------------------\n",
    "    bert_correct = (\n",
    "        ((df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth\"] == 1))\n",
    "    )\n",
    "    bert_correct_kw_incorrect = (\n",
    "        ((df[\"kw_child\"] != df[\"llm_child\"]) & (df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] != df[\"llm_pet\"]) & (df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] != df[\"llm_handicap\"]) & (df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth\"] == 1))\n",
    "    )\n",
    "    metrics[\"bert_layer_precision\"] = bert_correct.sum() / total_reviews\n",
    "    metrics[\"bert_layer_gain\"] = bert_correct_kw_incorrect.sum() / total_reviews\n",
    "    metrics[\"bert_layer_coverage\"] = total_reviews / total_reviews\n",
    "\n",
    "    # -------------------------\n",
    "    # 3️⃣ Third layer : LLM validation\n",
    "    # -------------------------\n",
    "    correct = (df[\"human_truth\"] == 1)\n",
    "    llm_correct = (df[\"validation_status\"].isin([\"llm_validated\", \"disputed\"]) & (df[\"human_truth\"] == 1))\n",
    "    metrics[\"llm_layer_precision\"] = correct.sum() / total_reviews\n",
    "    metrics[\"llm_layer_coverage\"] = len(df[df[\"validation_status\"].isin([\"llm_validated\", \"disputed\"])]) / total_reviews\n",
    "    metrics['llm_layer_gain'] = llm_correct.sum() / total_reviews\n",
    "\n",
    "    # LLM préférences quand ce n'est pas \"agreed\"\n",
    "    mask = df[\"validation_status\"] != \"agreed\"\n",
    "    llm_pref = {}\n",
    "\n",
    "    llm_pref[\"agree_kw_percentage\"] = (\n",
    "        ((df[\"kw_child\"] == df[\"llm_child\"]) & mask) &\n",
    "        ((df[\"kw_pet\"] == df[\"llm_pet\"]) & mask) &\n",
    "        ((df[\"kw_handicap\"] == df[\"llm_handicap\"]) & mask)\n",
    "    ).sum() / mask.sum()\n",
    "    \n",
    "    llm_pref[\"agree_bert_percentage\"] = (\n",
    "        ((df[\"bert_child\"] == df[\"llm_child\"]) & mask) &\n",
    "        ((df[\"bert_pet\"] == df[\"llm_pet\"]) & mask) &\n",
    "        ((df[\"bert_handicap\"] == df[\"llm_handicap\"]) & mask)\n",
    "    ).sum() / mask.sum()\n",
    "    \n",
    "    llm_pref[\"disagree_both_percentage\"] = (\n",
    "        mask.sum() - llm_pref[\"agree_kw_percentage\"] * mask.sum() - llm_pref[\"agree_bert_percentage\"] * mask.sum()\n",
    "    ) / mask.sum()\n",
    "\n",
    "    metrics[\"llm_preferences_counts\"] = llm_pref\n",
    "\n",
    "    # -------------------------\n",
    "    # 4️⃣ Gains pipeline\n",
    "    # -------------------------\n",
    "    metrics[\"pipeline_added_value\"] = (bert_correct_kw_incorrect.sum() + llm_correct.sum()) / total_reviews\n",
    "    metrics[\"final_pipeline_precision\"] = (df[\"human_truth\"] == 1).sum() / total_reviews\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def display_pipeline_metrics(metrics: dict):\n",
    "    print(\"=== PIPELINE METRICS ===\\n\")\n",
    "    print(f\"Keywords layer: Layer Precision = {metrics['keywords_precision']:.3%}, Coverage = {metrics['keywords_coverage']:.3%}\")\n",
    "    print(f\"BERT layer: Layer Precision = {metrics['bert_layer_precision']:.3%}, Coverage = {metrics['bert_layer_coverage']:.3%}, Layer Gain = {metrics['bert_layer_gain']:.3%}\")\n",
    "    print(f\"LLM layer: Layer Precision = {metrics['llm_layer_precision']:.3%}, Coverage = {metrics['llm_layer_coverage']:.3%}, Layer Gain = {metrics['llm_layer_gain']:.3%}\")\n",
    "\n",
    "    print(\"\\nLLM preferences when BERT and Keywords disagree:\")\n",
    "    for k, v in metrics['llm_preferences_counts'].items():\n",
    "        print(f\"  - {k}: {v:.3%}\" if isinstance(v, float) else f\"  - {k}: {v}\")\n",
    "    \n",
    "    print(f\"\\nFinal pipeline: Overall Precision = {metrics['final_pipeline_precision']:.3%}\")\n",
    "    print(f\"Final Gain of the pipeline: Gain = {metrics['pipeline_added_value']:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dad10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PIPELINE METRICS ===\n",
      "\n",
      "Keywords layer: Layer Precision = 63.219%, Coverage = 100.000%\n",
      "BERT layer: Layer Precision = 51.404%, Coverage = 100.000%, Layer Gain = 0.000%\n",
      "LLM layer: Layer Precision = 63.904%, Coverage = 23.904%, Layer Gain = 12.500%\n",
      "\n",
      "LLM preferences when BERT and Keywords disagree:\n",
      "  - agree_kw_percentage: 90.544%\n",
      "  - agree_bert_percentage: 6.734%\n",
      "  - disagree_both_percentage: 2.722%\n",
      "\n",
      "Final pipeline: Overall Precision = 63.904%\n",
      "Final Gain of the pipeline: Gain = 12.500%\n"
     ]
    }
   ],
   "source": [
    "display_pipeline_metrics(compute_pipeline_metrics(final_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bce277",
   "metadata": {},
   "source": [
    "## Evaluation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a177eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB REVIEWS : 4217\n",
      "\n",
      "=== Metrics pour catégorie : child ===\n",
      "KW: TP=1261 FP=1234 FN=119 TN=1603 | Acc=67.916% Prec=50.541% Recall=91.377% F1=65.084%\n",
      "BERT: TP=1033 FP=744 FN=347 TN=2093 | Acc=74.129% Prec=58.132% Recall=74.855% F1=65.442%\n",
      "LLM: TP=1251 FP=1167 FN=129 TN=1670 | Acc=69.267% Prec=51.737% Recall=90.652% F1=65.877%\n",
      "\n",
      "=== Metrics pour catégorie : pet ===\n",
      "KW: TP=560 FP=458 FN=106 TN=3093 | Acc=86.626% Prec=55.010% Recall=84.084% F1=66.508%\n",
      "BERT: TP=461 FP=353 FN=205 TN=3198 | Acc=86.768% Prec=56.634% Recall=69.219% F1=62.297%\n",
      "LLM: TP=560 FP=432 FN=106 TN=3119 | Acc=87.242% Prec=56.452% Recall=84.084% F1=67.551%\n",
      "\n",
      "=== Metrics pour catégorie : handicap ===\n",
      "KW: TP=742 FP=554 FN=192 TN=2729 | Acc=82.310% Prec=57.253% Recall=79.443% F1=66.547%\n",
      "BERT: TP=543 FP=385 FN=391 TN=2898 | Acc=81.598% Prec=58.513% Recall=58.137% F1=58.324%\n",
      "LLM: TP=745 FP=550 FN=189 TN=2733 | Acc=82.476% Prec=57.529% Recall=79.764% F1=66.846%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Paths et catégories\n",
    "# -------------------------\n",
    "FINAL_PATH = Path(\"../../data/processed/final\")\n",
    "VALIDATED_PATH = Path(\"../../data/processed/data_validated\")\n",
    "CATEGORIES = [\"child\", \"pet\", \"handicap\"]\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Charger les labels humains\n",
    "# -------------------------\n",
    "def load_human_labels():\n",
    "    rows = []\n",
    "    for cat in CATEGORIES:\n",
    "        cat_dir = FINAL_PATH / cat\n",
    "        for file in cat_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            if \"review\" not in df.columns:\n",
    "                print(f\"Attention, fichier sans review : {file.name}\")\n",
    "\n",
    "            # human_truth = 1 si \"good\" dans le nom du fichier\n",
    "            truth = 1 if \"good\" in file.stem else 0\n",
    "\n",
    "            # Colonnes human_truth par catégorie\n",
    "            for c in CATEGORIES:\n",
    "                df[f\"human_truth_{c}\"] = truth if c == cat else 0\n",
    "\n",
    "            df[\"category\"] = cat\n",
    "\n",
    "            cols_to_keep = [\"id\", \"review\", \"category\"] + [f\"human_truth_{c}\" for c in CATEGORIES]\n",
    "            rows.append(df[cols_to_keep])\n",
    "\n",
    "    human_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Fusionner les lignes par id pour avoir une seule ligne par review\n",
    "    human_df = human_df.groupby(\"id\").agg(\n",
    "        review=(\"review\", \"first\"),\n",
    "        human_truth_child=(\"human_truth_child\", \"max\"),\n",
    "        human_truth_pet=(\"human_truth_pet\", \"max\"),\n",
    "        human_truth_handicap=(\"human_truth_handicap\", \"max\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return human_df\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Charger les données validées\n",
    "# -------------------------\n",
    "def load_validated_data():\n",
    "    dfs = []\n",
    "    for cat in CATEGORIES:\n",
    "        for file in (VALIDATED_PATH / cat).glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"category\"] = cat\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Charger les données\n",
    "# -------------------------\n",
    "human_df = load_human_labels()\n",
    "validated_df = load_validated_data()\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Merge avec validated_df\n",
    "# -------------------------\n",
    "merged = human_df.merge(validated_df, on=\"id\", how=\"left\")  # merge sur id seulement\n",
    "# Prendre la colonne review de human_df\n",
    "if \"review_x\" in merged.columns:\n",
    "    merged[\"review\"] = merged[\"review_x\"]\n",
    "    if \"review_y\" in merged.columns:\n",
    "        merged = merged.drop(columns=[\"review_y\"])\n",
    "    merged = merged.drop(columns=[\"review_x\"])\n",
    "else:\n",
    "    raise ValueError(\"Aucune colonne 'review_x' trouvée dans le merge\")\n",
    "\n",
    "# Nettoyage du texte\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\r\", \" \", regex=False)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Préparer dataset final\n",
    "# -------------------------\n",
    "final_cols = [\n",
    "    \"id\", \"review\",\n",
    "    \"human_truth_child\", \"human_truth_pet\", \"human_truth_handicap\",\n",
    "    \"validation_status\",\n",
    "    \"llm_child\", \"llm_pet\", \"llm_handicap\",\n",
    "    \"bert_child\", \"bert_pet\", \"bert_handicap\",\n",
    "    \"kw_child\", \"kw_pet\", \"kw_handicap\"\n",
    "]\n",
    "\n",
    "final_dataset = merged[final_cols]\n",
    "\n",
    "print(f\"NB REVIEWS : {len(final_dataset)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Fonctions métriques TP/FP/FN/TN\n",
    "# -------------------------\n",
    "def compute_confusion(df, pred_cols, truth_col):\n",
    "    y_true = df[truth_col].astype(int)\n",
    "    y_pred = df[pred_cols].astype(int).any(axis=1).astype(int)\n",
    "    \n",
    "    TP = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "    FP = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    FN = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "    TN = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "    total = len(df)\n",
    "    \n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    accuracy = (TP + TN) / total\n",
    "    f1 = (2 * precision * recall / (precision + recall)) if precision + recall > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "        \"accuracy\": accuracy, \"precision\": precision,\n",
    "        \"recall\": recall, \"f1\": f1,\n",
    "        \"total\": total\n",
    "    }\n",
    "\n",
    "def compute_pipeline_metrics(df):\n",
    "    results = {}\n",
    "    for cat in CATEGORIES:\n",
    "        results[cat] = {}\n",
    "        for layer in [\"kw\", \"bert\", \"llm\"]:\n",
    "            pred_cols = [f\"{layer}_{cat}\"]\n",
    "            truth_col = f\"human_truth_{cat}\"\n",
    "            results[cat][layer] = compute_confusion(df, pred_cols, truth_col)\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ Calculer et afficher métriques\n",
    "# -------------------------\n",
    "metrics = compute_pipeline_metrics(final_dataset)\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    print(f\"\\n=== Metrics pour catégorie : {cat} ===\")\n",
    "    for layer in [\"kw\", \"bert\", \"llm\"]:\n",
    "        m = metrics[cat][layer]\n",
    "        print(f\"{layer.upper()}: TP={m['TP']} FP={m['FP']} FN={m['FN']} TN={m['TN']} | \"\n",
    "              f\"Acc={m['accuracy']:.3%} Prec={m['precision']:.3%} Recall={m['recall']:.3%} F1={m['f1']:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3e699de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pipeline_metrics_v2(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calcul des métriques du pipeline pour chaque layer : keywords, BERT, LLM\n",
    "    en prenant en compte human_truth_* par catégorie.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    total_reviews = len(df)\n",
    "\n",
    "    # -------------------------\n",
    "    # 1️⃣ Layer Keywords\n",
    "    # -------------------------\n",
    "    kw_correct = (\n",
    "        ((df[\"kw_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "    metrics[\"keywords_precision\"] = kw_correct.sum() / total_reviews\n",
    "    metrics[\"keywords_coverage\"] = total_reviews / total_reviews  # toutes les reviews passent par ce layer\n",
    "    metrics[\"keywords_gain\"] = kw_correct.sum() / total_reviews  # pseudo gain = nombre correct / total\n",
    "\n",
    "    # -------------------------\n",
    "    # 2️⃣ Layer BERT\n",
    "    # -------------------------\n",
    "    bert_correct = (\n",
    "        ((df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "\n",
    "    bert_correct_kw_incorrect = (\n",
    "        ((df[\"kw_child\"] != df[\"llm_child\"]) & (df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] != df[\"llm_pet\"]) & (df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] != df[\"llm_handicap\"]) & (df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "\n",
    "    metrics[\"bert_precision\"] = bert_correct.sum() / total_reviews\n",
    "    metrics[\"bert_coverage\"] = total_reviews / total_reviews\n",
    "    metrics[\"bert_gain\"] = bert_correct_kw_incorrect.sum() / total_reviews\n",
    "\n",
    "    # -------------------------\n",
    "    # 3️⃣ Layer LLM\n",
    "    # -------------------------\n",
    "    llm_mask = df[\"validation_status\"].isin([\"llm_validated\", \"disputed\"])\n",
    "    llm_correct = (\n",
    "        ((df[\"human_truth_child\"] == 1) & llm_mask) &\n",
    "        ((df[\"human_truth_pet\"] == 1) & llm_mask) &\n",
    "        ((df[\"human_truth_handicap\"] == 1) & llm_mask)\n",
    "    )\n",
    "\n",
    "    metrics[\"llm_precision\"] = llm_correct.sum() / total_reviews\n",
    "    metrics[\"llm_coverage\"] = llm_mask.sum() / total_reviews\n",
    "    metrics[\"llm_gain\"] = llm_correct.sum() / total_reviews\n",
    "\n",
    "    # -------------------------\n",
    "    # 4️⃣ Gains pipeline global\n",
    "    # -------------------------\n",
    "    metrics[\"pipeline_added_value\"] = (bert_correct_kw_incorrect.sum() + llm_correct.sum()) / total_reviews\n",
    "    metrics[\"final_precision\"] = (\n",
    "        ((df[\"human_truth_child\"] == 1) | (df[\"human_truth_pet\"] == 1) | (df[\"human_truth_handicap\"] == 1)).sum()\n",
    "        / total_reviews\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def display_pipeline_metrics_v2(metrics: dict):\n",
    "    print(\"=== PIPELINE METRICS V2 ===\\n\")\n",
    "    print(f\"Keywords layer: Precision = {metrics['keywords_precision']:.3%}, Coverage = {metrics['keywords_coverage']:.3%}, Gain = {metrics['keywords_gain']:.3%}\")\n",
    "    print(f\"BERT layer: Precision = {metrics['bert_precision']:.3%}, Coverage = {metrics['bert_coverage']:.3%}, Gain = {metrics['bert_gain']:.3%}\")\n",
    "    print(f\"LLM layer: Precision = {metrics['llm_precision']:.3%}, Coverage = {metrics['llm_coverage']:.3%}, Gain = {metrics['llm_gain']:.3%}\")\n",
    "    print(f\"\\nPipeline added value (gain total) = {metrics['pipeline_added_value']:.3%}\")\n",
    "    print(f\"Final pipeline pseudo-precision = {metrics['final_precision']:.3%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b657564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PIPELINE METRICS V2 ===\n",
      "\n",
      "Keywords layer: Precision = 0.166%, Coverage = 100.000%, Gain = 0.166%\n",
      "BERT layer: Precision = 0.119%, Coverage = 100.000%, Gain = 0.000%\n",
      "LLM layer: Precision = 0.047%, Coverage = 28.338%, Gain = 0.047%\n",
      "\n",
      "Pipeline added value (gain total) = 0.047%\n",
      "Final pipeline pseudo-precision = 65.900%\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_pipeline_metrics_v2(final_dataset)\n",
    "display_pipeline_metrics_v2(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f0074bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB REVIEWS : 4217\n",
      "=== PIPELINE METRICS ===\n",
      "\n",
      "Layer: KEYWORDS\n",
      "  child: Precision=50.54%, Coverage=100.00%, Gain=0.00% | TP=1261 FP=1234 FN=119 TN=1603\n",
      "  pet: Precision=55.01%, Coverage=100.00%, Gain=0.00% | TP=560 FP=458 FN=106 TN=3093\n",
      "  handicap: Precision=57.25%, Coverage=100.00%, Gain=0.00% | TP=742 FP=554 FN=192 TN=2729\n",
      "\n",
      "Layer: BERT\n",
      "  child: Precision=58.13%, Coverage=100.00%, Gain=0.02% | TP=1033 FP=744 FN=347 TN=2093\n",
      "  pet: Precision=56.63%, Coverage=100.00%, Gain=0.00% | TP=461 FP=353 FN=205 TN=3198\n",
      "  handicap: Precision=58.51%, Coverage=100.00%, Gain=0.14% | TP=543 FP=385 FN=391 TN=2898\n",
      "\n",
      "Layer: LLM\n",
      "  child: Precision=33.03%, Coverage=100.00%, Gain=18.33% | TP=257 FP=521 FN=33 TN=384\n",
      "  pet: Precision=53.79%, Coverage=100.00%, Gain=8.28% | TP=149 FP=128 FN=38 TN=880\n",
      "  handicap: Precision=53.60%, Coverage=100.00%, Gain=16.90% | TP=298 FP=258 FN=44 TN=595\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Paths et catégories\n",
    "# -------------------------\n",
    "FINAL_PATH = Path(\"../../data/processed/final\")\n",
    "VALIDATED_PATH = Path(\"../../data/processed/data_validated\")\n",
    "CATEGORIES = [\"child\", \"pet\", \"handicap\"]\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Charger les labels humains\n",
    "# -------------------------\n",
    "def load_human_labels():\n",
    "    rows = []\n",
    "    for cat in CATEGORIES:\n",
    "        cat_dir = FINAL_PATH / cat\n",
    "        for file in cat_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            if \"review\" not in df.columns:\n",
    "                print(f\"Attention, fichier sans review : {file.name}\")\n",
    "                continue\n",
    "\n",
    "            truth = 1 if \"good\" in file.stem else 0\n",
    "\n",
    "            # human_truth par catégorie\n",
    "            for c in CATEGORIES:\n",
    "                df[f\"human_truth_{c}\"] = truth if c == cat else 0\n",
    "\n",
    "            df[\"category\"] = cat\n",
    "            cols_to_keep = [\"id\", \"review\", \"category\"] + [f\"human_truth_{c}\" for c in CATEGORIES]\n",
    "            rows.append(df[cols_to_keep])\n",
    "\n",
    "    human_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Fusion par id pour avoir une seule ligne par review\n",
    "    human_df = human_df.groupby(\"id\").agg(\n",
    "        review=(\"review\", \"first\"),\n",
    "        human_truth_child=(\"human_truth_child\", \"max\"),\n",
    "        human_truth_pet=(\"human_truth_pet\", \"max\"),\n",
    "        human_truth_handicap=(\"human_truth_handicap\", \"max\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return human_df\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Charger les données validées\n",
    "# -------------------------\n",
    "def load_validated_data():\n",
    "    dfs = []\n",
    "    for cat in CATEGORIES:\n",
    "        for file in (VALIDATED_PATH / cat).glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"category\"] = cat\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Charger les datasets\n",
    "# -------------------------\n",
    "human_df = load_human_labels()\n",
    "validated_df = load_validated_data()\n",
    "\n",
    "# Merge sur id\n",
    "merged = human_df.merge(validated_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# Gestion des colonnes review\n",
    "if \"review_x\" in merged.columns:\n",
    "    merged[\"review\"] = merged[\"review_x\"]\n",
    "    merged = merged.drop(columns=[\"review_x\"], errors=\"ignore\")\n",
    "    if \"review_y\" in merged.columns:\n",
    "        merged = merged.drop(columns=[\"review_y\"], errors=\"ignore\")\n",
    "elif \"review\" not in merged.columns:\n",
    "    raise ValueError(\"La colonne 'review' est absente après merge\")\n",
    "\n",
    "# Nettoyage des retours à la ligne\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\r\", \" \", regex=False)\n",
    "\n",
    "# Colonnes finales\n",
    "final_cols = [\n",
    "    \"id\", \"review\",\n",
    "    \"human_truth_child\", \"human_truth_pet\", \"human_truth_handicap\",\n",
    "    \"validation_status\",\n",
    "    \"llm_child\", \"llm_pet\", \"llm_handicap\",\n",
    "    \"bert_child\", \"bert_pet\", \"bert_handicap\",\n",
    "    \"kw_child\", \"kw_pet\", \"kw_handicap\"\n",
    "]\n",
    "\n",
    "final_dataset = merged[final_cols]\n",
    "\n",
    "print(f\"NB REVIEWS : {len(final_dataset)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Fonctions métriques par catégorie\n",
    "# -------------------------\n",
    "def compute_layer_metrics(df, layer, previous_layer=None):\n",
    "    \"\"\"\n",
    "    Calcul Precision, Coverage, Gain par catégorie pour un layer.\n",
    "    previous_layer: dict avec booléens TP/FP du layer précédent pour calcul du gain\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total_reviews = len(df)\n",
    "\n",
    "    for cat in CATEGORIES:\n",
    "        pred = df[f\"{layer}_{cat}\"]\n",
    "        truth = df[f\"human_truth_{cat}\"]\n",
    "\n",
    "        TP = ((pred == 1) & (truth == 1)).sum()\n",
    "        FP = ((pred == 1) & (truth == 0)).sum()\n",
    "        FN = ((pred == 0) & (truth == 1)).sum()\n",
    "        TN = ((pred == 0) & (truth == 0)).sum()\n",
    "\n",
    "        precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "        coverage = total_reviews / total_reviews  # toutes les reviews observées\n",
    "        if previous_layer is None:\n",
    "            gain = 0  # pas de gain pour le premier layer\n",
    "        else:\n",
    "            # gain = corrections apportées par ce layer par rapport au précédent\n",
    "            prev_pred = previous_layer[cat][\"pred\"]\n",
    "            gain = ((prev_pred == 0) & (pred == 1) & (truth == 1)).sum() / total_reviews\n",
    "\n",
    "        results[cat] = {\n",
    "            \"TP\": TP, \"FP\": FP, \"FN\": FN, \"TN\": TN,\n",
    "            \"precision\": precision,\n",
    "            \"coverage\": coverage,\n",
    "            \"gain\": gain,\n",
    "            \"pred\": pred  # pour calcul du gain au layer suivant\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Calcul pipeline metrics\n",
    "# -------------------------\n",
    "def compute_pipeline_metrics(df):\n",
    "    metrics = {}\n",
    "\n",
    "    # Keywords\n",
    "    metrics[\"keywords\"] = compute_layer_metrics(df, \"kw\")\n",
    "    # BERT\n",
    "    metrics[\"bert\"] = compute_layer_metrics(df, \"bert\", previous_layer=metrics[\"keywords\"])\n",
    "    # LLM (on ne prend que les reviews validées ou disputed)\n",
    "    df_llm = df[df[\"validation_status\"].isin([\"llm_validated\", \"disputed\"])]\n",
    "    metrics[\"llm\"] = compute_layer_metrics(df_llm, \"llm\", previous_layer=metrics[\"bert\"])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Affichage\n",
    "# -------------------------\n",
    "def display_pipeline_metrics(metrics):\n",
    "    print(\"=== PIPELINE METRICS ===\")\n",
    "    for layer in [\"keywords\", \"bert\", \"llm\"]:\n",
    "        print(f\"\\nLayer: {layer.upper()}\")\n",
    "        for cat in CATEGORIES:\n",
    "            m = metrics[layer][cat]\n",
    "            print(f\"  {cat}: Precision={m['precision']:.2%}, Coverage={m['coverage']:.2%}, Gain={m['gain']:.2%} | TP={m['TP']} FP={m['FP']} FN={m['FN']} TN={m['TN']}\")\n",
    "\n",
    "# -------------------------\n",
    "# 7️⃣ Exécution\n",
    "# -------------------------\n",
    "metrics = compute_pipeline_metrics(final_dataset)\n",
    "display_pipeline_metrics(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "393ddb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB REVIEWS : 4217\n",
      "=== PIPELINE METRICS ===\n",
      "\n",
      "Keywords layer: Precision = 0.166%, Coverage = 100.000%, Gain = 0.000%\n",
      "BERT layer: Precision = 0.119%, Coverage = 100.000%, Gain = 0.000%\n",
      "LLM layer: Precision = 54.393%, Coverage = 28.338%, Gain = 15.414%\n",
      "\n",
      "LLM preferences when BERT and Keywords disagree:\n",
      "  - agree_kw_percentage: 89.456%\n",
      "  - agree_bert_percentage: 7.029%\n",
      "  - disagree_both_percentage: 3.515%\n",
      "\n",
      "Final pipeline pseudo-precision = 65.900%\n",
      "Total pipeline added value (gain) = 15.414%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Paths et catégories\n",
    "# -------------------------\n",
    "FINAL_PATH = Path(\"../../data/processed/final\")\n",
    "VALIDATED_PATH = Path(\"../../data/processed/data_validated\")\n",
    "CATEGORIES = [\"child\", \"pet\", \"handicap\"]\n",
    "\n",
    "# -------------------------\n",
    "# 1️⃣ Charger les labels humains\n",
    "# -------------------------\n",
    "def load_human_labels():\n",
    "    rows = []\n",
    "    for cat in CATEGORIES:\n",
    "        cat_dir = FINAL_PATH / cat\n",
    "        for file in cat_dir.glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "\n",
    "            if \"review\" not in df.columns:\n",
    "                print(f\"Attention, fichier sans review : {file.name}\")\n",
    "                continue\n",
    "\n",
    "            truth = 1 if \"good\" in file.stem else 0\n",
    "\n",
    "            # human_truth par catégorie\n",
    "            for c in CATEGORIES:\n",
    "                df[f\"human_truth_{c}\"] = truth if c == cat else 0\n",
    "\n",
    "            df[\"category\"] = cat\n",
    "            cols_to_keep = [\"id\", \"review\", \"category\"] + [f\"human_truth_{c}\" for c in CATEGORIES]\n",
    "            rows.append(df[cols_to_keep])\n",
    "\n",
    "    human_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # Fusion par id pour avoir une seule ligne par review\n",
    "    human_df = human_df.groupby(\"id\").agg(\n",
    "        review=(\"review\", \"first\"),\n",
    "        human_truth_child=(\"human_truth_child\", \"max\"),\n",
    "        human_truth_pet=(\"human_truth_pet\", \"max\"),\n",
    "        human_truth_handicap=(\"human_truth_handicap\", \"max\")\n",
    "    ).reset_index()\n",
    "\n",
    "    return human_df\n",
    "\n",
    "# -------------------------\n",
    "# 2️⃣ Charger les données validées\n",
    "# -------------------------\n",
    "def load_validated_data():\n",
    "    dfs = []\n",
    "    for cat in CATEGORIES:\n",
    "        for file in (VALIDATED_PATH / cat).glob(\"*.csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            df[\"category\"] = cat\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# 3️⃣ Charger et fusionner les datasets\n",
    "# -------------------------\n",
    "human_df = load_human_labels()\n",
    "validated_df = load_validated_data()\n",
    "\n",
    "# Merge sur id\n",
    "merged = human_df.merge(validated_df, on=\"id\", how=\"left\")\n",
    "\n",
    "# Gestion des colonnes review\n",
    "if \"review_x\" in merged.columns:\n",
    "    merged[\"review\"] = merged[\"review_x\"]\n",
    "    merged = merged.drop(columns=[\"review_x\"], errors=\"ignore\")\n",
    "    if \"review_y\" in merged.columns:\n",
    "        merged = merged.drop(columns=[\"review_y\"], errors=\"ignore\")\n",
    "elif \"review\" not in merged.columns:\n",
    "    raise ValueError(\"La colonne 'review' est absente après merge\")\n",
    "\n",
    "# Nettoyage des retours à la ligne\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\n\", \" \", regex=False)\n",
    "merged[\"review\"] = merged[\"review\"].str.replace(\"\\r\", \" \", regex=False)\n",
    "\n",
    "# Colonnes finales\n",
    "final_cols = [\n",
    "    \"id\", \"review\",\n",
    "    \"human_truth_child\", \"human_truth_pet\", \"human_truth_handicap\",\n",
    "    \"validation_status\",\n",
    "    \"llm_child\", \"llm_pet\", \"llm_handicap\",\n",
    "    \"bert_child\", \"bert_pet\", \"bert_handicap\",\n",
    "    \"kw_child\", \"kw_pet\", \"kw_handicap\"\n",
    "]\n",
    "\n",
    "final_dataset = merged[final_cols]\n",
    "\n",
    "print(f\"NB REVIEWS : {len(final_dataset)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4️⃣ Calcul des métriques par layer\n",
    "# -------------------------\n",
    "def compute_pipeline_metrics(df: pd.DataFrame):\n",
    "    metrics = {}\n",
    "    total_reviews = len(df)\n",
    "\n",
    "    # Keywords layer\n",
    "    kw_correct = (\n",
    "        ((df[\"kw_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "    metrics[\"keywords_precision\"] = kw_correct.sum() / total_reviews\n",
    "    metrics[\"keywords_coverage\"] = total_reviews / total_reviews\n",
    "    metrics[\"keywords_gain\"] = 0  # pas de gain pour le premier layer\n",
    "\n",
    "    # BERT layer\n",
    "    bert_correct = (\n",
    "        ((df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "    bert_gain = (\n",
    "        ((df[\"kw_child\"] != df[\"llm_child\"]) & (df[\"bert_child\"] == df[\"llm_child\"]) & (df[\"human_truth_child\"] == 1)) &\n",
    "        ((df[\"kw_pet\"] != df[\"llm_pet\"]) & (df[\"bert_pet\"] == df[\"llm_pet\"]) & (df[\"human_truth_pet\"] == 1)) &\n",
    "        ((df[\"kw_handicap\"] != df[\"llm_handicap\"]) & (df[\"bert_handicap\"] == df[\"llm_handicap\"]) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "    metrics[\"bert_layer_precision\"] = bert_correct.sum() / total_reviews\n",
    "    metrics[\"bert_layer_coverage\"] = total_reviews / total_reviews\n",
    "    metrics[\"bert_layer_gain\"] = bert_gain.sum() / total_reviews\n",
    "\n",
    "    # LLM layer\n",
    "    mask_llm = df[\"validation_status\"].isin([\"llm_validated\", \"disputed\"])\n",
    "    llm_correct = mask_llm & (\n",
    "        ((df[\"llm_child\"] == 1) & (df[\"human_truth_child\"] == 1)) |\n",
    "        ((df[\"llm_pet\"] == 1) & (df[\"human_truth_pet\"] == 1)) |\n",
    "        ((df[\"llm_handicap\"] == 1) & (df[\"human_truth_handicap\"] == 1))\n",
    "    )\n",
    "    metrics[\"llm_layer_precision\"] = llm_correct.sum() / mask_llm.sum() if mask_llm.sum() > 0 else 0\n",
    "    metrics[\"llm_layer_coverage\"] = mask_llm.sum() / total_reviews\n",
    "    metrics['llm_layer_gain'] = llm_correct.sum() / total_reviews\n",
    "\n",
    "    # LLM préférences quand ce n'est pas \"agreed\"\n",
    "    mask_disagree = df[\"validation_status\"] != \"agreed\"\n",
    "    llm_pref = {}\n",
    "\n",
    "    llm_pref[\"agree_kw_percentage\"] = (\n",
    "        ((df[\"kw_child\"] == df[\"llm_child\"]) & mask_disagree) &\n",
    "        ((df[\"kw_pet\"] == df[\"llm_pet\"]) & mask_disagree) &\n",
    "        ((df[\"kw_handicap\"] == df[\"llm_handicap\"]) & mask_disagree)\n",
    "    ).sum() / mask_disagree.sum() if mask_disagree.sum() > 0 else 0\n",
    "\n",
    "    llm_pref[\"agree_bert_percentage\"] = (\n",
    "        ((df[\"bert_child\"] == df[\"llm_child\"]) & mask_disagree) &\n",
    "        ((df[\"bert_pet\"] == df[\"llm_pet\"]) & mask_disagree) &\n",
    "        ((df[\"bert_handicap\"] == df[\"llm_handicap\"]) & mask_disagree)\n",
    "    ).sum() / mask_disagree.sum() if mask_disagree.sum() > 0 else 0\n",
    "\n",
    "    llm_pref[\"disagree_both_percentage\"] = (\n",
    "        1 - llm_pref[\"agree_kw_percentage\"] - llm_pref[\"agree_bert_percentage\"]\n",
    "    )\n",
    "\n",
    "    metrics[\"llm_preferences_counts\"] = llm_pref\n",
    "\n",
    "    # Gains pipeline\n",
    "    metrics[\"pipeline_added_value\"] = metrics[\"bert_layer_gain\"] + metrics['llm_layer_gain']\n",
    "    metrics[\"final_pipeline_precision\"] = (\n",
    "        ((df[\"human_truth_child\"] == 1) | (df[\"human_truth_pet\"] == 1) | (df[\"human_truth_handicap\"] == 1)).sum()\n",
    "        / total_reviews\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# 5️⃣ Affichage des métriques\n",
    "# -------------------------\n",
    "def display_pipeline_metrics(metrics: dict):\n",
    "    print(\"=== PIPELINE METRICS ===\\n\")\n",
    "    print(f\"Keywords layer: Precision = {metrics['keywords_precision']:.3%}, \"\n",
    "          f\"Coverage = {metrics['keywords_coverage']:.3%}, Gain = {metrics['keywords_gain']:.3%}\")\n",
    "    print(f\"BERT layer: Precision = {metrics['bert_layer_precision']:.3%}, \"\n",
    "          f\"Coverage = {metrics['bert_layer_coverage']:.3%}, Gain = {metrics['bert_layer_gain']:.3%}\")\n",
    "    print(f\"LLM layer: Precision = {metrics['llm_layer_precision']:.3%}, \"\n",
    "          f\"Coverage = {metrics['llm_layer_coverage']:.3%}, Gain = {metrics['llm_layer_gain']:.3%}\")\n",
    "\n",
    "    print(\"\\nLLM preferences when BERT and Keywords disagree:\")\n",
    "    for k, v in metrics['llm_preferences_counts'].items():\n",
    "        print(f\"  - {k}: {v:.3%}\" if isinstance(v, float) else f\"  - {k}: {v}\")\n",
    "\n",
    "    print(f\"\\nFinal pipeline pseudo-precision = {metrics['final_pipeline_precision']:.3%}\")\n",
    "    print(f\"Total pipeline added value (gain) = {metrics['pipeline_added_value']:.3%}\")\n",
    "\n",
    "# -------------------------\n",
    "# 6️⃣ Exécution\n",
    "# -------------------------\n",
    "metrics = compute_pipeline_metrics(final_dataset)\n",
    "display_pipeline_metrics(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
