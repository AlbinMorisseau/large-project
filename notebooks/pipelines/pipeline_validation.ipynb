{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59208fa0",
   "metadata": {},
   "source": [
    "# Pipeline Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88158dc9",
   "metadata": {},
   "source": [
    "We created a pipeline that combines the results of three methods to have a \"cross validation\" on our reveiws classifications:\n",
    "\n",
    "- the result of the keywords extractionn that classified a review in the theme related to the keyword\n",
    "- the result of a finetunned BERT model on the review enabling better classification\n",
    "- the result of a small LLM on the review enabling better context understanding\n",
    "\n",
    "If a review gets classified the same way by each of these 3 methods it is considered valid. \n",
    "Either way, it has to be submitted to human validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4161ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff90d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filterign HTTP logging\n",
    "class HttpStatusFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        message = record.getMessage()\n",
    "        if 'HTTP/1.1 200' not in message:\n",
    "            record.levelname = \"WARNING\"\n",
    "            record.levelno = logging.WARNING\n",
    "        return 'HTTP/1.1 200' not in message\n",
    "    \n",
    "logging.getLogger(\"httpx\").addFilter(HttpStatusFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:09:38,352 - INFO - NUM_THREAD fixed to 8\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisation: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CATEGORIES = ['handicap', 'pet', 'child']\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_THREADS = 4\n",
    "BERT_PATH = \"../../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"../bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"../bert/best_weights.pth\"\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilisation: {device}\")\n",
    "\n",
    "# Création des dossiers de sortie\n",
    "Path(\"../../data/processed/data_validated/good\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../../data/processed/data_validated/rejected\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2faab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_review_chunks(review_text, max_length=128):\n",
    "    \"\"\"Divise une review en chunks de max_length tokens\"\"\"\n",
    "    words = review_text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk).split()) >= max_length - 20:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks if chunks else [review_text]\n",
    "\n",
    "\n",
    "def predict_bert_chunks(reviews, model, tokenizer, threshold=0.95, batch_size=32):\n",
    "    \"\"\"Prédit les catégories pour une liste de reviews avec chunking\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch_reviews = reviews[i:i + batch_size]\n",
    "            \n",
    "            # Encoder le batch\n",
    "            encoded = tokenizer(\n",
    "                batch_reviews,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Prédiction\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "            pred_bin = (pred > threshold).astype(int)\n",
    "            \n",
    "            all_predictions.extend(pred_bin.tolist())\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification via Ollama\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": (\n",
    "             \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "             f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "             f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "             \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "         )},\n",
    "        {\"role\": \"assistant\",\n",
    "         \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "    ]\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=messages,messages=messages,options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bfdb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:09:55,934 - INFO - BERT tokenizer loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:09:57,176 - INFO - BERT finetunned model loaded\n"
     ]
    }
   ],
   "source": [
    "# Loading BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "logger.info(\"BERT tokenizer loaded\")\n",
    "\n",
    "print(\"Chargement du modèle BERT...\")\n",
    "model = BertMultiLabelClassifier(n_classes=len(CATEGORIES))\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "logger.info(\"BERT finetunned model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4029a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Loading data\n",
    "    logger.info(\"Begining of the pipeline\")\n",
    "    keywords_df = pl.read_csv(\"../../data/processed/data_categorized/key_words_data_accessiblego.csv\")\n",
    "    original_df = pl.read_csv(\"../../data/original/dataset/data_accessiblego.csv\")\n",
    "    logger.info(\"Data loaded\")\n",
    "    \n",
    "    # Fusion to get the original reviews\n",
    "    keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "    original_reviews = original_df.select([\"id\", \"review\"])\n",
    "    df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "    df = df.group_by('id').agg(\n",
    "        pl.col('review').first(), \n",
    "        pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "    )\n",
    "\n",
    "    # BERT prediction on chunks\n",
    "    logger.info(\"BERT prediction with chunking...\")\n",
    "    bert_predictions = []\n",
    "    \n",
    "    for review in tqdm(df['review'], desc=\"Review Processing\"):\n",
    "        chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "        chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "        # Agregation: if a chunk is positive the while review is\n",
    "        final_pred = [0, 0, 0]\n",
    "        for pred in chunk_preds:\n",
    "            for i in range(3):\n",
    "                if pred[i]:\n",
    "                    final_pred[i] = 1\n",
    "        \n",
    "        bert_predictions.append(final_pred)\n",
    "    \n",
    "    # Conversion of keywords extraction category to one hot format\n",
    "    keyword_preds = []\n",
    "    for category in df['category']:\n",
    "        pred = [0, 0, 0]\n",
    "        if 'handicap' in str(category).lower():\n",
    "            pred[0] = 1\n",
    "        if 'pet' in str(category).lower():\n",
    "            pred[1] = 1\n",
    "        if 'child' in str(category).lower():\n",
    "            pred[2] = 1\n",
    "        keyword_preds.append(pred)\n",
    "    \n",
    "    # Adding predicitons to the dataframe\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_child\", [p[2] for p in keyword_preds]),\n",
    "        pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_child\", [p[2] for p in bert_predictions])  \n",
    "    ])\n",
    "    \n",
    "    # Comparaison and filtering\n",
    "    logger.info(\"Prediction comparaison\")\n",
    "    validated_rows = []\n",
    "    disputed_rows = []\n",
    "    \n",
    "    rows = iter(df.iter_rows(named=True))\n",
    "    for row in tqdm(rows, total=len(df), desc=\"Validation\"):\n",
    "        bert_preds = [row['bert_handicap'], row['bert_pet'], row['bert_child']]\n",
    "        kw_preds = [row['kw_handicap'], row['kw_pet'], row['kw_child']]\n",
    "        \n",
    "        if bert_preds == kw_preds:\n",
    "            # Agreement between kw and BERT\n",
    "            validated_rows.append({\n",
    "                **row,\n",
    "                'validation_status': 'agreed',\n",
    "                'llm_handicap':row['kw_handicap'],\n",
    "                'llm_pet': row['kw_pet'],\n",
    "                'llm_child': row['kw_child']\n",
    "            })\n",
    "        else:\n",
    "            # Desagreement: ask LLM\n",
    "            llm_preds = [None, None, None]\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "                futures = {}\n",
    "                for i, cat in enumerate(CATEGORIES):\n",
    "                    if bert_preds[i] != kw_preds[i]:\n",
    "                        futures[executor.submit(classify_review_ollama, row['review'], cat)] = i\n",
    "                    else:\n",
    "                        llm_preds[i]= kw_preds[i]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    cat_idx = futures[future]\n",
    "                    llm_preds[cat_idx] = future.result()\n",
    "            \n",
    "            # Verify if a the LLM agrees with keywords predictions\n",
    "            agrees_with_kw = any(llm_preds[i] == kw_preds[i] for i in range(3) if llm_preds[i] is not None)\n",
    "            \n",
    "            #if agrees_with_kw:\n",
    "            if agrees_with_kw:\n",
    "                validated_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'llm_validated',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "            else:\n",
    "                disputed_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'disputed',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "    \n",
    "    # Saving results\n",
    "    logger.info(\"Saving results...\")\n",
    "    validated_df = pl.DataFrame(validated_rows)\n",
    "    disputed_df = pl.DataFrame(disputed_rows)\n",
    "    \n",
    "    validated_df.write_csv(\"../../data/processed/data_validated/good/validated_reviews.csv\")\n",
    "    disputed_df.write_csv(\"../../data/processed/data_validated/rejected/disputed_reviews.csv\")\n",
    "    \n",
    "    logger.info(\"Pipeline ended\")\n",
    "    logger.info(f\"  - Reviews validated: {len(validated_rows)}\")\n",
    "    logger.info(f\"  - Reviews to validate: {len(disputed_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be7768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-20 18:28:00,634 - INFO - Begining of the pipeline\n",
      "2025-11-20 18:28:00,651 - INFO - Data loaded\n",
      "2025-11-20 18:28:00,673 - INFO - BERT prediction with chunking...\n",
      "Review Processing: 100%|██████████| 1487/1487 [00:52<00:00, 28.33it/s]\n",
      "2025-11-20 18:28:53,174 - INFO - Prediction comparaison\n",
      "Validation: 100%|██████████| 1487/1487 [08:03<00:00,  3.07it/s]\n",
      "2025-11-20 18:36:56,855 - INFO - Saving results...\n",
      "2025-11-20 18:36:56,872 - INFO - Pipeline ended\n",
      "2025-11-20 18:36:56,872 - INFO -   - Reviews validated: 1486\n",
      "2025-11-20 18:36:56,872 - INFO -   - Reviews to validate: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b78fa",
   "metadata": {},
   "source": [
    "## Bout de code uniquement pour faire tourner BERt sans le LLM et améliorer le fine tuning du modèle (à supprimer à la fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_amelioration_finetuning():\n",
    "    \n",
    "    # Loading data\n",
    "    logger.info(\"Begining of the pipeline\")\n",
    "    keywords_df = pl.read_csv(\"../../data/processed/data_categorized/key_words_data_hotel_reviews_2_cleaned.csv\")\n",
    "    original_df = pl.read_csv(\"../../data/original/dataset/data_hotel_reviews_2.csv\")\n",
    "    logger.info(\"Data loaded\")\n",
    "    \n",
    "    # Fusion to get the original reviews\n",
    "    keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "    original_reviews = original_df.select([\"id\", \"review\"])\n",
    "    df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "    df = df.group_by('id').agg(\n",
    "        pl.col('review').first(), \n",
    "        pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "    )\n",
    "\n",
    "    # BERT prediction on chunks\n",
    "    logger.info(\"BERT prediction with chunking...\")\n",
    "    bert_predictions = []\n",
    "    \n",
    "    for review in tqdm(df['review'], desc=\"Review Processing\"):\n",
    "        chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "        chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "        # Agregation: if a chunk is positive the while review is\n",
    "        final_pred = [0, 0, 0]\n",
    "        for pred in chunk_preds:\n",
    "            for i in range(3):\n",
    "                if pred[i]:\n",
    "                    final_pred[i] = 1\n",
    "        \n",
    "        bert_predictions.append(final_pred)\n",
    "    \n",
    "    # Conversion of keywords extraction category to one hot format\n",
    "    keyword_preds = []\n",
    "    for category in df['category']:\n",
    "        pred = [0, 0, 0]\n",
    "        if 'handicap' in str(category).lower():\n",
    "            pred[0] = 1\n",
    "        if 'pet' in str(category).lower():\n",
    "            pred[1] = 1\n",
    "        if 'child' in str(category).lower():\n",
    "            pred[2] = 1\n",
    "        keyword_preds.append(pred)\n",
    "    \n",
    "    # Adding predicitons to the dataframe\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_child\", [p[2] for p in keyword_preds]),\n",
    "        pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_child\", [p[2] for p in bert_predictions])  \n",
    "    ])\n",
    "    \n",
    "    # Comparaison and filtering\n",
    "    logger.info(\"Prediction comparaison\")\n",
    "    validated_rows = []\n",
    "    disputed_rows = []\n",
    "    \n",
    "    rows = iter(df.iter_rows(named=True))\n",
    "    for row in tqdm(rows, total=len(df), desc=\"Validation\"):\n",
    "        bert_preds = [row['bert_handicap'], row['bert_pet'], row['bert_child']]\n",
    "        kw_preds = [row['kw_handicap'], row['kw_pet'], row['kw_child']]\n",
    "        \n",
    "        if bert_preds == kw_preds:\n",
    "            # Agreement between kw and BERT\n",
    "            validated_rows.append({\n",
    "                **row,\n",
    "                'validation_status': 'agreed',\n",
    "            })\n",
    "\n",
    "        else:\n",
    "            disputed_rows.append({\n",
    "                **row,\n",
    "                'validation_status': 'disputed',\n",
    "            })\n",
    "    \n",
    "    # Saving results\n",
    "    logger.info(\"Saving results...\")\n",
    "    disputed_df = pl.DataFrame(disputed_rows)\n",
    "\n",
    "    \n",
    "    disputed_df.write_csv(\"reviews_à_regarder_hotel_reviews_2.csv\")\n",
    "    \n",
    "    logger.info(\"Pipeline ended\")\n",
    "    logger.info(f\"  - Reviews validated: {len(disputed_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3cdbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:11:50,871 - INFO - Begining of the pipeline\n",
      "2025-11-21 11:11:50,960 - INFO - Data loaded\n",
      "2025-11-21 11:11:50,979 - INFO - BERT prediction with chunking...\n",
      "Review Processing: 100%|██████████| 1292/1292 [00:11<00:00, 109.77it/s]\n",
      "2025-11-21 11:12:02,751 - INFO - Prediction comparaison\n",
      "Validation: 100%|██████████| 1292/1292 [00:00<00:00, 732500.78it/s]\n",
      "2025-11-21 11:12:02,755 - INFO - Saving results...\n",
      "2025-11-21 11:12:02,755 - INFO - Pipeline ended\n",
      "2025-11-21 11:12:02,755 - INFO -   - Reviews validated: 383\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_amelioration_finetuning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
