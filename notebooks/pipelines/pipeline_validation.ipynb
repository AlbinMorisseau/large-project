{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59208fa0",
   "metadata": {},
   "source": [
    "# Pipeline Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88158dc9",
   "metadata": {},
   "source": [
    "We created a pipeline that combines the results of three methods to have a \"cross validation\" on our reveiws classifications:\n",
    "\n",
    "- the result of the keywords extractionn that classified a review in the theme related to the keyword\n",
    "- the result of a finetunned BERT model on the review enabling better classification\n",
    "- the result of a small LLM on the review enabling better context understanding\n",
    "\n",
    "If a review gets classified the same way by each of these 3 methods it is considered valid. \n",
    "Either way, it has to be submitted to human validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4161ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff90d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loger for pipeline execution\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filterign HTTP logging\n",
    "class HttpStatusFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        message = record.getMessage()\n",
    "        if 'HTTP/1.1 200' not in message:\n",
    "            record.levelname = \"WARNING\"\n",
    "            record.levelno = logging.WARNING\n",
    "        return 'HTTP/1.1 200' not in message\n",
    "    \n",
    "logging.getLogger(\"httpx\").addFilter(HttpStatusFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0dc4ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 08:52:37,571 - INFO - NUM_THREAD fixed to 10\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "NUM_THREAD = int(os.environ.get(\"NUM_THREADS\"))\n",
    "logger.info(f\"NUM_THREAD fixed to {NUM_THREAD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd27fff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device utilisation: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CATEGORIES = ['handicap', 'pet', 'child']\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_THREADS = 4\n",
    "BERT_PATH = \"../../models/bert-base-uncased\"\n",
    "TOKENIZER_PATH = \"../bert/bert_tokenizer_pt\"\n",
    "MODEL_WEIGHTS = \"../bert/best_weights_v3.pth\"\n",
    "THRESHOLD = 0.95\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilisation: {device}\")\n",
    "\n",
    "# Création des dossiers de sortie\n",
    "Path(\"../../data/processed/data_validated/good\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"../../data/processed/data_validated/rejected\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2faab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_PATH)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8066e225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_review_chunks(review_text, max_length=128):\n",
    "    \"\"\"Divise une review en chunks de max_length tokens\"\"\"\n",
    "    words = review_text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(' '.join(current_chunk).split()) >= max_length - 20:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks if chunks else [review_text]\n",
    "\n",
    "\n",
    "def predict_bert_chunks(reviews, model, tokenizer, threshold=0.95, batch_size=32):\n",
    "    \"\"\"Prédit les catégories pour une liste de reviews avec chunking\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch_reviews = reviews[i:i + batch_size]\n",
    "            \n",
    "            # Encoder le batch\n",
    "            encoded = tokenizer(\n",
    "                batch_reviews,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            input_ids = encoded[\"input_ids\"].to(device)\n",
    "            attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "            \n",
    "            # Prédiction\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask).cpu().numpy()\n",
    "            pred_bin = (pred > threshold).astype(int)\n",
    "            \n",
    "            all_predictions.extend(pred_bin.tolist())\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "# def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "#     \"\"\"Classification via Ollama\"\"\"\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\",\n",
    "#          \"content\": (\n",
    "#              \"You are a strict classifier. Your task is to analyze a review and determine whether the \"\n",
    "#              f\"traveler(s) mentioned in the review have a very specific need in the category: '{category}'. \"\n",
    "#              f\"Respond strictly with 'yes' if the review indicates they travel with {category}, \"\n",
    "#              \"or 'no' if not. Your response must be ONE word only, without any explanation or extra text.\"\n",
    "#          )},\n",
    "#         {\"role\": \"assistant\",\n",
    "#          \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "#         {\"role\": \"user\",\n",
    "#          \"content\": f\"Here is the review to analyze:\\n\\n\\\"{review_text}\\\"\"}\n",
    "#     ]\n",
    "    \n",
    "#     response = ollama.chat(model=model, messages=messages,options={\"temperature\": 0})\n",
    "#     answer = response[\"message\"][\"content\"].strip().lower()\n",
    "#     cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "#     return 1 if cleaned == 'yes' else 0\n",
    "\n",
    "def classify_review_ollama(review_text, category, model=\"mistral\"):\n",
    "    \"\"\"Classification with Ollama \"\"\"\n",
    "    \n",
    "    if category == \"child\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                  \"You are a strict family-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with children. Especially, you need to determine\"\n",
    "                 \" if these children have a high chance to be under 18 years old.\"\n",
    "                 \"Respond strictly with 'yes' if the review indicates people travelling with children, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"We traveled with our kids and loved the family-friendly pool.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The hotel was great, but we went alone as a couple.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I got there at 6:30, and a kid that apparently worked there (no id/uniform) was scrambling to set everything up\\\" -> no\\n\"\n",
    "                 \"Review: \\\"My Grand kids loved the pool\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I travelled to Dakota to see my son graduatation\\\" -> no\\n\"\n",
    "                 \"Review: \\\"This family owned business has a welcoming staff which made us feel right at home\\\" -> no\\n\"\n",
    "                 \"Review: \\\"If I had to ask one thing of Best Western, please replace the mattresses or box springs every time our kids moved at night\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"pet\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict pet-friendly-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) are traveling with pets. \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates they travel with pets, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Thanks again Cat!\\\" -> no\\n\"\n",
    "                 \"Review: \\\"I only booked this hotel because it was dog friendly\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I wanted to see if I could bring my service dog with me but they told me it was impossible at the front desk.\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The bedsheets were smelling cat urine. Horrible !\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Perfect for travelers with cats or dogs.\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    elif category == \"handicap\":\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\n",
    "                 \"You are a strict business-travel-review classifier. Your task is to analyze a review and determine \"\n",
    "                 \"whether the traveler(s) have any type of handicap or if the reviews contains a specific needs\"\n",
    "                 \"associated with a disability (transporations, amenities, etc.) \"\n",
    "                 \"Respond strictly with 'yes' if the review indicates a handicaped traveler or a special need related to handicap travelling, or 'no' if not. \"\n",
    "                 \"ONE word only, no explanations or extra text.\"\n",
    "             )},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"Understood. I will respond only with 'yes' or 'no', one word.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": (\n",
    "                 \"Here are some examples:\\n\"\n",
    "                 \"Review: \\\"Plant to go to London in September Need information about Accessible Van in London airport\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"The room was great, big enough to move around in my power chair in both the bedroom and bathroom\\\" -> yes\\n\"\n",
    "                 \"Review: \\\"I would like to sell my wheelchair.please contact me\\\" -> no\\n\"\n",
    "                 \"Review: \\\"It's new digital travel magazine targeted exclusively for travelers with disabilities.\\\" -> no\\n\"\n",
    "                 \"Review: \\\"Nice roll-in shower with a pull-down bench, but the amenities were again too high\\\" -> yes\\n\\n\"\n",
    "                 f\"Now classify this review:\\n\\\"{review_text}\\\"\"\n",
    "             )}\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown category: {category}\")\n",
    "    \n",
    "    # Ici tu peux directement envoyer `messages` à Ollama\n",
    "    response = ollama.chat(model=model, messages=messages, options={\"temperature\": 0})\n",
    "    answer = response[\"message\"][\"content\"].strip().lower()\n",
    "    cleaned = re.sub(r'[^a-z]', '', answer)\n",
    "    \n",
    "    return 1 if cleaned == 'yes' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bfdb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 08:52:42,165 - INFO - BERT tokenizer loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle BERT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 08:52:44,174 - INFO - BERT finetunned model loaded\n"
     ]
    }
   ],
   "source": [
    "# Loading BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "logger.info(\"BERT tokenizer loaded\")\n",
    "\n",
    "print(\"Chargement du modèle BERT...\")\n",
    "model = BertMultiLabelClassifier(n_classes=len(CATEGORIES))\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "logger.info(\"BERT finetunned model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4029a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_name):\n",
    "\n",
    "    logger.info(\"Beginning of the pipeline\")\n",
    "\n",
    "    # Build paths dynamically\n",
    "    processed_path = f\"../../data/processed/data_categorized/{file_name}\"\n",
    "    original_name = file_name.replace(\"key_words_\", \"\")\n",
    "    original_path = f\"../../data/original/dataset/{original_name}\"\n",
    "\n",
    "    # Loading data\n",
    "    keywords_df = pl.read_csv(processed_path)\n",
    "    original_df = pl.read_csv(original_path)\n",
    "    logger.info(f\"Data loaded for {file_name}\")\n",
    "    \n",
    "    # Fusion to get the original reviews\n",
    "    keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "    original_reviews = original_df.select([\"id\", \"review\"])\n",
    "    df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "    df = df.group_by('id').agg(\n",
    "        pl.col('review').first(), \n",
    "        pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "    )\n",
    "\n",
    "    # BERT prediction on chunks\n",
    "    logger.info(\"BERT prediction with chunking...\")\n",
    "    bert_predictions = []\n",
    "    \n",
    "    for review in tqdm(df['review'], desc=\"BERT Review Processing\"):\n",
    "        chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "        chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "        # Agregation: if a chunk is positive the while review is\n",
    "        final_pred = [0, 0, 0]\n",
    "        for pred in chunk_preds:\n",
    "            for i in range(3):\n",
    "                if pred[i]:\n",
    "                    final_pred[i] = 1\n",
    "        \n",
    "        bert_predictions.append(final_pred)\n",
    "    \n",
    "    # Conversion of keywords extraction category to one hot format\n",
    "    keyword_preds = []\n",
    "    for category in df['category']:\n",
    "        pred = [0, 0, 0]\n",
    "        if 'handicap' in str(category).lower():\n",
    "            pred[0] = 1\n",
    "        if 'pet' in str(category).lower():\n",
    "            pred[1] = 1\n",
    "        if 'child' in str(category).lower():\n",
    "            pred[2] = 1\n",
    "        keyword_preds.append(pred)\n",
    "    \n",
    "    # Adding predicitons to the dataframe\n",
    "    df = df.with_columns([\n",
    "        pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "        pl.Series(\"kw_child\", [p[2] for p in keyword_preds]),\n",
    "        pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "        pl.Series(\"bert_child\", [p[2] for p in bert_predictions])  \n",
    "    ])\n",
    "    \n",
    "    # Comparaison and filtering\n",
    "    logger.info(\"Prediction comparaison\")\n",
    "    validated_rows = []\n",
    "    \n",
    "    rows = iter(df.iter_rows(named=True))\n",
    "    for row in tqdm(rows, total=len(df), desc=\"Validation\"):\n",
    "        bert_preds = [row['bert_handicap'], row['bert_pet'], row['bert_child']]\n",
    "        kw_preds = [row['kw_handicap'], row['kw_pet'], row['kw_child']]\n",
    "        \n",
    "        if bert_preds == kw_preds:\n",
    "            # Agreement between kw and BERT\n",
    "            validated_rows.append({\n",
    "                **row,\n",
    "                'validation_status': 'agreed',\n",
    "                'llm_handicap':row['kw_handicap'],\n",
    "                'llm_pet': row['kw_pet'],\n",
    "                'llm_child': row['kw_child']\n",
    "            })\n",
    "        else:\n",
    "            # Desagreement: ask LLM\n",
    "            llm_preds = [None, None, None]\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "                futures = {}\n",
    "                for i, cat in enumerate(CATEGORIES):\n",
    "                    if bert_preds[i] != kw_preds[i]:\n",
    "                        futures[executor.submit(classify_review_ollama, row['review'], cat)] = i\n",
    "                    else:\n",
    "                        llm_preds[i] = bert_preds[i]\n",
    "                \n",
    "                for future in as_completed(futures):\n",
    "                    cat_idx = futures[future]\n",
    "                    llm_preds[cat_idx] = future.result()\n",
    "            \n",
    "            # Verify if a the LLM agrees with keywords predictions\n",
    "            agrees_with_bert = all(llm_preds[i] == kw_preds[i] for i in range(3) if llm_preds[i] is not None)\n",
    "            \n",
    "            #if agrees_with_kw:\n",
    "            if agrees_with_bert:\n",
    "                validated_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'llm_validated',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "            else:\n",
    "                validated_rows.append({\n",
    "                    **row,\n",
    "                    'validation_status': 'disputed',\n",
    "                    'llm_handicap': llm_preds[0],\n",
    "                    'llm_pet': llm_preds[1],\n",
    "                    'llm_child': llm_preds[2]\n",
    "                })\n",
    "    \n",
    "    # Saving results\n",
    "    logger.info(\"Saving results...\")\n",
    "    validated_df = pl.DataFrame(validated_rows)\n",
    "    \n",
    "    validated_path=f\"../../data/processed/data_validated/validated_{file_name}\"\n",
    "    validated_df.write_csv(validated_path)\n",
    "    \n",
    "    logger.info(\"Pipeline ended\")\n",
    "    logger.info(f\"  - Reviews validated: {len(validated_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43be7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82afbe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import platform\n",
    "# import time\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     folder = \"../../data/processed/data_categorized/\"\n",
    "\n",
    "#     files = glob.glob(os.path.join(folder, \"key_words_*.csv\"))\n",
    "\n",
    "#     for f in files:\n",
    "#         filename = os.path.basename(f)\n",
    "#         if filename!=\"data_test\":\n",
    "#             try:\n",
    "#                 main(filename)\n",
    "#             except:\n",
    "#                 logger.warning(\"ERROR Something went wrong !!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "#     # Shutdown\n",
    "#     time.sleep(60) \n",
    "\n",
    "#     os.system(\"shutdown /s /t 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3aa6791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 08:53:05,172 - INFO - Skipping excluded dataset: data_accessiblego\n",
      "2025-11-27 08:53:05,173 - INFO - Skipping excluded dataset: data_activities_reviews\n",
      "2025-11-27 08:53:05,175 - INFO - Skipping excluded dataset: data_airline_reviews_1\n",
      "2025-11-27 08:53:05,175 - INFO - Skipping excluded dataset: data_airline_reviews_2\n",
      "2025-11-27 08:53:05,175 - INFO - Skipping excluded dataset: data_booking\n",
      "2025-11-27 08:53:05,175 - INFO - Skipping excluded dataset: data_european_hotel_reviews\n",
      "2025-11-27 08:53:05,175 - INFO - Skipping excluded dataset: data_european_restaurant_reviews\n",
      "2025-11-27 08:53:05,177 - INFO - Skipping excluded dataset: data_hotel_reviews_1\n",
      "2025-11-27 08:53:05,177 - INFO - Skipping excluded dataset: data_hotel_reviews_2\n",
      "2025-11-27 08:53:05,179 - INFO - Skipping excluded dataset: data_hotel_reviews_3\n",
      "2025-11-27 08:53:05,179 - INFO - Skipping excluded dataset: data_restaurant_reviews_1\n",
      "2025-11-27 08:53:05,179 - INFO - Skipping excluded dataset: data_restaurant_reviews_2\n",
      "2025-11-27 08:53:05,179 - INFO - Skipping excluded dataset: data_test\n",
      "2025-11-27 08:53:05,179 - INFO - Beginning of the pipeline\n",
      "2025-11-27 08:53:06,864 - INFO - Data loaded for key_words_data_tripadvisor_hotel_reviews.csv\n",
      "2025-11-27 08:53:06,975 - INFO - BERT prediction with chunking...\n",
      "BERT Review Processing: 100%|██████████| 132318/132318 [41:51<00:00, 52.68it/s] \n",
      "2025-11-27 09:35:00,811 - INFO - Prediction comparaison\n",
      "Validation: 100%|██████████| 132318/132318 [7:00:47<00:00,  5.24it/s]   \n",
      "2025-11-27 16:35:48,264 - INFO - Saving results...\n",
      "2025-11-27 16:35:50,578 - INFO - Pipeline ended\n",
      "2025-11-27 16:35:50,580 - INFO -   - Reviews validated: 132318\n",
      "2025-11-27 16:35:51,791 - INFO - Skipping excluded dataset: data_twitter\n",
      "2025-11-27 16:35:51,794 - INFO - Skipping excluded dataset: data_yelp_reviews\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import platform\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder = \"../../data/processed/data_categorized/\"\n",
    "\n",
    "    files = glob.glob(os.path.join(folder, \"key_words_*.csv\"))\n",
    "\n",
    "    excluded = [\n",
    "        \"data_test\",\n",
    "        \"data_accessiblego\",\n",
    "        \"data_activities_reviews\",\n",
    "        \"data_airline_reviews_1\",\n",
    "        \"data_airline_reviews_2\",\n",
    "        \"data_european_hotel_reviews\",\n",
    "        \"data_european_restaurant_reviews\",\n",
    "        \"data_hotel_reviews_1\",\n",
    "        \"data_booking\",\n",
    "        \"data_hotel_reviews_2\",\n",
    "        \"data_hotel_reviews_3\",\n",
    "        \"data_restaurant_reviews_1\",\n",
    "        \"data_restaurant_reviews_2\",\n",
    "        \"data_yelp_reviews\",\n",
    "        \"data_twitter\"\n",
    "    ]\n",
    "\n",
    "    for f in files:\n",
    "        filename = os.path.basename(f)  # ex: key_words_data_test.csv\n",
    "\n",
    "        # retirer prefixe + extension → ex: 'key_words_data_test' → 'data_test'\n",
    "        base_name = filename.replace(\"key_words_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "        if base_name in excluded:\n",
    "            logger.info(f\"Skipping excluded dataset: {base_name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            main(filename)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ERROR Something went wrong for {filename}: {e}\")\n",
    "\n",
    "    # Shutdown\n",
    "    # time.sleep(60)\n",
    "\n",
    "    # if platform.system() == \"Windows\":\n",
    "    #     os.system(\"shutdown /s /t 0\")\n",
    "    # else:\n",
    "    #     os.system(\"shutdown now\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b78fa",
   "metadata": {},
   "source": [
    "## Bout de code uniquement pour faire tourner BERt sans le LLM et améliorer le fine tuning du modèle (à supprimer à la fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228649c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main_amelioration_finetuning():\n",
    "    \n",
    "#     # Loading data\n",
    "#     logger.info(\"Begining of the pipeline\")\n",
    "#     keywords_df = pl.read_csv(\"../../data/processed/data_categorized/key_words_data_accessiblego.csv\")\n",
    "#     original_df = pl.read_csv(\"../../data/original/dataset/data_accessiblego.csv\")\n",
    "#     logger.info(\"Data loaded\")\n",
    "    \n",
    "#     # Fusion to get the original reviews\n",
    "#     keywords_df = keywords_df.rename({\"review\": \"kw_review\"})\n",
    "#     original_reviews = original_df.select([\"id\", \"review\"])\n",
    "#     df = keywords_df.join(original_reviews, on=\"id\", how=\"left\")\n",
    "#     df = df.group_by('id').agg(\n",
    "#         pl.col('review').first(), \n",
    "#         pl.col('category').cast(pl.Utf8).str.join(delimiter=' ')\n",
    "#     )\n",
    "\n",
    "#     # BERT prediction on chunks\n",
    "#     logger.info(\"BERT prediction with chunking...\")\n",
    "#     bert_predictions = []\n",
    "    \n",
    "#     for review in tqdm(df['review'], desc=\"Review Processing\"):\n",
    "#         chunks = split_review_chunks(str(review), MAX_LENGTH)\n",
    "#         chunk_preds = predict_bert_chunks(chunks, model, tokenizer, THRESHOLD, BATCH_SIZE)\n",
    "        \n",
    "#         # Agregation: if a chunk is positive the while review is\n",
    "#         final_pred = [0, 0, 0]\n",
    "#         for pred in chunk_preds:\n",
    "#             for i in range(3):\n",
    "#                 if pred[i]:\n",
    "#                     final_pred[i] = 1\n",
    "        \n",
    "#         bert_predictions.append(final_pred)\n",
    "    \n",
    "#     # Conversion of keywords extraction category to one hot format\n",
    "#     keyword_preds = []\n",
    "#     for category in df['category']:\n",
    "#         pred = [0, 0, 0]\n",
    "#         if 'handicap' in str(category).lower():\n",
    "#             pred[0] = 1\n",
    "#         if 'pet' in str(category).lower():\n",
    "#             pred[1] = 1\n",
    "#         if 'child' in str(category).lower():\n",
    "#             pred[2] = 1\n",
    "#         keyword_preds.append(pred)\n",
    "    \n",
    "#     # Adding predicitons to the dataframe\n",
    "#     df = df.with_columns([\n",
    "#         pl.Series(\"kw_handicap\", [p[0] for p in keyword_preds]),\n",
    "#         pl.Series(\"kw_pet\", [p[1] for p in keyword_preds]),\n",
    "#         pl.Series(\"kw_child\", [p[2] for p in keyword_preds]),\n",
    "#         pl.Series(\"bert_handicap\", [p[0] for p in bert_predictions]),\n",
    "#         pl.Series(\"bert_pet\", [p[1] for p in bert_predictions]),\n",
    "#         pl.Series(\"bert_child\", [p[2] for p in bert_predictions])  \n",
    "#     ])\n",
    "    \n",
    "#     # Comparaison and filtering\n",
    "#     logger.info(\"Prediction comparaison\")\n",
    "#     validated_rows = []\n",
    "#     disputed_rows = []\n",
    "    \n",
    "#     rows = iter(df.iter_rows(named=True))\n",
    "#     for row in tqdm(rows, total=len(df), desc=\"Validation\"):\n",
    "#         bert_preds = [row['bert_handicap'], row['bert_pet'], row['bert_child']]\n",
    "#         kw_preds = [row['kw_handicap'], row['kw_pet'], row['kw_child']]\n",
    "        \n",
    "#         if bert_preds == kw_preds:\n",
    "#             # Agreement between kw and BERT\n",
    "#             validated_rows.append({\n",
    "#                 **row,\n",
    "#                 'validation_status': 'agreed',\n",
    "#             })\n",
    "\n",
    "#         else:\n",
    "#             disputed_rows.append({\n",
    "#                 **row,\n",
    "#                 'validation_status': 'disputed',\n",
    "#             })\n",
    "    \n",
    "#     # Saving results\n",
    "#     logger.info(\"Saving results...\")\n",
    "#     disputed_df = pl.DataFrame(disputed_rows)\n",
    "\n",
    "    \n",
    "#     disputed_df.write_csv(\"reviews_à_regarder_hotel_reviews_2.csv\")\n",
    "    \n",
    "#     logger.info(\"Pipeline ended\")\n",
    "#     logger.info(f\"  - Reviews validated: {len(disputed_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cdbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:11:50,871 - INFO - Begining of the pipeline\n",
      "2025-11-21 11:11:50,960 - INFO - Data loaded\n",
      "2025-11-21 11:11:50,979 - INFO - BERT prediction with chunking...\n",
      "Review Processing: 100%|██████████| 1292/1292 [00:11<00:00, 109.77it/s]\n",
      "2025-11-21 11:12:02,751 - INFO - Prediction comparaison\n",
      "Validation: 100%|██████████| 1292/1292 [00:00<00:00, 732500.78it/s]\n",
      "2025-11-21 11:12:02,755 - INFO - Saving results...\n",
      "2025-11-21 11:12:02,755 - INFO - Pipeline ended\n",
      "2025-11-21 11:12:02,755 - INFO -   - Reviews validated: 383\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     main_amelioration_finetuning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122a595",
   "metadata": {},
   "source": [
    "## Split by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01fa0957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing validated_key_words_data_booking.csv ---\n",
      "  ✔ child saved: ../../data/processed/data_validated/child\\validated_data_booking_child.csv\n",
      "  ✔ handicap saved: ../../data/processed/data_validated/handicap\\validated_data_booking_handicap.csv\n",
      "  ✔ pet saved: ../../data/processed/data_validated/pet\\validated_data_booking_pet.csv\n",
      "\n",
      "--- Processing validated_key_words_data_tripadvisor_hotel_reviews.csv ---\n",
      "  ✔ child saved: ../../data/processed/data_validated/child\\validated_data_tripadvisor_hotel_reviews_child.csv\n",
      "  ✔ handicap saved: ../../data/processed/data_validated/handicap\\validated_data_tripadvisor_hotel_reviews_handicap.csv\n",
      "  ✔ pet saved: ../../data/processed/data_validated/pet\\validated_data_tripadvisor_hotel_reviews_pet.csv\n",
      "\n",
      "--- Processing validated_key_words_data_twitter.csv ---\n",
      "  ✔ child saved: ../../data/processed/data_validated/child\\validated_data_twitter_child.csv\n",
      "  ✔ pet saved: ../../data/processed/data_validated/pet\\validated_data_twitter_pet.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import polars as pl\n",
    "\n",
    "# dossiers source et destination\n",
    "base_folder = \"../../data/processed/data_validated/\"\n",
    "child_folder = os.path.join(base_folder, \"child\")\n",
    "handicap_folder = os.path.join(base_folder, \"handicap\")\n",
    "pet_folder = os.path.join(base_folder, \"pet\")\n",
    "\n",
    "# création des dossiers si inexistants\n",
    "os.makedirs(child_folder, exist_ok=True)\n",
    "os.makedirs(handicap_folder, exist_ok=True)\n",
    "os.makedirs(pet_folder, exist_ok=True)\n",
    "\n",
    "# récupérer tous les fichiers validated_*.csv\n",
    "validated_files = glob.glob(os.path.join(base_folder, \"validated_key_words_*.csv\"))\n",
    "\n",
    "for f in validated_files:\n",
    "    filename = os.path.basename(f)\n",
    "    print(f\"\\n--- Processing {filename} ---\")\n",
    "\n",
    "    # dataset = validated_data_accessiblego.csv → data_accessiblego\n",
    "    dataset = filename.replace(\"validated_key_words_\", \"\").replace(\".csv\", \"\")\n",
    "\n",
    "    # charger le dataframe\n",
    "    df = pl.read_csv(f)\n",
    "\n",
    "    # --- CHILD ---\n",
    "    df_child = df.filter(pl.col(\"llm_child\") == 1)\n",
    "    if len(df_child) > 0:\n",
    "        out_path = os.path.join(child_folder, f\"validated_{dataset}_child.csv\")\n",
    "        df_child.write_csv(out_path)\n",
    "        print(f\"  ✔ child saved: {out_path}\")\n",
    "\n",
    "    # --- HANDICAP ---\n",
    "    df_handicap = df.filter(pl.col(\"llm_handicap\") == 1)\n",
    "    if len(df_handicap) > 0:\n",
    "        out_path = os.path.join(handicap_folder, f\"validated_{dataset}_handicap.csv\")\n",
    "        df_handicap.write_csv(out_path)\n",
    "        print(f\"  ✔ handicap saved: {out_path}\")\n",
    "\n",
    "    # --- PET ---\n",
    "    df_pet = df.filter(pl.col(\"llm_pet\") == 1)\n",
    "    if len(df_pet) > 0:\n",
    "        out_path = os.path.join(pet_folder, f\"validated_{dataset}_pet.csv\")\n",
    "        df_pet.write_csv(out_path)\n",
    "        print(f\"  ✔ pet saved: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
